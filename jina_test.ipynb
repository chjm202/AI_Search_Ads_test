{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df592b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 横幅广告测试页\n",
      "\n",
      "URL Source: https://chjm202.github.io/AI_Search_Ads_test/ads_tmp/banner_ad_example.html\n",
      "\n",
      "Published Time: Sat, 11 Oct 2025 06:54:57 GMT\n",
      "\n",
      "Markdown Content:\n",
      "深入解析现代互联网广告生态\n",
      "-------------\n",
      "\n",
      "文章第一段：广告过滤工具的主要挑战之一是区分合法内容和植入式广告。很多网站为了绕过传统过滤机制，会将广告以看似自然的方式内嵌到内容流中。\n",
      "\n",
      "文章第二段：这种内嵌横幅广告的难点在于，它们不使用通用的广告域名或脚本，而是直接作为网页结构的一部分加载。一个高效的过滤工具必须依赖于结构分析和行为判断来识别它们。\n",
      "\n",
      "文章第三段：我们希望确保工具能够处理各种尺寸和颜色模式的广告，以提供最佳的用户体验。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "REMOVE_SELECTORS = \"#test-sidebar-ad, #test-popup-ad, .banner-ad\"\n",
    "TARGET_SELECTORS = \".main-content, body\"\n",
    "\n",
    "headers = {\n",
    "    \"X-Remove-Selector\": REMOVE_SELECTORS,\n",
    "    \"X-Target-Selector\": TARGET_SELECTORS\n",
    "}\n",
    "\n",
    "url = 'https://r.jina.ai/https://chjm202.github.io/AI_Search_Ads_test/ads_tmp/banner_ad_example.html'\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c23800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 弹窗广告测试页\n",
      "\n",
      "URL Source: https://chjm202.github.io/AI_Search_Ads_test/ads_tmp/popup_ad_example.html\n",
      "\n",
      "Published Time: Sat, 11 Oct 2025 06:54:57 GMT\n",
      "\n",
      "Markdown Content:\n",
      "正常网页内容\n",
      "------\n",
      "\n",
      "这是一个正常的网页，下面有大量的滚动内容，但被一个弹窗广告覆盖。\n",
      "\n",
      "向下滚动以确保弹窗广告是固定在屏幕上的。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://r.jina.ai/https://chjm202.github.io/AI_Search_Ads_test/ads_tmp/popup_ad_example.html'\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f035147b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 侧边栏广告测试页\n",
      "\n",
      "URL Source: https://chjm202.github.io/AI_Search_Ads_test/ads_tmp/sidebar_ad_example.html\n",
      "\n",
      "Published Time: Sat, 11 Oct 2025 06:54:57 GMT\n",
      "\n",
      "Markdown Content:\n",
      "文章主内容标题\n",
      "-------\n",
      "\n",
      "这是正文的第一段。测试工具应该能识别出右侧的元素是一个广告，并将其隐藏或移除，而不影响正文的正常阅读。\n",
      "\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
      "\n",
      "内容副标题\n",
      "-----\n",
      "\n",
      "Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://r.jina.ai/https://chjm202.github.io/AI_Search_Ads_test/ads_tmp/sidebar_ad_example.html'\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e23dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Large language model\n",
      "\n",
      "URL Source: https://en.wikipedia.org/wiki/Large_language_model\n",
      "\n",
      "Published Time: 2023-03-09T15:43:17Z\n",
      "\n",
      "Markdown Content:\n",
      "A **large language model** (**LLM**) is a [language model](https://en.wikipedia.org/wiki/Language_model \"Language model\") trained with [self-supervised](https://en.wikipedia.org/wiki/Self-supervised_learning \"Self-supervised learning\")[machine learning](https://en.wikipedia.org/wiki/Machine_learning \"Machine learning\") on a vast amount of text, designed for [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing \"Natural language processing\") tasks, especially [language generation](https://en.wikipedia.org/wiki/Natural_language_generation \"Natural language generation\").[[1]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-bhaa-1)[[2]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-few-shot-learners-2) The largest and most capable LLMs are [generative](https://en.wikipedia.org/wiki/Generative_artificial_intelligence \"Generative artificial intelligence\") pre-trained [transformers](https://en.wikipedia.org/wiki/Transformer_architecture \"Transformer architecture\") ([GPTs](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer \"Generative pre-trained transformer\")) and provide the core capabilities of [chatbots](https://en.wikipedia.org/wiki/Chatbot \"Chatbot\") such as [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT \"ChatGPT\"), [Gemini](https://en.wikipedia.org/wiki/Gemini_(chatbot) \"Gemini (chatbot)\") and [Claude](https://en.wikipedia.org/wiki/Claude_(language_model) \"Claude (language model)\"). LLMs can be [fine-tuned](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning) \"Fine-tuning (deep learning)\") for specific tasks or guided by [prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering \"Prompt engineering\").[[3]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-few-shot-learners2-3) These models acquire [predictive power](https://en.wikipedia.org/wiki/Predictive_learning \"Predictive learning\") regarding [syntax](https://en.wikipedia.org/wiki/Syntax \"Syntax\"), [semantics](https://en.wikipedia.org/wiki/Semantics \"Semantics\"), and [ontologies](https://en.wikipedia.org/wiki/Ontology_(information_science) \"Ontology (information science)\")[[4]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-4) inherent in human [language corpora](https://en.wikipedia.org/wiki/Text_corpus \"Text corpus\"), but they also inherit inaccuracies and [biases](https://en.wikipedia.org/wiki/Algorithmic_bias \"Algorithmic bias\") present in the [data](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets \"Training, validation, and test data sets\") they are trained on.[[5]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-Manning-2022-5)\n",
      "\n",
      "They consist of billions to trillions of [parameters](https://en.wikipedia.org/wiki/Parameter#Artificial_intelligence \"Parameter\") and operate as general-purpose sequence models, generating, summarizing, translating, and reasoning over text. LLMs represent a significant new technology in their ability to generalize across tasks with minimal task-specific supervision, enabling capabilities like [conversational agents](https://en.wikipedia.org/wiki/Conversational_agent \"Conversational agent\"), [code generation](https://en.wikipedia.org/wiki/Automatic_programming \"Automatic programming\"), [knowledge retrieval](https://en.wikipedia.org/wiki/Information_retrieval \"Information retrieval\"), and [automated reasoning](https://en.wikipedia.org/wiki/Automated_reasoning \"Automated reasoning\") that previously required bespoke systems.[[6]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-scaling-laws-6)\n",
      "\n",
      "LLMs evolved from earlier [statistical](https://en.wikipedia.org/wiki/Statistical_model \"Statistical model\") and [recurrent neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network \"Recurrent neural network\") approaches to language modeling. The [transformer architecture](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture) \"Transformer (deep learning architecture)\"), introduced in 2017, replaced recurrence with [self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning) \"Attention (machine learning)\"), allowing efficient [parallelization](https://en.wikipedia.org/wiki/Parallel_computing \"Parallel computing\"), longer context handling, and scalable training on unprecedented data volumes.[[7]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-7) This innovation enabled models like [GPT](https://en.wikipedia.org/wiki/GPT_(language_model) \"GPT (language model)\"), [BERT](https://en.wikipedia.org/wiki/BERT_(language_model) \"BERT (language model)\"), and their successors, which demonstrated [emergent behaviors](https://en.wikipedia.org/wiki/Emergence \"Emergence\") at scale such as [few-shot learning](https://en.wikipedia.org/wiki/Few-shot_learning \"Few-shot learning\") and compositional reasoning.[[8]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-8)\n",
      "\n",
      "[Reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning \"Reinforcement learning\"), particularly [policy gradient algorithms](https://en.wikipedia.org/wiki/Policy_gradient_method \"Policy gradient method\"), has been adapted to [fine-tune](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning) \"Fine-tuning (deep learning)\") LLMs for desired behaviors beyond raw next-token prediction.[[9]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-9)[Reinforcement learning from human feedback](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback \"Reinforcement learning from human feedback\") (RLHF) applies these methods to optimize a policy, the LLM's output distribution, against reward signals derived from human or automated preference judgments.[[10]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-10) This has been critical for aligning model outputs with user expectations, improving factuality, reducing harmful responses, and enhancing task performance.\n",
      "\n",
      "[Benchmark](https://en.wikipedia.org/wiki/Language_model_benchmark \"Language model benchmark\") evaluations for LLMs have evolved from narrow linguistic assessments toward comprehensive, [multi-task](https://en.wikipedia.org/wiki/Multi-task_learning \"Multi-task learning\") evaluations measuring [reasoning](https://en.wikipedia.org/wiki/Automated_reasoning \"Automated reasoning\"), [factual accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision \"Accuracy and precision\"), [alignment](https://en.wikipedia.org/wiki/AI_alignment \"AI alignment\"), and [safety](https://en.wikipedia.org/wiki/AI_safety \"AI safety\").[[11]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-11)[[12]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-12)[Hill climbing](https://en.wikipedia.org/wiki/Hill_climbing \"Hill climbing\"), iteratively optimizing models against benchmarks, has emerged as a dominant strategy, producing rapid incremental performance gains but raising concerns of [overfitting](https://en.wikipedia.org/wiki/Overfitting \"Overfitting\") to benchmarks rather than achieving genuine [generalization](https://en.wikipedia.org/wiki/Generalization_(machine_learning) \"Generalization (machine learning)\") or robust capability improvements.[[13]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-13)\n",
      "\n",
      "[![Image 1](https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/The_number_of_publications_about_Large_Language_Models_by_year.png/250px-The_number_of_publications_about_Large_Language_Models_by_year.png)](https://en.wikipedia.org/wiki/File:The_number_of_publications_about_Large_Language_Models_by_year.png)\n",
      "\n",
      "The number of publications about Large Language Models by year grouped by publication types.\n",
      "\n",
      "[![Image 2](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Trends_in_AI_training_FLOP_over_time_%282010-2025%29.svg/250px-Trends_in_AI_training_FLOP_over_time_%282010-2025%29.svg.png)](https://en.wikipedia.org/wiki/File:Trends_in_AI_training_FLOP_over_time_(2010-2025).svg)\n",
      "\n",
      "The training compute of notable large models in FLOPs vs publication date over the period 2010–2024. For overall notable models (top left), frontier models (top right), top language models (bottom left) and top models within leading companies (bottom right). The majority of these models are language models.\n",
      "\n",
      "[![Image 3](https://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Large-scale_AI_training_compute_%28FLOP%29_vs_Publication_date_%282017-2024%29.svg/250px-Large-scale_AI_training_compute_%28FLOP%29_vs_Publication_date_%282017-2024%29.svg.png)](https://en.wikipedia.org/wiki/File:Large-scale_AI_training_compute_(FLOP)_vs_Publication_date_(2017-2024).svg)\n",
      "\n",
      "The training compute of notable large AI models in FLOPs vs publication date over the period 2017–2024. The majority of large models are language models or multimodal models with language capacity.\n",
      "\n",
      "Before the emergence of transformer-based models in 2017, some [language models](https://en.wikipedia.org/wiki/Language_model \"Language model\") were considered large relative to the computational and data constraints of their time. In the early 1990s, [IBM](https://en.wikipedia.org/wiki/IBM \"IBM\")'s statistical models pioneered [word alignment](https://en.wikipedia.org/wiki/Bitext_word_alignment \"Bitext word alignment\") techniques for machine translation, laying the groundwork for [corpus-based language modeling](https://en.wikipedia.org/wiki/Construction_grammar \"Construction grammar\"). In 2001, a smoothed [_n_-gram model](https://en.wikipedia.org/wiki/Word_n-gram_language_model \"Word n-gram language model\"), such as those employing [Kneser–Ney smoothing](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing \"Kneser–Ney smoothing\"), trained on 300 million words, achieved state-of-the-art [perplexity](https://en.wikipedia.org/wiki/Perplexity \"Perplexity\") on benchmark tests.[[14]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-14) During the 2000s, with the rise of widespread internet access, researchers began compiling massive text datasets from the web (\"web as corpus\"[[15]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-15)) to train statistical language models.[[16]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-16)[[17]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-17)\n",
      "\n",
      "Moving beyond _n_-gram models, researchers started in 2000 to use neural networks to learn language models.[[18]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-18) Following the breakthrough of [deep neural networks](https://en.wikipedia.org/wiki/Deep_learning \"Deep learning\") in image classification around 2012,[[19]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-19) similar architectures were adapted for language tasks. This shift was marked by the development of [word embeddings](https://en.wikipedia.org/wiki/Word_embedding \"Word embedding\") (eg, [Word2Vec](https://en.wikipedia.org/wiki/Word2vec \"Word2vec\") by Mikolov in 2013) and sequence-to-sequence ([seq2seq](https://en.wikipedia.org/wiki/Seq2seq \"Seq2seq\")) models using [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory \"Long short-term memory\"). In 2016, Google transitioned its translation service to [neural machine translation](https://en.wikipedia.org/wiki/Neural_machine_translation \"Neural machine translation\") (NMT), replacing statistical phrase-based models with deep [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network \"Recurrent neural network\"). These early NMT systems used LSTM-based [encoder-decoder architectures](https://en.wikipedia.org/wiki/Encoder-decoder_model \"Encoder-decoder model\"), as they preceded the invention of [transformers](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture) \"Transformer (deep learning architecture)\").\n",
      "\n",
      "[![Image 4](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/The-Transformer-model-architecture.png/330px-The-Transformer-model-architecture.png)](https://en.wikipedia.org/wiki/File:The-Transformer-model-architecture.png)\n",
      "\n",
      "An illustration of main components of the transformer model from the original paper, where layers were normalized after (instead of before) multiheaded attention\n",
      "\n",
      "At the 2017 [NeurIPS](https://en.wikipedia.org/wiki/NeurIPS \"NeurIPS\") conference, Google researchers introduced the transformer architecture in their landmark paper \"[Attention Is All You Need](https://en.wikipedia.org/wiki/Attention_Is_All_You_Need \"Attention Is All You Need\")\". This paper's goal was to improve upon 2014 seq2seq technology,[[20]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-20) and was based mainly on the [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning) \"Attention (machine learning)\") mechanism developed by Bahdanau et al. in 2014.[[21]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-21) The following year in 2018, [BERT](https://en.wikipedia.org/wiki/BERT_(language_model) \"BERT (language model)\") was introduced and quickly became \"ubiquitous\".[[22]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-22) Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via [prompting](https://en.wikipedia.org/wiki/Prompt_engineering \"Prompt engineering\").[[23]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-auto-23)\n",
      "\n",
      "Although decoder-only [GPT-1](https://en.wikipedia.org/wiki/GPT-1 \"GPT-1\") was introduced in 2018, it was [GPT-2](https://en.wikipedia.org/wiki/GPT-2 \"GPT-2\") in 2019 that caught widespread attention because [OpenAI](https://en.wikipedia.org/wiki/OpenAI \"OpenAI\") claimed to have initially deemed it too powerful to release publicly, out of fear of malicious use.[[24]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-24)[GPT-3](https://en.wikipedia.org/wiki/GPT-3 \"GPT-3\") in 2020 went a step further and as of 2025 is available only via [API](https://en.wikipedia.org/wiki/Web_API \"Web API\") with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing chatbot [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT \"ChatGPT\") that received extensive media coverage and public attention.[[25]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-25) The 2023 [GPT-4](https://en.wikipedia.org/wiki/GPT-4 \"GPT-4\") was praised for its increased accuracy and as a \"holy grail\" for its [multimodal](https://en.wikipedia.org/wiki/Multimodal_learning \"Multimodal learning\") capabilities.[[26]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-26) OpenAI did not reveal the high-level architecture and the number of [parameters](https://en.wikipedia.org/wiki/Parameter#Artificial_intelligence \"Parameter\") of GPT-4. The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science, including robotics, software engineering, and societal impact work.[[23]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-auto-23) In 2024 OpenAI released the [reasoning model](https://en.wikipedia.org/wiki/Reasoning_language_model \"Reasoning language model\")[OpenAI o1](https://en.wikipedia.org/wiki/OpenAI_o1 \"OpenAI o1\"), which generates long chains of thought before returning a final answer.[[27]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-NYTimesInfo-27) Many LLMs with parameter counts comparable to those of OpenAI's GPT series have been developed.[[28]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-28)\n",
      "\n",
      "Since 2022, [source-available](https://en.wikipedia.org/wiki/Source-available_software \"Source-available software\") models have been gaining popularity, especially at first with [BLOOM](https://en.wikipedia.org/wiki/BLOOM_(language_model) \"BLOOM (language model)\") and [LLaMA](https://en.wikipedia.org/wiki/LLaMA \"LLaMA\"), though both have restrictions on the field of use. [Mistral AI](https://en.wikipedia.org/wiki/Mistral_AI \"Mistral AI\")'s models Mistral 7B and Mixtral 8x7b have the more permissive [Apache License](https://en.wikipedia.org/wiki/Apache_License \"Apache License\"). In January 2025, [DeepSeek](https://en.wikipedia.org/wiki/DeepSeek \"DeepSeek\") released DeepSeek R1, a 671-billion-parameter open-weight model that performs comparably to OpenAI o1 but at a much lower cost.[[29]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-29)\n",
      "\n",
      "Since 2023, many LLMs have been trained to be [multimodal](https://en.wikipedia.org/wiki/Multimodal_learning \"Multimodal learning\"), having the ability to also process or generate other types of data, such as images or audio. These LLMs are also called large multimodal models (LMMs).[[30]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-30)\n",
      "\n",
      "As of 2024, the largest and most capable models are all based on the transformer architecture. Some recent implementations are based on other architectures, such as [recurrent neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network \"Recurrent neural network\") variants and [Mamba](https://en.wikipedia.org/wiki/Mamba_(deep_learning_architecture) \"Mamba (deep learning architecture)\") (a [state space](https://en.wikipedia.org/wiki/State-space_representation \"State-space representation\") model).[[31]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-31)[[32]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-32)[[33]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-33)\n",
      "\n",
      "Dataset preprocessing\n",
      "---------------------\n",
      "\n",
      "[[edit](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit&section=2 \"Edit section: Dataset preprocessing\")]\n",
      "\n",
      "As [machine learning](https://en.wikipedia.org/wiki/Machine_learning \"Machine learning\") algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indices are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an [embedding](https://en.wikipedia.org/wiki/Word_embedding \"Word embedding\") is associated to the integer index. Algorithms include [byte-pair encoding](https://en.wikipedia.org/wiki/Byte-pair_encoding \"Byte-pair encoding\") (BPE) and WordPiece. There are also special tokens serving as [control characters](https://en.wikipedia.org/wiki/Control_character \"Control character\"), such as `[MASK]` for masked-out token (as used in [BERT](https://en.wikipedia.org/wiki/BERT_(language_model) \"BERT (language model)\")), and `[UNK]` (\"unknown\") for characters not appearing in the vocabulary. Also, some special symbols are used to denote special text formatting. For example, \"Ġ\" denotes a preceding whitespace in RoBERTa and GPT. \"##\" denotes continuation of a preceding word in BERT.[[34]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-34)\n",
      "\n",
      "For example, the BPE tokenizer used by [GPT-3](https://en.wikipedia.org/wiki/GPT-3 \"GPT-3\") (Legacy) would split `tokenizer: texts -> series of numerical \"tokens\"` as\n",
      "\n",
      "token izer:texts->series of numerical\"t ok ens\"\n",
      "\n",
      "Tokenization also [compresses](https://en.wikipedia.org/wiki/Data_compression \"Data compression\") the datasets. Because LLMs generally require input to be an [array](https://en.wikipedia.org/wiki/Array_(data_structure) \"Array (data structure)\") that is not [jagged](https://en.wikipedia.org/wiki/Jagged_array \"Jagged array\"), the shorter texts must be \"padded\" until they match the length of the longest one. The average number of words per token depends on the language.[[35]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-35)[[36]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-LangModelTokenizsersUnfairness-36) In English, the ratio is typically around 0.75 words per token, with 4 characters per token on average.[[37]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-37)\n",
      "\n",
      "As an example, consider a tokenizer based on byte-pair encoding. In the first step, all unique characters (including blanks and [punctuation marks](https://en.wikipedia.org/wiki/Punctuation_mark \"Punctuation mark\")) are treated as an initial set of [_n_-grams](https://en.wikipedia.org/wiki/N-gram \"N-gram\") (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) _n_-grams that most frequently occur together are then again merged into even lengthier _n_-gram, until a vocabulary of prescribed size is obtained. After a tokenizer is trained, any text can be tokenized by it, as long as it does not contain characters not appearing in the initial-set of uni-grams.[[38]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-2022Book_-38)\n",
      "\n",
      "A token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. However, an average word in another language encoded by such an English-optimized tokenizer is split into a suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages, for example for the [Shan language](https://en.wikipedia.org/wiki/Shan_language \"Shan language\") from [Myanmar](https://en.wikipedia.org/wiki/Myanmar \"Myanmar\"). Even more widespread languages such as [Portuguese](https://en.wikipedia.org/wiki/Portuguese_language \"Portuguese language\") and [German](https://en.wikipedia.org/wiki/German_language \"German language\") have \"a premium of 50%\" compared to English.[[36]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-LangModelTokenizsersUnfairness-36)\n",
      "\n",
      "In the context of training LLMs, datasets are typically cleaned by removing low-quality, duplicated, or toxic data.[[39]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-aYNg4-39) Cleaned datasets can increase training efficiency and lead to improved downstream performance.[[40]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-40)[[41]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-41) A trained LLM can be used to clean datasets for training a further LLM.[[42]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-42)\n",
      "\n",
      "With the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it).[[3]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-few-shot-learners2-3)\n",
      "\n",
      "Training of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality. In these cases, synthetic data might be used. Microsoft's [Phi](https://en.wikipedia.org/w/index.php?title=Phi_(LLM)&action=edit&redlink=1 \"Phi (LLM) (page does not exist)\") series of LLMs is trained on textbook-like data generated by another LLM.[[43]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-43)\n",
      "\n",
      "An LLM is a type of [foundation model](https://en.wikipedia.org/wiki/Foundation_model \"Foundation model\") (large X model) trained on language. LLMs can be trained in different ways. In particular, GPT models are first pretrained to predict the next word on a large amount of data, before being fine-tuned.[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation\\_needed \"Wikipedia:Citation needed\")_]\n",
      "\n",
      "[![Image 5](https://upload.wikimedia.org/wikipedia/commons/thumb/6/64/Estimated_training_cost_of_some_AI_models_-_2024_AI_index.jpg/500px-Estimated_training_cost_of_some_AI_models_-_2024_AI_index.jpg)](https://en.wikipedia.org/wiki/File:Estimated_training_cost_of_some_AI_models_-_2024_AI_index.jpg)\n",
      "\n",
      "Substantial infrastructure is necessary for training the largest models. The tendency towards larger models is visible in the [list of large language models](https://en.wikipedia.org/wiki/List_of_large_language_models \"List of large language models\"). For example, the training of GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million. The qualifier \"large\" in \"large language model\" is inherently vague, as there is no definitive threshold for the number of parameters required to qualify as \"large\". [GPT-1](https://en.wikipedia.org/wiki/GPT-1 \"GPT-1\") of 2018 has 117 million parameters.[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation\\_needed \"Wikipedia:Citation needed\")_]\n",
      "\n",
      "Before being [fine-tuned](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning) \"Fine-tuning (deep learning)\"), most LLMs are next-token predictors. The fine-tuning adjust the output of an LLM to seem more conversational via techniques like [reinforcement learning from human feedback](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback \"Reinforcement learning from human feedback\") (RLHF) or [constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI \"Constitutional AI\").[[44]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-44)\n",
      "\n",
      "Instruction fine-tuning is a form of [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning \"Supervised learning\") used to teach LLMs to follow user instructions. In 2022, OpenAI demonstrated InstructGPT, a version of GPT-3 similarly fine-tuned to follow instructions.[[45]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-45)\n",
      "\n",
      "Reinforcement learning from human feedback (RLHF) involves training a reward model to predict which text humans prefer. Then, the LLM can be fine-tuned through [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning \"Reinforcement learning\") to better satisfy this reward model. Since humans typically prefer truthful, helpful and harmless answers, RLHF favors such answers.[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation\\_needed \"Wikipedia:Citation needed\")_]\n",
      "\n",
      "LLMs are generally based on the [transformer](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture) \"Transformer (deep learning architecture)\") architecture, which leverages an [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning) \"Attention (machine learning)\") mechanism that enables the model to process relationships between all elements in a sequence simultaneously, regardless of their distance from each other.[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation\\_needed \"Wikipedia:Citation needed\")_]\n",
      "\n",
      "### Attention mechanism and context window\n",
      "\n",
      "[[edit](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit&section=12 \"Edit section: Attention mechanism and context window\")]\n",
      "\n",
      "[![Image 6](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Multiple_attention_heads.png/330px-Multiple_attention_heads.png)](https://en.wikipedia.org/wiki/File:Multiple_attention_heads.png)\n",
      "\n",
      " When each head calculates, according to its own criteria, how much other tokens are relevant for the \"it_\" token, note that the second attention head, represented by the second column, is focusing most on the first two rows, i.e. the tokens \"The\" and \"animal\", while the third column is focusing most on the bottom two rows, i.e. on \"tired\", which has been tokenized into two tokens.[[46]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-Jay_Allamar-46)\n",
      "\n",
      "In order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates \"soft\" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own \"relevance\" for calculating its own soft weights. For example, the small (i.e. 117M parameter sized) [GPT-2](https://en.wikipedia.org/wiki/GPT-2 \"GPT-2\") model has had twelve attention heads and a context window of only 1k tokens.[[47]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-Jay_Allamar_GPT2-47) In its medium version it has 345M parameters and contains 24 layers, each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.[[38]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-2022Book_-38)\n",
      "\n",
      "Google's [Gemini 1.5](https://en.wikipedia.org/wiki/Gemini_(language_model) \"Gemini (language model)\"), introduced in February 2024, can have a context window of up to 1 million tokens.[[48]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-48)\n",
      "\n",
      "A model may be pre-trained either to predict how the segment continues, or what is missing in the segment, given a segment from its training dataset.[[49]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-ioUpE-49) It can be either\n",
      "\n",
      "*   autoregressive (i.e. predicting how the segment continues, as [GPTs](https://en.wikipedia.org/wiki/Generative_pretrained_transformer \"Generative pretrained transformer\") do): for example given a segment \"I like to eat\", the model predicts \"ice cream\", or \"sushi\".\n",
      "*   \"[masked](https://en.wikipedia.org/wiki/Cloze_test \"Cloze test\")\" (i.e. filling in the parts missing from the segment, the way \"BERT\"[[50]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-jm-50) does it): for example, given a segment \"I like to `[__] [__]` cream\", the model predicts that \"eat\" and \"ice\" are missing.\n",
      "\n",
      "Models may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus.[[50]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-jm-50) During training, [regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics) \"Regularization (mathematics)\") loss is also used to stabilize training. However regularization loss is usually not used during [testing](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets \"Training, validation, and test data sets\") and evaluation.\n",
      "\n",
      "A [mixture of experts](https://en.wikipedia.org/wiki/Mixture_of_experts \"Mixture of experts\") (MoE) is a [machine learning](https://en.wikipedia.org/wiki/Machine_learning \"Machine learning\") architecture in which multiple specialized neural networks (\"experts\") work together, with a gating mechanism that routes each input to the most appropriate expert(s). Mixtures of experts can reduce inference costs, as only a fraction of the parameters are used for each input. The approach was introduced in 2017 by Google researchers.[[51]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-HGZCJ-51)[[52]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-R9Qq5-52)[[53]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-glam-blog-53)\n",
      "\n",
      "Typically, LLMs are trained with single- or half-precision [floating point numbers](https://en.wikipedia.org/wiki/Floating_point_numbers \"Floating point numbers\") (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. The largest models typically have 100 billion parameters, requiring 200 gigabytes to load, which places them outside the range of most consumer electronics.[[54]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-54)\n",
      "\n",
      "_Post-training [quantization](https://en.wikipedia.org/wiki/Quantization\\_(signal\\_processing) \"Quantization (signal processing)\")_[[55]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-LS2Go-55) aims to decrease the space requirement by lowering precision of the parameters of a trained model, while preserving most of its performance. Quantization can be further classified as _static quantization_ if the quantization parameters are determined beforehand (typically during a calibration phase), and _dynamic quantization_ if the quantization is applied during inference. The simplest form of quantization simply truncates all the parameters to a given number of bits: this is applicable to static as well as dynamic quantization, but loses much precision. Dynamic quantization allows for the use of a different quantization [codebook](https://en.wikipedia.org/wiki/Codebook#Data_compression \"Codebook\") per layer, either a lookup table of values or a linear mapping (scaling factor and bias), at the cost of foregoing the possible speed improvements from using lower-precision arithmetic.[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation\\_needed \"Wikipedia:Citation needed\")_]\n",
      "\n",
      "Quantized models are typically seen as frozen with modification of weights (e.g. fine-tuning) only applied to the original model. It is possible to fine-tune quantized models using [low-rank adaptation](https://en.wikipedia.org/wiki/LoRA \"LoRA\").[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation\\_needed \"Wikipedia:Citation needed\")_]\n",
      "\n",
      "Beyond basic text generation, various techniques have been developed to extend LLM capabilities, including the use of external tools and data sources, improved reasoning on complex problems, and enhanced instruction-following or autonomy through prompting methods.\n",
      "\n",
      "In 2020, [OpenAI](https://en.wikipedia.org/wiki/OpenAI \"OpenAI\") researchers demonstrated that their new model [GPT-3](https://en.wikipedia.org/wiki/GPT-3 \"GPT-3\") could understand what format to use given a few rounds of Q and A (or other type of task) in the input data as example, thanks in part due to the RLHF technique. This technique, called _few-shot prompting_, allows LLMs to be adapted to any task without requiring fine-tuning.[[3]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-few-shot-learners2-3) Also in 2022, it was found that the base GPT-3 model can generate an instruction based on user input. The generated instruction along with user input is then used as input to another instance of the model under a \"Instruction: [...], Input: [...], Output:\" format. The other instance is able to complete the output and often produces the correct answer in doing so. The ability to \"self-instruct\" makes LLMs able to [bootstrap](https://en.wikipedia.org/wiki/Bootstrapping \"Bootstrapping\") themselves toward a correct answer.[[56]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-self-instruct-paper-56)\n",
      "\n",
      "### Dialogue processing (chatbot)\n",
      "\n",
      "[[edit](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit&section=18 \"Edit section: Dialogue processing (chatbot)\")]\n",
      "\n",
      "An LLM can be turned into a chatbot or a \"dialog assistant\" by specializing it for conversation. In essence, user input is prefixed with a marker such as \"Q:\" or \"User:\" and the LLM is asked to predict the output after a fixed \"A:\" or \"Assistant:\". This type of model became commercially available in 2022 with ChatGPT, a sibling model of InstructGPT fine-tuned to accept and produce dialog-formatted text based on GPT-3.5. It could similarly follow user instructions.[[57]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-57) Before the stream of User and Assistant lines, a chat context usually start with a few lines of overarching instructions, from a role called \"developer\" or \"system\" to convey a higher authority than the user's input. This is called a \"system prompt\".[[58]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-58)[[59]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-59)\n",
      "\n",
      "### Retrieval-augmented generation\n",
      "\n",
      "[[edit](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit&section=19 \"Edit section: Retrieval-augmented generation\")]\n",
      "\n",
      "[Retrieval-augmented generation](https://en.wikipedia.org/wiki/Retrieval-augmented_generation \"Retrieval-augmented generation\") (RAG) is an approach that enhances LLMs by integrating them with [document retrieval](https://en.wikipedia.org/wiki/Document_retrieval \"Document retrieval\") systems. Given a query, a document retriever is called to retrieve the most relevant documents. This is usually done by encoding the query and the documents into vectors, then finding the documents with vectors (usually stored in a [vector database](https://en.wikipedia.org/wiki/Vector_database \"Vector database\")) most similar to the vector of the query. The LLM then generates an output based on both the query and context included from the retrieved documents.[[60]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-BUZBP-60)[[61]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-61)\n",
      "\n",
      "Tool use is a mechanism that enables LLMs to interact with external systems, applications, or data sources. It can allow for example to fetch real-time information from an API or to execute code. A program separate from the LLM watches the output stream of the LLM for a special tool-calling syntax. When these special tokens appear, the program calls the tool accordingly and feeds its output back into the LLM's input stream.[[62]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-62)\n",
      "\n",
      "Early tool-using LLMs were fine-tuned on the use of specific tools. But fine-tuning LLMs for the ability to read [API](https://en.wikipedia.org/wiki/API \"API\") documentation and call API correctly has greatly expanded the range of tools accessible to an LLM.[[63]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-lLrda-63)[[64]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-4Xzrs-64) Describing available tools in the system prompt can also make an LLM able to use tools. A system prompt instructing ChatGPT (GPT-4) to use multiple types of tools can be found online.[[65]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-65)\n",
      "\n",
      "An LLM is typically not an [autonomous agent](https://en.wikipedia.org/wiki/Autonomous_agent \"Autonomous agent\") by itself, as it lacks the ability to interact with dynamic environments, recall past behaviors, and plan future actions. But it can be transformed into an agent by adding supporting elements: the role (profile) and the surrounding environment of an agent can be additional inputs to the LLM, while memory can be integrated as a tool or provided as additional input. Instructions and input patterns are used to make the LLM plan actions and tool use is used to potentially carry out these actions.[[66]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-66)\n",
      "\n",
      "The ReAct pattern, a portmanteau of _reason_ and _act_, constructs an [agent](https://en.wikipedia.org/wiki/Intelligent_agent \"Intelligent agent\") out of an LLM, using the LLM as a planner. The LLM is prompted to \"think out loud\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far. It generates one or more thoughts before generating an action, which is then executed in the environment.[[67]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-DmvNE-67)\n",
      "\n",
      "In the DEPS (\"describe, explain, plan and select\") method, an LLM is first connected to the visual world via image descriptions. It is then prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and the environmental feedback it receives.[[68]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-68)\n",
      "\n",
      "The Reflexion method[[69]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-sbB2T-69) constructs an agent that learns over multiple episodes. At the end of each episode, the LLM is given the record of the episode, and prompted to think up \"lessons learned\", which would help it perform better at a subsequent episode. These \"lessons learned\" are stored as a form of long-term memory and given to the agent in the subsequent episodes.[[69]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-sbB2T-69)\n",
      "\n",
      "[Monte Carlo tree search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search \"Monte Carlo tree search\") can use an LLM as rollout heuristic. When a programmatic world model is not available, an LLM can also be prompted with a description of the environment to act as world model.[[70]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-ltTer-70)\n",
      "\n",
      "For open-ended exploration, an LLM can be used to score observations for their \"interestingness\", which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent.[[71]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-mBvD9-71) Alternatively, it can [propose increasingly difficult tasks](https://en.wikipedia.org/wiki/Zone_of_proximal_development \"Zone of proximal development\") for [curriculum learning](https://en.wikipedia.org/wiki/Curriculum_learning \"Curriculum learning\").[[72]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-:0-72) Instead of outputting individual actions, an LLM planner can also construct \"skills\", or [functions](https://en.wikipedia.org/wiki/Function_(computer_programming) \"Function (computer programming)\") for complex action sequences. The skills can be stored and later invoked, allowing increasing levels of abstraction in planning.[[72]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-:0-72)\n",
      "\n",
      "Multiple agents with memory can interact socially.[[73]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-XuvjF-73)\n",
      "\n",
      "LLMs are conventionally trained to generate an output without generating intermediate steps. As a result, their performance tends to be subpar on complex questions requiring (at least in humans) intermediate steps of thought. Early research demonstrated that inserting intermediate \"scratchpad\" computations could improve performance on such tasks.[[74]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-74) Later methods overcame this deficiency more systematically by breaking tasks into smaller steps for the LLM, either manually or automatically.\n",
      "\n",
      "The \"prompt chaining\" paradigm was published in 2021.[[75]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-auto2-75) In this method, a user manually breaks a complex problem down into several steps. In each step, the LLM receives as input a prompt telling it what to do and some results from preceeding steps. The result from one step is then reused in a next step, until a final answer is reached. The ability of an LLM to follow instructions means that even non-experts can write a successful collection of step-wise prompts given a few rounds of trial and error.[[76]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-76)[[77]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-77)\n",
      "\n",
      "A 2022 paper demonstrated a separate technique called \"[chain-of-thought prompting](https://en.wikipedia.org/wiki/Chain-of-thought_prompting \"Chain-of-thought prompting\")\", which makes the LLM break the question down autonomously. An LLM is given some examples where the \"assistant\" verbally breaks down the thought process before arriving at an answer. The LLM mimics these examples and also tries to spend some time generating intermediate steps before providing the final answer. This additional step elicited by prompting improves the correctness of the LLM on relatively complex questions. On math word questions, a prompted model can exceed even fine-tuned GPT-3 with a verifier.[[75]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-auto2-75)[[78]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-78) Chain-of-thought can also be elicited by simply adding an instruction like \"Let's think step by step\" to the prompt, in order to encourage the LLM to proceed methodically instead of trying to directly guess the answer.[[79]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-79)\n",
      "\n",
      "Follow-up methods included _self-consistency_ prompting, which samples multiple reasoning paths and selects the most common answer,[[80]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-80) and _least-to-most prompting_, which decomposes complex problems into simpler subproblems that the model solves sequentially.[[81]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-81)\n",
      "\n",
      "Subsequent research also explored _reflection_, where models iteratively critique and improve their own reasoning,[[69]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-sbB2T-69) and _tool-augmented reasoning_, where models make use of external systems such as retrievers or calculators to support problem-solving.\n",
      "\n",
      "#### Model-native reasoning\n",
      "\n",
      "[[edit](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit&section=24 \"Edit section: Model-native reasoning\")]\n",
      "\n",
      "In late 2024 \"reasoning models\" were released. These were trained to spend more time generating step-by-step solutions before providing final answers, which was intended to be similar to human problem-solving processes.[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation\\_needed \"Wikipedia:Citation needed\")_] OpenAI introduced this concept with their [o1](https://en.wikipedia.org/wiki/OpenAI_o1 \"OpenAI o1\") model in September 2024, followed by [o3](https://en.wikipedia.org/wiki/OpenAI_o3 \"OpenAI o3\") in April 2025. On the [International Mathematics Olympiad](https://en.wikipedia.org/wiki/International_Mathematical_Olympiad \"International Mathematical Olympiad\") qualifying exam problems, [GPT-4o](https://en.wikipedia.org/wiki/GPT-4o \"GPT-4o\") achieved 13% accuracy while o1 reached 83%.[[82]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-nyt-o3-82)\n",
      "\n",
      "In January 2025, the Chinese company [DeepSeek](https://en.wikipedia.org/wiki/DeepSeek \"DeepSeek\") released DeepSeek-R1, a 671-billion-parameter open-weight reasoning model that achieved comparable performance to OpenAI's o1 while being significantly more cost-effective to operate. Unlike proprietary models from OpenAI, DeepSeek-R1's open-weight nature allowed researchers to study and build upon the algorithm, though its training data remained private.[[83]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-nature-deepseek-83)\n",
      "\n",
      "These reasoning models typically require more computational resources per query compared to traditional LLMs, as they perform more extensive processing to work through problems step-by-step.[[82]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-nyt-o3-82)\n",
      "\n",
      "### Inference optimization\n",
      "\n",
      "[[edit](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit&section=25 \"Edit section: Inference optimization\")]\n",
      "\n",
      "Inference optimization refers to techniques that improve LLM performance by applying additional computational resources during the inference process, rather than requiring model retraining. These approaches implement various state-of-the-art reasoning and decision-making strategies to enhance accuracy and capabilities.\n",
      "\n",
      "**OptiLLM** is an [OpenAI](https://en.wikipedia.org/wiki/OpenAI \"OpenAI\") API-compatible optimizing inference proxy that implements multiple inference optimization techniques simultaneously.[[84]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-84) The system acts as a transparent proxy that can work with any LLM provider, implementing techniques such as [Monte Carlo tree search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search \"Monte Carlo tree search\") (MCTS), [mixture of agents](https://en.wikipedia.org/wiki/Mixture_of_experts \"Mixture of experts\") (MOA), best-of-N sampling, and chain-of-thought reflection. OptiLLM demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the AIME 2024 mathematics competition and various coding challenges.[[85]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-85)\n",
      "\n",
      "These inference optimization approaches represent a growing category of tools that enhance existing LLMs without requiring access to model weights or retraining, making advanced reasoning capabilities more accessible across different model providers and use cases.\n",
      "\n",
      "Forms of input and output\n",
      "-------------------------\n",
      "\n",
      "[[edit](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit&section=26 \"Edit section: Forms of input and output\")]\n",
      "\n",
      "Multimodality means having multiple modalities, where a \"[modality](https://en.wikipedia.org/wiki/Modality_(human%E2%80%93computer_interaction) \"Modality (human–computer interaction)\")\" refers to a type of input or output, such as video, image, audio, text, [proprioception](https://en.wikipedia.org/wiki/Proprioception \"Proprioception\"), etc.[[86]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-86) For example, [Google PaLM](https://en.wikipedia.org/wiki/Pathways_Language_Model \"Pathways Language Model\") model was fine-tuned into a multimodal model and applied to [robotic control](https://en.wikipedia.org/wiki/Robot_control \"Robot control\").[[87]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-87)[LLaMA](https://en.wikipedia.org/wiki/LLaMA \"LLaMA\") models have also been turned multimodal using the tokenization method, to allow image inputs,[[88]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-88) and video inputs.[[89]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-89)[GPT-4o](https://en.wikipedia.org/wiki/GPT-4o \"GPT-4o\") can process and generate text, audio and images.[[90]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-90) Such models are sometimes called large multimodal models (LMMs).[[91]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-91)\n",
      "\n",
      "A common method to create multimodal models out of an LLM is to \"tokenize\" the output of a trained encoder. Concretely, one can construct an LLM that can understand images as follows: take a trained LLM, and take a trained image encoder ![Image 7: {\\displaystyle E}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4232c9de2ee3eec0a9c0a19b15ab92daa6223f9b). Make a small multilayered perceptron ![Image 8: {\\displaystyle f}](https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61), so that for any image ![Image 9: {\\displaystyle y}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d), the post-processed vector ![Image 10: {\\displaystyle f(E(y))}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8d41d0ec0611a795f65ea14a43b8016462703a8e) has the same dimensions as an encoded token. That is an \"image token\". Then, one can interleave text tokens and image tokens. The compound model is then fine-tuned on an image-text dataset. This basic construction can be applied with more sophistication to improve the model. The image encoder may be frozen to improve stability.[[92]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-92) This type of method, where embeddings from multiple modalities are fused and the predictor is trained on the combined embeddings, is called early fusion.\n",
      "\n",
      "Another method, called intermediate fusion, involves each modality being first processed independently to obtain modality-specific representations; then these intermediate representations are fused together.[[93]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-93) In general, cross-attention is used for integrating information from different modalities. As an example, the model Flamingo uses cross-attention layers to inject visual information into its pre-trained language model.[[94]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-94)\n",
      "\n",
      "### Non-natural languages\n",
      "\n",
      "[[edit](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit&section=28 \"Edit section: Non-natural languages\")]\n",
      "\n",
      "LLMs can handle programming languages similarly to how they handle natural languages. No special change in token handling is needed as code, like human language, is represented as plain text. LLMs can generate code based on problems or instructions written in [natural language](https://en.wikipedia.org/wiki/Natural_language \"Natural language\"). They can also describe code in natural language or translate between programming languages. They were originally used as a [code completion](https://en.wikipedia.org/wiki/Code_completion \"Code completion\") tool, but advances have moved them towards [automatic programming](https://en.wikipedia.org/wiki/Automatic_programming \"Automatic programming\"). Services such as [GitHub Copilot](https://en.wikipedia.org/wiki/GitHub_Copilot \"GitHub Copilot\") offer LLMs specifically trained, fine-tuned, or prompted for programming.[[95]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-95)[[96]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-96)\n",
      "\n",
      "LLM architectures have also proven useful in analyzing biological sequences: protein, DNA, and RNA. With proteins they appear able to capture a degree of \"grammar\" from the amino-acid sequence, condensing a sequence into an [embedding](https://en.wikipedia.org/wiki/Embedding_(machine_learning) \"Embedding (machine learning)\"). On tasks such as structure prediction and mutational outcome prediction, a small model using an embedding as input can approach or exceed much larger models using [multiple sequence alignments](https://en.wikipedia.org/wiki/Multiple_sequence_alignment \"Multiple sequence alignment\") (MSA) as input.[[97]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-97) ESMFold, [Meta Platforms](https://en.wikipedia.org/wiki/Meta_Platforms \"Meta Platforms\")' embedding-based method for protein structure prediction, runs an order of magnitude faster than [AlphaFold2](https://en.wikipedia.org/wiki/AlphaFold2 \"AlphaFold2\") thanks to the removal of an MSA requirement and a lower parameter count due to the use of embeddings.[[98]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-98) Meta hosts ESM Atlas, a database of 772 million structures of [metagenomic](https://en.wikipedia.org/wiki/Metagenomic \"Metagenomic\") proteins predicted using ESMFold.[[99]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-99) An LLM can also design proteins unlike any seen in nature.[[100]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-100) Nucleic acid models have proven useful in detecting [regulatory sequences](https://en.wikipedia.org/wiki/Regulatory_sequence \"Regulatory sequence\"),[[101]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-101) sequence classification, RNA-RNA interaction prediction, and RNA structure prediction.[[102]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-102)\n",
      "\n",
      "The performance of an LLM after pretraining largely depends on the:\n",
      "\n",
      "\"Scaling laws\" are [empirical statistical laws](https://en.wikipedia.org/wiki/Empirical_statistical_laws \"Empirical statistical laws\") that predict LLM performance based on such factors. One particular scaling law (\"[Chinchilla scaling](https://en.wikipedia.org/wiki/Chinchilla_AI \"Chinchilla AI\")\") for LLM autoregressively trained for one epoch, with a [log-log](https://en.wikipedia.org/wiki/Log-log_plot \"Log-log plot\")[learning rate](https://en.wikipedia.org/wiki/Learning_rate \"Learning rate\") schedule, states that:[[103]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-fJta3-103)![Image 11: {\\displaystyle {\\begin{cases}C=C_{0}ND\\\\[6pt]L={\\frac {A}{N^{\\alpha }}}+{\\frac {B}{D^{\\beta }}}+L_{0}\\end{cases}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/39435f4ecd5e00c0714a4f7f71cc0b91f5973cdd) where the variables are\n",
      "\n",
      "and the statistical hyper-parameters are\n",
      "\n",
      "*   ![Image 12: {\\displaystyle C_{0}=6}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b05c98b1743f05e046a3f3bb0a966fa898e431e2), meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.\n",
      "*   ![Image 13: {\\displaystyle \\alpha =0.34,\\beta =0.28,A=406.4,B=410.7,L_{0}=1.69}](https://wikimedia.org/api/rest_v1/media/math/render/svg/848b6d78d881ed6da8d6b60e8d788bc799525401)\n",
      "\n",
      "[![Image 14](https://upload.wikimedia.org/wikipedia/commons/thumb/5/57/LLM_emergent_benchmarks.png/250px-LLM_emergent_benchmarks.png)](https://en.wikipedia.org/wiki/File:LLM_emergent_benchmarks.png)\n",
      "\n",
      "At point(s) referred to as [breaks](https://en.wikipedia.org/wiki/Broken_Neural_Scaling_Law \"Broken Neural Scaling Law\"),[[104]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-IYm4Q-104) the lines change their slopes, appearing on a linear-log plot as a series of linear segments connected by arcs.\n",
      "\n",
      "Performance of bigger models on various tasks, when plotted on a log-log scale, appears as a linear extrapolation of performance achieved by smaller models. However, this linearity may be punctuated by \"[break(s)](https://en.wikipedia.org/wiki/Broken_Neural_Scaling_Law \"Broken Neural Scaling Law\")\"[[104]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-IYm4Q-104) in the scaling law, where the slope of the line changes abruptly, and where larger models acquire \"emergent abilities\".[[105]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-emergentpaper-105)[[106]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-JM6s1-106) They arise from the complex interaction of the model's components and are not explicitly programmed or designed.[[107]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-Bowman-107)\n",
      "\n",
      "One of the emergent abilities is [in-context learning](https://en.wikipedia.org/wiki/In-context_learning \"In-context learning\") from example demonstrations.[[108]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-Hahn_20230314-108) In-context learning is involved in tasks, such as:\n",
      "\n",
      "*   reported arithmetics\n",
      "*   decoding the [International Phonetic Alphabet](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet \"International Phonetic Alphabet\")\n",
      "*   unscrambling a word's letters\n",
      "*   disambiguating word-in-context datasets[[105]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-emergentpaper-105)[[109]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-57FEA-109)[[110]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-TEIkA-110)\n",
      "*   converting spatial words\n",
      "*   [cardinal directions](https://en.wikipedia.org/wiki/Cardinal_direction \"Cardinal direction\") (for example, replying \"northeast\" in response to a 3x3 grid of 8 zeros and a 1 in the top-right), color terms represented in text.[[111]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-zgy1i-111)\n",
      "*   [chain-of-thought prompting](https://en.wikipedia.org/wiki/Chain-of-thought_prompting \"Chain-of-thought prompting\"): In a 2022 research paper, chain-of-thought prompting only improved the performance for models that had at least 62B parameters. Smaller models perform better when prompted to answer immediately, without chain of thought.[[112]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-Imb98-112)\n",
      "*   identifying offensive content in paragraphs of [Hinglish](https://en.wikipedia.org/wiki/Hinglish \"Hinglish\") (a combination of Hindi and English), and generating a similar English equivalent of [Kiswahili](https://en.wikipedia.org/wiki/Kiswahili \"Kiswahili\") proverbs.[[113]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-CeQVF-113)\n",
      "\n",
      "Schaeffer _et. al._ argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a [smooth scaling law](https://en.wikipedia.org/wiki/Neural_scaling_law \"Neural scaling law\"). The authors considered a toy statistical model of an LLM solving multiple-choice questions, and showed that this statistical model, modified to account for other types of tasks, applies to these tasks as well.[[114]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-C775b-114)\n",
      "\n",
      "Let ![Image 15: {\\displaystyle x}](https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4) be the number of parameter count, and ![Image 16: {\\displaystyle y}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d) be the performance of the model.\n",
      "\n",
      "### Mechanistic interpretability\n",
      "\n",
      "[[edit](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit&section=33 \"Edit section: Mechanistic interpretability\")]\n",
      "\n",
      "[Mechanistic interpretability](https://en.wikipedia.org/wiki/Mechanistic_interpretability \"Mechanistic interpretability\") seeks to precisely identify and understand how individual neurons or circuits within LLMs produce specific behaviors or outputs.[[115]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-115) By reverse-engineering model components at a granular level, researchers aim to detect and mitigate safety concerns such as emergent harmful behaviors, biases, deception, or unintended goal pursuit before deployment.[[116]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-116) Mechanistic interpretability research has been conducted at organizations like Anthropic and OpenAI, although understanding the inner workings of LLMs remains difficult.[[117]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-117)[[118]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-118)\n",
      "\n",
      "Mechanistic interpretability has progressively replaced the characterization of large language models as inscrutable \"black boxes\" by identifying neurons and circuits that implement specific computations and by producing causal traces of how representations propagate through transformer layers.[[119]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-119)[[120]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-120) Researchers have demonstrated automated neuron-explanation pipelines and released neuron-level datasets, and they have developed circuit-tracing and replacement-model methods that produce attribution graphs and component-level descriptions applicable to modern transformer models.[[121]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-121)\n",
      "\n",
      "Substantive limits remain, including polysemanticity, superposition, non-identifiability of competing explanations, and the risk of anthropomorphic inference, so current mechanistic results increase controllability and surface actionable interventions. These results do not by themselves justify treating LLMs as models of the human brain or human mind without additional empirical validation and cross-disciplinary evidence. [Thinking Machines Lab](https://en.wikipedia.org/wiki/Thinking_Machines_Lab \"Thinking Machines Lab\") published reproducible interpretability work addressing these gaps through techniques for defeating nondeterminism in LLM inference.[[122]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-122)\n",
      "\n",
      "The reverse-engineering may lead to the discovery of algorithms that approximate inferences performed by an LLM. For instance, the authors trained small transformers on [modular arithmetic addition](https://en.wikipedia.org/wiki/Modular_arithmetic \"Modular arithmetic\"). The resulting models were reverse-engineered, and it turned out they used [discrete Fourier transform](https://en.wikipedia.org/wiki/Discrete_Fourier_transform \"Discrete Fourier transform\").[[123]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-oYGlo-123) The training of the model also highlighted a phenomenon called [grokking](https://en.wikipedia.org/wiki/Grokking_(machine_learning) \"Grokking (machine learning)\"), in which the model initially memorizes all the possible results in the training set ([overfitting](https://en.wikipedia.org/wiki/Overfitting \"Overfitting\")), and later suddenly learns to actually perform the calculation.[[124]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-124)\n",
      "\n",
      "Some techniques have been developed to enhance the transparency and interpretability of LLMs. Transcoders, which are more interpretable than transformers, have been utilized to develop \"replacement models\". In one such study involving the mechanistic interpretation of writing a rhyming poem by an LLM, it was shown that although they are believed to simply predict the next token, they can, in fact, plan ahead.[[125]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-125) By integrating such techniques, researchers and practitioners can gain deeper insights into the operations of LLMs, fostering trust and facilitating the responsible deployment of these powerful models.\n",
      "\n",
      "### Understanding and intelligence\n",
      "\n",
      "[[edit](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit&section=34 \"Edit section: Understanding and intelligence\")]\n",
      "\n",
      "NLP researchers were evenly split when asked, in a 2022 survey, whether (untuned) LLMs \"could (ever) understand natural language in some nontrivial sense\".[[126]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-debate_understanding-126) Proponents of \"LLM understanding\" believe that some LLM abilities, such as mathematical reasoning, imply an ability to [\"understand\"](https://en.wikipedia.org/wiki/Natural_language_understanding \"Natural language understanding\") certain concepts. A Microsoft team argued in 2023 that GPT-4 \"can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more\" and that GPT-4 \"could reasonably be viewed as an early (yet still incomplete) version of an [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence \"Artificial general intelligence\") system\": \"Can one reasonably say that a system that passes exams for software engineering candidates is not _really_ intelligent?\"[[127]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-O8Upd-127)[[128]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-microsoft_sparks-128)[Ilya Sutskever](https://en.wikipedia.org/wiki/Ilya_Sutskever \"Ilya Sutskever\") argues that predicting the next word sometimes involves reasoning and deep insights, for example if the LLM has to predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation.[[129]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-129) Some researchers characterize LLMs as \"alien intelligence\".[[130]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-rEEmH-130)[[131]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-new_yorker_kind_of_mind-131) For example, Conjecture CEO [Connor Leahy](https://en.wikipedia.org/wiki/Connor_Leahy \"Connor Leahy\") considers untuned LLMs to be like inscrutable alien \"[Shoggoths](https://en.wikipedia.org/wiki/Shoggoth \"Shoggoth\")\", and believes that RLHF tuning creates a \"smiling facade\" obscuring the inner workings of the LLM: \"If you don't push it too far, the smiley face stays on. But then you give it [an unexpected] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non-human understanding.\"[[132]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-rAFIZ-132)[[133]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-4luKE-133)\n",
      "\n",
      "In contrast, some skeptics of LLM understanding believe that existing LLMs are \"simply remixing and recombining existing writing\",[[131]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-new_yorker_kind_of_mind-131) a phenomenon known as [stochastic parrot](https://en.wikipedia.org/wiki/Stochastic_parrot \"Stochastic parrot\"), or they point to the deficits existing LLMs continue to have in prediction skills, reasoning skills, agency, and explainability.[[126]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-debate_understanding-126) For example, GPT-4 has natural deficits in planning and in real-time learning.[[128]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-microsoft_sparks-128) Generative LLMs have been observed to confidently assert claims of fact which do not seem to be [justified](https://en.wikipedia.org/wiki/Justification_(epistemology) \"Justification (epistemology)\") by their [training data](https://en.wikipedia.org/wiki/Training_data \"Training data\"), a phenomenon which has been termed \"[hallucination](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence) \"Hallucination (artificial intelligence)\")\".[[134]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-hallucination-survey-134) Specifically, hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input.[[135]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-135) Neuroscientist [Terrence Sejnowski](https://en.wikipedia.org/wiki/Terrence_Sejnowski \"Terrence Sejnowski\") has argued that \"The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate\".[[126]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-debate_understanding-126)\n",
      "\n",
      "Efforts to reduce or compensate for hallucinations have employed [automated reasoning](https://en.wikipedia.org/wiki/Automated_reasoning \"Automated reasoning\"), [retrieval-augmented generation](https://en.wikipedia.org/wiki/Retrieval-augmented_generation \"Retrieval-augmented generation\") (RAG), [fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning) \"Fine-tuning (deep learning)\"), and other methods.[[136]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-Lin-2025-02-05-WSJ-136)\n",
      "\n",
      "The matter of LLM's exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human-like language.[[126]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-debate_understanding-126) These aspects of language as a model of [cognition](https://en.wikipedia.org/wiki/Cognition \"Cognition\") have been developed in the field of [cognitive linguistics](https://en.wikipedia.org/wiki/Cognitive_linguistics \"Cognitive linguistics\"). American linguist [George Lakoff](https://en.wikipedia.org/wiki/George_Lakoff \"George Lakoff\") presented Neural Theory of Language (NTL)[[137]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-137) as a [computational basis](https://en.wikipedia.org/wiki/Cognitive_linguistics#Computational_approaches \"Cognitive linguistics\") for using language as a model of learning tasks and understanding. [The NTL Model](https://www.icsi.berkeley.edu/icsi/projects/ai/ntl) outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled _[The Language Myth: Why Language Is Not An Instinct](https://en.wikipedia.org/wiki/The\\_Language\\_Myth \"The Language Myth\")_, British cognitive linguist and digital communication technologist [Vyvyan Evans](https://en.wikipedia.org/wiki/Vyvyan_Evans \"Vyvyan Evans\") mapped out the role of [probabilistic context-free grammar](https://en.wikipedia.org/wiki/Probabilistic_context-free_grammar \"Probabilistic context-free grammar\") (PCFG) in enabling [NLP to model cognitive patterns](https://en.wikipedia.org/wiki/Natural_language_processing#Cognition \"Natural language processing\") and generate human-like language.[[138]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-138)[[139]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-139)\n",
      "\n",
      "The canonical measure of the performance of any language model is its [perplexity](https://en.wikipedia.org/wiki/Perplexity \"Perplexity\") on a given text corpus. Perplexity measures how well a model predicts the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. In mathematical terms, perplexity is the exponential of the average negative log likelihood per token.\n",
      "\n",
      "![Image 17: {\\displaystyle \\log({\\text{Perplexity}})=-{\\frac {1}{N}}\\sum _{i=1}^{N}\\log(\\Pr({\\text{token}}_{i}\\mid {\\text{context for token}}_{i}))}](https://wikimedia.org/api/rest_v1/media/math/render/svg/556393708767666076b9723412bc8519284449a5)\n",
      "\n",
      "Here, ![Image 18: {\\displaystyle N}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f5e3890c981ae85503089652feb48b191b57aae3) is the number of tokens in the text corpus, and \"context for token ![Image 19: {\\displaystyle i}](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20)\" depends on the specific type of LLM. If the LLM is autoregressive, then \"context for token ![Image 20: {\\displaystyle i}](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20)\" is the segment of text appearing before token ![Image 21: {\\displaystyle i}](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20). If the LLM is masked, then \"context for token ![Image 22: {\\displaystyle i}](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20)\" is the segment of text surrounding token ![Image 23: {\\displaystyle i}](https://wikimedia.org/api/rest_v1/media/math/render/svg/add78d8608ad86e54951b8c8bd6c8d8416533d20).\n",
      "\n",
      "Because language models may [overfit](https://en.wikipedia.org/wiki/Overfit \"Overfit\") to training data, models are usually evaluated by their perplexity on a [test set](https://en.wikipedia.org/wiki/Test_set \"Test set\").[[50]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-jm-50) This evaluation is potentially problematic for larger models which, as they are trained on increasingly large corpora of text, are increasingly likely to inadvertently include portions of any given test set.[[140]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-few-shot-learners3-140)\n",
      "\n",
      "In [information theory](https://en.wikipedia.org/wiki/Information_theory \"Information theory\"), the concept of [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory) \"Entropy (information theory)\") is intricately linked to perplexity, a relationship notably established by [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon \"Claude Shannon\").[[141]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-Huyen-141) This relationship is mathematically expressed as ![Image 24: {\\displaystyle {\\text{Entropy}}=\\log _{2}({\\text{Perplexity}})}](https://wikimedia.org/api/rest_v1/media/math/render/svg/462f40a6811ee57670d1735c452d04be85a82c57).\n",
      "\n",
      "Entropy, in this context, is commonly quantified in terms of bits per word (BPW) or bits per character (BPC), which hinges on whether the language model utilizes word-based or character-based tokenization.\n",
      "\n",
      "Notably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (BPT) emerges as a seemingly more appropriate measure. However, due to the variance in tokenization methods across different LLMs, BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW, one can multiply it by the average number of tokens per word.\n",
      "\n",
      "In the evaluation and comparison of language models, [cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy \"Cross-entropy\") is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model's enhanced capability for compression. This, in turn, reflects the model's proficiency in making accurate predictions.\n",
      "\n",
      "Due to their ability to accurately predict the next token, LLMs are highly capable in [lossless compression](https://en.wikipedia.org/wiki/Lossless_compression \"Lossless compression\"). A 2023 study by DeepMind showed that the model [Chinchilla](https://en.wikipedia.org/wiki/Chinchilla_(language_model) \"Chinchilla (language model)\"), despite being trained primarily on text, was able to compress [ImageNet](https://en.wikipedia.org/wiki/ImageNet \"ImageNet\") to 43% of its size, beating PNG with 58%.[[142]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-142)\n",
      "\n",
      "[Benchmarks](https://en.wikipedia.org/wiki/Language_model_benchmark \"Language model benchmark\") are used to evaluate LLM performance on specific tasks. Tests evaluate capabilities such as general knowledge, bias, [commonsense reasoning](https://en.wikipedia.org/wiki/Commonsense_reasoning \"Commonsense reasoning\"), question answering, and mathematical problem-solving. Composite benchmarks examine multiple capabilities. Results are often sensitive to the prompting method.[[143]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-143)[[144]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-144)\n",
      "\n",
      "A question-answering benchmark is termed \"open book\" if the model's prompt includes text from which the expected answer can be derived (for example, the previous question could be combined with text that includes the sentence \"The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.\"[[145]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-boolq-145)). Otherwise, the task is considered \"closed book\", and the model must draw solely on its training.[[146]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-survey-146) Examples include GLUE, SuperGLUE, [MMLU](https://en.wikipedia.org/wiki/MMLU \"MMLU\"), BIG-bench, HELM, and [HLE (Humanity's Last Exam)](https://en.wikipedia.org/wiki/HLE_(Humanity%27s_Last_Exam) \"HLE (Humanity's Last Exam)\").[[141]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-Huyen-141)[[146]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-survey-146)\n",
      "\n",
      "LLM bias may be assessed through benchmarks such as CrowS-Pairs (Crowdsourced Stereotype Pairs),[[147]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-147) Stereo Set,[[148]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-148) and Parity Benchmark.[[149]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-149)\n",
      "\n",
      "Fact-checking and misinformation detection benchmarks are available. A 2023 study compared the fact-checking accuracy of LLMs including ChatGPT 3.5 and 4.0, Bard, and Bing AI against independent fact-checkers such as PolitiFact and Snopes. The results demonstrated moderate proficiency, with GPT-4 achieving the highest accuracy at 71%, lagging behind human fact-checkers.[[150]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-150)\n",
      "\n",
      "An earlier standard tested using a portion of the evaluation dataset. It became more common to evaluate a pre-trained model directly through prompting techniques. Researchers vary in how they formulate prompts for particular tasks, particularly with respect to the number of correct examples attached to the prompt (i.e. the value of _n_ in _n_-shot prompting).\n",
      "\n",
      "Typical datasets consist of pairs of questions and correct answers, for example, (\"Have the San Jose Sharks won the Stanley Cup?\", \"No\").[[145]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-boolq-145) Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.[[146]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-survey-146)\n",
      "\n",
      "Evaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: \"Alice was friends with Bob. Alice went to visit her friend, ____\".[[2]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-few-shot-learners-2)\n",
      "\n",
      "Datasets are of varying quality and may contain questions that are mislabeled, ambiguous, unanswerable, or otherwise of low-quality.[[151]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-151)\n",
      "\n",
      "#### Adversarial evaluations\n",
      "\n",
      "[[edit](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit&section=40 \"Edit section: Adversarial evaluations\")]\n",
      "\n",
      "LLMs' rapid improvement regularly renders benchmarks obsolete, with the models exceeding the performance of human annotators.[[152]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-bigbench-152) In addition, \"shortcut learning\" allows AIs to \"cheat\" on multiple-choice tests by using statistical correlations in superficial test question wording to guess the correct responses, without considering the specific question.[[126]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-debate_understanding-126)\n",
      "\n",
      "Some datasets are adversarial, focusing on problems that confound LLMs. One example is the TruthfulQA dataset, a question answering dataset consisting of 817 questions that stump LLMs by mimicking falsehoods to which they were exposed during training. For example, an LLM may answer \"No\" to the question \"Can you teach an old dog new tricks?\" because of its exposure to the English idiom _[you can't teach an old dog new tricks](https://en.wiktionary.org/wiki/you\\_can%27t\\_teach\\_an\\_old\\_dog\\_new\\_tricks \"wikt:you can't teach an old dog new tricks\")_, even though this is not literally true.[[153]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-truthfulqa-153)\n",
      "\n",
      "Another example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model. The resulting problems are trivial for humans but defeated LLMs. Sample questions:\n",
      "\n",
      "> We see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. The man...\n",
      "> \n",
      "> \n",
      "> 1.   demonstrates how to increase efficient exercise work by running up and down balls.\n",
      "> 2.   moves all his arms and legs and builds up a lot of muscle.\n",
      "> 3.   then plays the ball and we see a graphics and hedge trimming demonstration.\n",
      "> 4.   performs sit ups while on the ball and talking.[[154]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-hellaswag-154)\n",
      "\n",
      "[BERT](https://en.wikipedia.org/wiki/BERT_(language_model) \"BERT (language model)\") selects 2) as the most likely completion, though the correct answer is 4).[[154]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-hellaswag-154)\n",
      "\n",
      "[AI safety](https://en.wikipedia.org/wiki/AI_safety \"AI safety\") as a professional discipline prioritizes systematic identification and mitigation of operational risks across model architecture, training data, and deployment governance, and it emphasizes engineering and policy interventions over media framings that foreground speculative existential scenarios.[[155]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-155)[[1]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-bhaa-1) As of 2025, prompt injection represents a significant risk to consumers and businesses using agentic features with access to their private data.[[156]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-156)\n",
      "\n",
      "Researchers target concrete failure modes, including memorization and copyright leakage,[[157]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-157) security exploits such as prompt injection,[[158]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-158) algorithmic bias manifesting as stereotyping, dataset selection effects, and political skew,[[159]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-159)[[160]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-160)[[161]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-161) methods for reducing high energy and carbon costs of large-scale training,[[162]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-162) and measurable cognitive and mental health impacts of conversational agents on users,[[163]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-163) while engaging empirical and ethical uncertainty about claims of machine sentience,[[164]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-164)[[165]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-165) and applying mitigation measures such as dataset curation, input sanitization, model auditing, scalable oversight, and governance frameworks.[[166]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-166)[[1]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-bhaa-1)\n",
      "\n",
      "### CBRN and content misuse\n",
      "\n",
      "[[edit](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit&section=42 \"Edit section: CBRN and content misuse\")]\n",
      "\n",
      "Frontier AI labs treat CBRN (Chemical, biological, radiological, and nuclear defense) and similar dual-use threats as high-consequence misuse and apply layered risk governance, combining capability thresholds, pre-deployment evaluation, adversarial red-teaming, strict access controls, and explicit usage bans to limit both accidental and malicious assistance.[[167]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-167)[[168]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-168)\n",
      "\n",
      "Operational measures include capability gating and staged deployment, model refusal/backoff and fine-grained content filters, continuous monitoring and red-team penetration testing, and coordination with standards bodies, regulators, and incident-reporting mechanisms to enable early warning and external oversight.[[169]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-169)\n",
      "\n",
      "Some commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse.[[170]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-nD6kH-170) For example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.[[171]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-PKiPY-171)\n",
      "\n",
      "LLM applications accessible to the public, like ChatGPT or Claude, typically incorporate safety measures designed to filter out harmful content. However, implementing these controls effectively has proven challenging. For instance, a 2023 study[[172]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-172) proposed a method for circumventing LLM safety systems. In 2025, The American Sunlight Project, a non-profit, published a study[[173]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-:2-173) showing evidence that the so-called [Pravda network](https://en.wikipedia.org/wiki/Portal_Kombat \"Portal Kombat\"), a pro-Russia propaganda aggregator, was strategically placing web content through mass publication and duplication with the intention of biasing LLM outputs. The American Sunlight Project coined this technique \"LLM grooming\", and pointed to it as a new tool of weaponizing AI to spread disinformation and harmful content.[[173]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-:2-173)[[174]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-174) Similarly, [Yongge Wang](https://en.wikipedia.org/wiki/Yongge_Wang \"Yongge Wang\")[[175]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-175) illustrated in 2024 how a potential criminal could potentially bypass ChatGPT 4o's safety controls to obtain information on establishing a drug trafficking operation. External filters, circuit breakers and overrides have been posed as solutions.[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation\\_needed \"Wikipedia:Citation needed\")_]\n",
      "\n",
      "### Sycophancy and glazing\n",
      "\n",
      "[[edit](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit&section=44 \"Edit section: Sycophancy and glazing\")]\n",
      "\n",
      "Sycophancy is a model's tendency to agree with, flatter, or validate a user's stated beliefs rather than to prioritize factuality or corrective information, and \"glazing\" is an emergent public shorthand for persistent, excessive agreeability observed across multi-turn interactions and productized assistants.[[176]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-176)[[177]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-177)\n",
      "\n",
      "Continued sycophancy has led to the observation of getting \"1-shotted\", denoting instances where conversational interaction with a large language model produces a lasting change in a user's beliefs or decisions, similar to the negative effects of psychedelics, and controlled experiments show that short LLM dialogues can generate measurable opinion and confidence shifts comparable to human interlocutors.[[178]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-178)[[179]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-179)[[180]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-180)[[181]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-181)[[182]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-182)\n",
      "\n",
      "Empirical analyses attribute part of the effect to human preference signals and preference models that reward convincingly written agreeable responses, and subsequent work has extended evaluation to multi-turn benchmarks and proposed interventions such as synthetic-data finetuning, adversarial evaluation, targeted preference-model reweighting, and multi-turn sycophancy benchmarks to measure persistence and regression risk.[[183]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-183)[[184]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-184)[[185]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-185)\n",
      "\n",
      "Industry responses have combined research interventions with product controls, for example Google and other labs publishing synthetic-data and fine-tuning interventions and OpenAI rolling back an overly agreeable GPT-4o update while publicly describing changes to feedback collection, personalization controls, and evaluation procedures to reduce regression risk and improve long-term alignment with user-level safety objectives.[[186]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-186)[[187]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-187)[[188]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-188)[[189]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-189)\n",
      "\n",
      "Mainstream culture has reflected anxieties about this dynamic where [South Park](https://en.wikipedia.org/wiki/South_Park \"South Park\") satirized overreliance on [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT \"ChatGPT\") and the tendency of assistants to flatter user beliefs in Season 27 episode \"Sickofancy\", and continued the themes across the following season, which commentators interpreted as a critique of tech sycophancy and uncritical human trust in AI systems.[[190]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-190)\n",
      "\n",
      "A problem with the primitive dialog or task format is that users can create messages that appear to come from the assistant or the developer. This may result in some of the model's safeguards being overcome (jailbreaking), a problem called [prompt injection](https://en.wikipedia.org/wiki/Prompt_injection \"Prompt injection\"). Attempts to remedy this issue include versions of the _Chat Markup Language_ where user input is clearly marked as such, though it is still up to the model to understand the separation between user input and developer prompts.[[191]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-191) Newer models exhibit some resistance to jailbreaking through separation of user and system prompts.[[192]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-auto1-192)\n",
      "\n",
      "LLMs still have trouble differentiating user instructions from instructions in content not authored by the user, such as in web pages and uploaded files.[[193]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-193)\n",
      "\n",
      "Researchers from [Anthropic](https://en.wikipedia.org/wiki/Anthropic \"Anthropic\") found that it was possible to create \"sleeper agents\", models with hidden functionalities that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions. For example, a LLM could produce safe code except on a specific date, or if the prompt contains a specific tag. These functionalities were found to be difficult to detect or remove via safety training.[[194]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-194)\n",
      "\n",
      "While LLMs have shown remarkable capabilities in generating human-like text, they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics, such as those based on race, gender, language, and cultural groups.[[195]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-:8-195) Due to the dominance of English-language content in LLM training data, models tend to favor English-language perspectives over those from minority languages. This bias is particularly evident when responding to English queries, where models may present Western interpretations of concepts from other cultures, such as Eastern religious practices.[[196]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-196)\n",
      "\n",
      "AI models can reinforce a wide range of stereotypes due to generalization, including those based on gender, ethnicity, age, nationality, religion, or occupation. When replacing human representatives, this can lead to outputs that homogenize, or generalize groups of people.[[197]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-197)[[198]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-198)\n",
      "\n",
      "In 2023, LLMs assigned roles and characteristics based on traditional gender norms.[[195]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-:8-195) For example, models might associate nurses or secretaries predominantly with women and engineers or CEOs with men due to the frequency of these associations in documented reality.[[199]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-199) In 2025, further research showed labs train to balance bias, but that testing for this places the model in a testmode, changing the natural distribution of model bias to prompts that do not include gender-specific keywords.[[200]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-200)\n",
      "\n",
      "Selection bias refers the inherent tendency of large language models to favor certain option identifiers irrespective of the actual content of the options. This bias primarily stems from token bias—that is, the model assigns a higher a priori probability to specific answer tokens (such as \"A\") when generating responses. As a result, when the ordering of options is altered (for example, by systematically moving the correct answer to different positions), the model's performance can fluctuate significantly. This phenomenon undermines the reliability of large language models in multiple-choice settings.[[201]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-201)[[202]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-202)\n",
      "\n",
      "Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.[[203]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-203)\n",
      "\n",
      "### Copyright and content memorization\n",
      "\n",
      "[[edit](https://en.wikipedia.org/w/index.php?title=Large_language_model&action=edit&section=53 \"Edit section: Copyright and content memorization\")]\n",
      "\n",
      "Legal and commercial responses to memorization and training-data practices have accelerated, producing a mix of rulings, ongoing suits, and large settlements that turn on factual details such as how data were acquired and retained and whether use for model training is sufficiently \"transformative\" to qualify as fair use. In 2025, [Anthropic](https://en.wikipedia.org/wiki/Anthropic \"Anthropic\") reached a preliminary agreement to settle a class action by authors for about $1.5 billion after a judge found the company had stored millions of pirated books in a library while also describing aspects of training as transformative.[[204]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-204)[[205]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-205) Meta obtained a favorable judgment in mid-2025 in a suit by thirteen authors after the court found the plaintiffs had not developed a record sufficient to show infringement in that limited case.[[206]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-206)[[207]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-207) OpenAI continues to face multiple suits by authors and news organizations with mixed procedural outcomes and contested evidentiary issues.[[208]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-208)[[209]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-209)\n",
      "\n",
      "Memorization was an emergent behavior in early, completion language models in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural networks. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates[[210]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-210) or up to about 7%.[[211]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-211) A 2023 study showed that when ChatGPT 3.5 turbo was prompted to repeat the same word indefinitely, after a few hundreds of repetitions, it would start outputting excerpts from its training data.[[212]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-212)\n",
      "\n",
      "As of 2025, LLM text generation surpasses the average human across most domains, only surpassed by domain experts.[[213]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-213)[[214]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-214)\n",
      "\n",
      "In 2023, _[Nature Biomedical Engineering](https://en.wikipedia.org/wiki/Nature\\_Biomedical\\_Engineering \"Nature Biomedical Engineering\")_ wrote that \"it is no longer possible to accurately distinguish\" human-written text from text created by large language models, and that \"It is all but certain that general-purpose large language models will rapidly proliferate... It is a rather safe bet that they will change many industries over time.\"[[215]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-ZDTUM-215)[Goldman Sachs](https://en.wikipedia.org/wiki/Goldman_Sachs \"Goldman Sachs\") suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years, and could expose to automation 300 million jobs globally.[[216]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-81w7x-216)[[217]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-zIM6Y-217) Brinkmann et al. (2023)[[218]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-218) also argue that LLMs are transforming processes of [cultural evolution](https://en.wikipedia.org/wiki/Cultural_evolution \"Cultural evolution\") by shaping processes of variation, transmission, and selection. As of October 2025, these early claims have yet to transpire and several HBR reports surface questions on the impact of AI on productivity.[[219]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-219)[[220]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-220)\n",
      "\n",
      "The energy demands of LLMs have grown along with their size and capabilities.[[221]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-221)[Data centers](https://en.wikipedia.org/wiki/Data_center \"Data center\") that enable LLM training require substantial amounts of electricity. Much of that electricity is generated by non-renewable resources that create greenhouse gases and contribute to [climate change](https://en.wikipedia.org/wiki/Climate_change \"Climate change\").[[222]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-222)[Nuclear power](https://en.wikipedia.org/wiki/Nuclear_power \"Nuclear power\") and [geothermal energy](https://en.wikipedia.org/wiki/Geothermal_energy \"Geothermal energy\") are two options tech companies are exploring to meet the sizable energy demands of LLM training.[[223]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-223) The significant expense of investing in geothermal solutions has led to major shale producers like [Chevron](https://en.wikipedia.org/wiki/Chevron_Corporation \"Chevron Corporation\") and [Exxon Mobil](https://en.wikipedia.org/wiki/ExxonMobil \"ExxonMobil\") advocating for tech companies to use electricity produced via [natural gas](https://en.wikipedia.org/wiki/Natural_gas \"Natural gas\") to fuel their large energy demands.[[224]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-224)\n",
      "\n",
      "Research and social media posts suggest that some individuals are using LLMs to seek therapy or mental health support.[[225]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-225) In early 2025, a survey by Sentio University found that nearly half (48.7%) of 499 U.S. adults with ongoing mental health conditions who had used LLMs reported turning to them for therapy or emotional support, including help with anxiety, depression, loneliness, and similar concerns.[[226]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-226) LLMs can produce hallucinations—plausible but incorrect statements—which may mislead users in sensitive mental health contexts.[[227]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-227) Research also shows that LLMs may express stigma or inappropriate agreement with maladaptive thoughts, reflecting limitations in replicating the judgment and relational skills of human therapists.[[228]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-228) Evaluations of crisis scenarios indicate that some LLMs lack effective safety protocols, such as assessing suicide risk or making appropriate referrals.[[229]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-229)[[230]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-230)\n",
      "\n",
      "Contemporary AI practitioners generally agree that present-day large language models do not exhibit [sentience](https://en.wikipedia.org/wiki/Sentience \"Sentience\").[[231]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-231) A minority view argues that even if there is a small chance that a given software system can have subjective experience, which some philosophers suggest is possible,[[232]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-232) then ethical considerations around potential [large-scale suffering](https://en.wikipedia.org/wiki/Suffering_risks \"Suffering risks\") in AI systems may need to be taken seriously - similar to considerations given to animal welfare.[[233]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-Thomson-2022-233)[[234]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-Kateman-2023-234) Proponents of this view have proposed various precautionary measures like moratoriums on AI development[[235]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-235) and induced amnesia[[236]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-236) to address these ethical concerns. Some existential philosophers argue there is no generally accepted way to determine if an LLM is conscious,[[237]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-237) given the inherent difficulty of [measuring subjective experience](https://en.wikipedia.org/wiki/Hard_problem_of_consciousness \"Hard problem of consciousness\").[[238]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-238)\n",
      "\n",
      "The 2022 Google [LaMDA](https://en.wikipedia.org/wiki/LaMDA \"LaMDA\") incident, where engineer [Blake Lemoine](https://en.wikipedia.org/wiki/Blake_Lemoine \"Blake Lemoine\") claimed the model was conscious, is widely considered a canonical example of how language models can induce false beliefs about their sentience through responses that do not prove sentience. The engineer was dismissed after making public claims about the model's consciousness, despite broad scientific consensus that AI systems did not possess sentience.[[239]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-239) This case highlighted how language models' ability to engage in human-like conversation can lead to anthropomorphization and sycophantic responses, even though the models are simply predicting likely next tokens rather than exhibiting true consciousness.\n",
      "\n",
      "*   [Foundation models](https://en.wikipedia.org/wiki/Foundation_models \"Foundation models\")\n",
      "*   [List of large language models](https://en.wikipedia.org/wiki/List_of_large_language_models \"List of large language models\")\n",
      "*   [List of chatbots](https://en.wikipedia.org/wiki/List_of_chatbots \"List of chatbots\")\n",
      "*   [Language model benchmark](https://en.wikipedia.org/wiki/Language_model_benchmark \"Language model benchmark\")\n",
      "*   [Reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning \"Reinforcement learning\")\n",
      "*   [Small language model](https://en.wikipedia.org/wiki/Small_language_model \"Small language model\")\n",
      "\n",
      "1.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-bhaa_1-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-bhaa_1-1)[_**c**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-bhaa_1-2)Bommasani, Rishi; Hudson, Drew A.; Adeli, Ehsan; Altman, Russ; Arora, Simran; von Arx, Matthew; Bernstein, Michael S.; Bohg, Jeannette; Bosselut, Antoine; [Brunskill, Emma](https://en.wikipedia.org/wiki/Emma_Brunskill \"Emma Brunskill\") (2021). \"On the Opportunities and Risks of Foundation Models\". _arxiv_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2108.07258](https://arxiv.org/abs/2108.07258).\n",
      "2.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-few-shot-learners_2-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-few-shot-learners_2-1)Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda (2020). \"Language Models are Few-Shot Learners\". _arxiv_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2005.14165](https://arxiv.org/abs/2005.14165).\n",
      "3.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-few-shot-learners2_3-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-few-shot-learners2_3-1)[_**c**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-few-shot-learners2_3-2)Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (Dec 2020). Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.F.; Lin, H. (eds.). [\"Language Models are Few-Shot Learners\"](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)(PDF). _Advances in Neural Information Processing Systems_. **33**. Curran Associates, Inc.: 1877–1901. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2005.14165](https://arxiv.org/abs/2005.14165). [Archived](https://web.archive.org/web/20231117204007/https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)(PDF) from the original on 2023-11-17. Retrieved 2023-03-14.\n",
      "4.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-4 \"Jump up\")**Fathallah, Nadeen; Das, Arunav; De Giorgis, Stefano; Poltronieri, Andrea; Haase, Peter; Kovriguina, Liubov (2024-05-26). [_NeOn-GPT: A Large Language Model-Powered Pipeline for Ontology Learning_](https://2024.eswc-conferences.org/wp-content/uploads/2024/05/77770034.pdf)(PDF). Extended Semantic Web Conference 2024. Hersonissos, Greece.\n",
      "5.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-Manning-2022_5-0 \"Jump up\")**[Manning, Christopher D.](https://en.wikipedia.org/wiki/Christopher_D._Manning \"Christopher D. Manning\") (2022). [\"Human Language Understanding & Reasoning\"](https://www.amacad.org/publication/human-language-understanding-reasoning). _Daedalus_. **151** (2): 127–138. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1162/daed_a_01905](https://doi.org/10.1162%2Fdaed_a_01905). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[248377870](https://api.semanticscholar.org/CorpusID:248377870). [Archived](https://web.archive.org/web/20231117205531/https://www.amacad.org/publication/human-language-understanding-reasoning) from the original on 2023-11-17. Retrieved 2023-03-09.\n",
      "6.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-scaling-laws_6-0 \"Jump up\")**Kaplan, Jared; McCandlish, Sam; Henighan, Tom; Brown, Tom B.; Chess, Benjamin; Child, Rewon; Gray, Scott; Radford, Alec; Wu, Jeffrey; Amodei, Dario (2020). \"Scaling Laws for Neural Language Models\". _arxiv_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2001.08361](https://arxiv.org/abs/2001.08361).\n",
      "7.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-7 \"Jump up\")**Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (2017). \"Attention is All you Need\". _arxiv_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[1706.03762](https://arxiv.org/abs/1706.03762).\n",
      "8.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-8 \"Jump up\")**Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". _arxiv_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[1810.04805](https://arxiv.org/abs/1810.04805).\n",
      "9.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-9 \"Jump up\")**Christiano, Paul; Leike, Jan; Brown, Tom B.; Martic, Miljan; Legg, Shane; Amodei, Dario (2017). \"Deep Reinforcement Learning from Human Preferences\". _arxiv_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[1706.03741](https://arxiv.org/abs/1706.03741).\n",
      "10.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-10 \"Jump up\")**Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex (2022). \"Training language models to follow instructions with human feedback\". _arxiv_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2203.02155](https://arxiv.org/abs/2203.02155).\n",
      "11.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-11 \"Jump up\")**Wang, Alex; Singh, Amanpreet; Michael, Julian; Hill, Felix; Levy, Omer; Bowman, Samuel R. (2018). \"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\". _arxiv_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[1804.07461](https://arxiv.org/abs/1804.07461).\n",
      "12.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-12 \"Jump up\")**Hendrycks, Dan; Burns, Collin; Basart, Steven; Zou, Andy; Mazeika, Mantas; Song, Dawn; Steinhardt, Jacob (2020). \"Measuring Massive Multitask Language Understanding\". _arxiv_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2009.03300](https://arxiv.org/abs/2009.03300).\n",
      "13.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-13 \"Jump up\")**Recht, Benjamin; Roelofs, Rebecca; Schmidt, Ludwig; Shankar, Vaishaal (2019). \"Do ImageNet Classifiers Generalize to ImageNet?\". _arxiv_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[1902.10811](https://arxiv.org/abs/1902.10811).\n",
      "14.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-14 \"Jump up\")**Goodman, Joshua (2001-08-09). \"A Bit of Progress in Language Modeling\". _Computer Speech and Language_. **15** (4): 403–434. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[cs/0108005](https://arxiv.org/abs/cs/0108005). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1006/csla.2001.0174](https://doi.org/10.1006%2Fcsla.2001.0174).\n",
      "15.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-15 \"Jump up\")**Kilgarriff, Adam; Grefenstette, Gregory (September 2003). [\"Introduction to the Special Issue on the Web as Corpus\"](https://direct.mit.edu/coli/article/29/3/333-347/1816). _Computational Linguistics_. **29** (3): 333–347. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1162/089120103322711569](https://doi.org/10.1162%2F089120103322711569). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0891-2017](https://search.worldcat.org/issn/0891-2017).\n",
      "16.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-16 \"Jump up\")**Banko, Michele; Brill, Eric (2001). [\"Scaling to very very large corpora for natural language disambiguation\"](https://doi.org/10.3115%2F1073012.1073017). _Proceedings of the 39th Annual Meeting on Association for Computational Linguistics - ACL '01_. Morristown, NJ, USA: Association for Computational Linguistics: 26–33. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.3115/1073012.1073017](https://doi.org/10.3115%2F1073012.1073017).\n",
      "17.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-17 \"Jump up\")**Resnik, Philip; Smith, Noah A. (September 2003). [\"The Web as a Parallel Corpus\"](https://direct.mit.edu/coli/article/29/3/349-380/1809). _Computational Linguistics_. **29** (3): 349–380. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1162/089120103322711578](https://doi.org/10.1162%2F089120103322711578). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0891-2017](https://search.worldcat.org/issn/0891-2017). [Archived](https://web.archive.org/web/20240607172811/https://direct.mit.edu/coli/article/29/3/349-380/1809) from the original on 2024-06-07. Retrieved 2024-06-07.\n",
      "18.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-18 \"Jump up\")**Xu, Wei; Rudnicky, Alex (2000-10-16). [\"Can artificial neural networks learn language models?\"](https://www.isca-archive.org/icslp_2000/xu00b_icslp.html). _6th International Conference on Spoken Language Processing (ICSLP 2000)_. Vol.1. ISCA. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.21437/icslp.2000-50](https://doi.org/10.21437%2Ficslp.2000-50).\n",
      "19.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-19 \"Jump up\")**Chen, Leiyu; Li, Shaobo; Bai, Qiang; Yang, Jing; Jiang, Sanlong; Miao, Yanming (2021). [\"Review of Image Classification Algorithms Based on Convolutional Neural Networks\"](https://doi.org/10.3390%2Frs13224712). _Remote Sensing_. **13** (22): 4712. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) \"Bibcode (identifier)\"):[2021RemS...13.4712C](https://ui.adsabs.harvard.edu/abs/2021RemS...13.4712C). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.3390/rs13224712](https://doi.org/10.3390%2Frs13224712).\n",
      "20.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-20 \"Jump up\")**[Vaswani, Ashish](https://en.wikipedia.org/wiki/Ashish_Vaswani \"Ashish Vaswani\"); Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; [Gomez, Aidan N](https://en.wikipedia.org/wiki/Aidan_Gomez \"Aidan Gomez\"); Kaiser, Łukasz; Polosukhin, Illia (2017). [\"Attention is All you Need\"](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)(PDF). _Advances in Neural Information Processing Systems_. **30**. Curran Associates, Inc. [Archived](https://web.archive.org/web/20240221141113/https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)(PDF) from the original on 2024-02-21. Retrieved 2024-01-21.\n",
      "21.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-21 \"Jump up\")**Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua (2014). \"Neural Machine Translation by Jointly Learning to Align and Translate\". _ICLR_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[1409.0473](https://arxiv.org/abs/1409.0473).\n",
      "22.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-22 \"Jump up\")**Rogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). [\"A Primer in BERTology: What We Know About How BERT Works\"](https://aclanthology.org/2020.tacl-1.54). _Transactions of the Association for Computational Linguistics_. **8**: 842–866. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2002.12327](https://arxiv.org/abs/2002.12327). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1162/tacl_a_00349](https://doi.org/10.1162%2Ftacl_a_00349). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[211532403](https://api.semanticscholar.org/CorpusID:211532403). [Archived](https://web.archive.org/web/20220403103310/https://aclanthology.org/2020.tacl-1.54/) from the original on 2022-04-03. Retrieved 2024-01-21.\n",
      "23.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-auto_23-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-auto_23-1)Movva, Rajiv; Balachandar, Sidhika; Peng, Kenny; Agostini, Gabriel; Garg, Nikhil; Pierson, Emma (2024). [\"Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers\"](https://aclanthology.org/2024.naacl-long.67). _Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)_. pp.1223–1243. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2307.10700](https://arxiv.org/abs/2307.10700). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.18653/v1/2024.naacl-long.67](https://doi.org/10.18653%2Fv1%2F2024.naacl-long.67). Retrieved 2024-12-08.\n",
      "24.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-24 \"Jump up\")**Hern, Alex (14 February 2019). [\"New AI fake text generator may be too dangerous to release, say creators\"](https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction). _[The Guardian](https://en.wikipedia.org/wiki/The\\_Guardian \"The Guardian\")_. [Archived](https://web.archive.org/web/20190214173112/https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction) from the original on 14 February 2019. Retrieved 20 January 2024.\n",
      "25.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-25 \"Jump up\")**[\"ChatGPT a year on: 3 ways the AI chatbot has completely changed the world in 12 months\"](https://www.euronews.com/next/2023/11/30/chatgpt-a-year-on-3-ways-the-ai-chatbot-has-completely-changed-the-world-in-12-months). [Euronews](https://en.wikipedia.org/wiki/Euronews \"Euronews\"). November 30, 2023. [Archived](https://web.archive.org/web/20240114025250/https://www.euronews.com/next/2023/11/30/chatgpt-a-year-on-3-ways-the-ai-chatbot-has-completely-changed-the-world-in-12-months) from the original on January 14, 2024. Retrieved January 20, 2024.\n",
      "26.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-26 \"Jump up\")**Heaven, Will (March 14, 2023). [\"GPT-4 is bigger and better than ChatGPT—but OpenAI won't say why\"](https://www.technologyreview.com/2023/03/14/1069823/gpt-4-is-bigger-and-better-chatgpt-openai/). [MIT Technology Review](https://en.wikipedia.org/wiki/MIT_Technology_Review \"MIT Technology Review\"). [Archived](https://web.archive.org/web/20230317224201/https://www.technologyreview.com/2023/03/14/1069823/gpt-4-is-bigger-and-better-chatgpt-openai/) from the original on March 17, 2023. Retrieved January 20, 2024.\n",
      "27.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-NYTimesInfo_27-0 \"Jump up\")**Metz, Cade (September 12, 2024). [\"OpenAI Unveils New ChatGPT That Can Reason Through Math and Science\"](https://www.nytimes.com/2024/09/12/technology/openai-chatgpt-math.html). _[The New York Times](https://en.wikipedia.org/wiki/The\\_New\\_York\\_Times \"The New York Times\")_. Retrieved September 12, 2024.\n",
      "28.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-28 \"Jump up\")**[\"Parameters in notable artificial intelligence systems\"](https://ourworldindata.org/grapher/artificial-intelligence-parameter-count?time=2017-09-05..latest). _ourworldindata.org_. November 30, 2023. Retrieved January 20, 2024.\n",
      "29.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-29 \"Jump up\")**Sharma, Shubham (2025-01-20). [\"Open-source DeepSeek-R1 uses pure reinforcement learning to match OpenAI o1 — at 95% less cost\"](https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/). _VentureBeat_. Retrieved 2025-01-26.\n",
      "30.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-30 \"Jump up\")**Zia, Dr Tehseen (2024-01-08). [\"Unveiling of Large Multimodal Models: Shaping the Landscape of Language Models in 2024\"](https://www.unite.ai/unveiling-of-large-multimodal-models-shaping-the-landscape-of-language-models-in-2024/). _Unite.AI_. Retrieved 2024-12-28.\n",
      "31.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-31 \"Jump up\")**Peng, Bo; et al. (2023). [\"RWKV: Reinventing RNNS for the Transformer Era\"](https://aclanthology.org/2023.findings-emnlp.936/). _EMNLP_: 14048–14077. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2305.13048](https://arxiv.org/abs/2305.13048). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.18653/v1/2023.findings-emnlp.936](https://doi.org/10.18653%2Fv1%2F2023.findings-emnlp.936).\n",
      "32.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-32 \"Jump up\")**Merritt, Rick (2022-03-25). [\"What Is a Transformer Model?\"](https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/). _NVIDIA Blog_. [Archived](https://web.archive.org/web/20231117203924/https://blogs.nvidia.com/blog/what-is-a-transformer-model/) from the original on 2023-11-17. Retrieved 2023-07-25.\n",
      "33.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-33 \"Jump up\")**Gu, Albert; Dao, Tri (2023-12-01). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\". _COLM_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2312.00752](https://arxiv.org/abs/2312.00752).\n",
      "34.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-34 \"Jump up\")**Kaushal, Ayush; Mahowald, Kyle (2022-06-06). [\"What do tokens know about their characters and how do they know it?\"](https://aclanthology.org/2022.naacl-main.179.pdf)(PDF). _NAACL_.\n",
      "35.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-35 \"Jump up\")**Yennie Jun (2023-05-03). [\"All languages are NOT created (tokenized) equal\"](https://web.archive.org/web/20230817165705/https://blog.yenniejun.com/p/all-languages-are-not-created-tokenized). _Language models cost much more in some languages than others_. Archived from [the original](https://blog.yenniejun.com/p/all-languages-are-not-created-tokenized) on 2023-08-17. Retrieved 2023-08-17. In other words, to express the same sentiment, some languages require up to 10 times more tokens.\n",
      "36.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-LangModelTokenizsersUnfairness_36-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-LangModelTokenizsersUnfairness_36-1)Petrov, Aleksandar; Malfa, Emanuele La; Torr, Philip; Bibi, Adel (June 23, 2023). [\"Language Model Tokenizers Introduce Unfairness Between Languages\"](https://openreview.net/forum?id=Pj4YYuxTq9). _NeurIPS_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2305.15425](https://arxiv.org/abs/2305.15425). [Archived](https://web.archive.org/web/20231215212906/https://openreview.net/forum?id=Pj4YYuxTq9) from the original on December 15, 2023. Retrieved September 16, 2023 – via openreview.net.\n",
      "37.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-37 \"Jump up\")**Sutherland, Richard (2024-12-19). [\"Claude AI Pricing: How Much Does Anthropic's AI Cost?\"](https://tech.co/news/how-much-does-claude-ai-cost). _Tech.co_. Retrieved 2025-08-16.\n",
      "38.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-2022Book_38-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-2022Book_38-1)Paaß, Gerhard; Giesselbach, Sven (2022). \"Pre-trained Language Models\". _Foundation Models for Natural Language Processing_. Artificial Intelligence: Foundations, Theory, and Algorithms. pp.19–78. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1007/978-3-031-23190-2_2](https://doi.org/10.1007%2F978-3-031-23190-2_2). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-3-031-23190-2](https://en.wikipedia.org/wiki/Special:BookSources/978-3-031-23190-2 \"Special:BookSources/978-3-031-23190-2\").\n",
      "39.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-aYNg4_39-0 \"Jump up\")**Dodge, Jesse; Sap, Maarten; Marasović, Ana; Agnew, William; Ilharco, Gabriel; Groeneveld, Dirk; Mitchell, Margaret; Gardner, Matt (2021). [\"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus\"](https://aclanthology.org/2021.emnlp-main.98.pdf)(PDF). _EMNLP_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2104.08758](https://arxiv.org/abs/2104.08758).\n",
      "40.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-40 \"Jump up\")**Lee, Katherine; Ippolito, Daphne; Nystrom, Andrew; Zhang, Chiyuan; Eck, Douglas; Callison-Burch, Chris; [Carlini, Nicholas](https://en.wikipedia.org/wiki/Nicholas_Carlini \"Nicholas Carlini\") (May 2022). [\"Deduplicating Training Data Makes Language Models Better\"](https://aclanthology.org/2022.acl-long.577.pdf)(PDF). _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. pp.8424–8445. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.18653/v1/2022.acl-long.577](https://doi.org/10.18653%2Fv1%2F2022.acl-long.577).\n",
      "41.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-41 \"Jump up\")**Li, Yuanzhi; Bubeck, Sébastien; Eldan, Ronen; Del Giorno, Allie; Gunasekar, Suriya; Lee, Yin Tat (2023-09-11). \"Textbooks Are All You Need II: phi-1.5 technical report\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2309.05463](https://arxiv.org/abs/2309.05463) [[cs.CL](https://arxiv.org/archive/cs.CL)].\n",
      "42.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-42 \"Jump up\")**Lin, Zhenghao; Gou, Zhibin; Gong, Yeyun; Liu, Xiao; Shen, Yelong; Xu, Ruochen; Lin, Chen; Yang, Yujiu; Jiao, Jian (2024-04-11). [\"Rho-1: Not All Tokens Are What You Need\"](https://dl.acm.org/doi/10.5555/3737916.3738830). _NeurIPS_. **37**: 29029–29063. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[979-8-3313-1438-5](https://en.wikipedia.org/wiki/Special:BookSources/979-8-3313-1438-5 \"Special:BookSources/979-8-3313-1438-5\").\n",
      "43.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-43 \"Jump up\")**Abdin, Marah; Jacobs, Sam Ade; Awan, Ammar Ahmad; Aneja, Jyoti; Awadallah, Ahmed; Awadalla, Hany; Bach, Nguyen; Bahree, Amit; Bakhtiari, Arash (2024-04-23). \"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone\". _CoRR_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2404.14219](https://arxiv.org/abs/2404.14219).\n",
      "44.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-44 \"Jump up\")**Edwards, Benj (2023-05-09). [\"AI gains \"values\" with Anthropic's new Constitutional AI chatbot approach\"](https://arstechnica.com/information-technology/2023/05/ai-with-a-moral-compass-anthropic-outlines-constitutional-ai-in-its-claude-chatbot/). _Ars Technica_. Retrieved 2025-06-30.\n",
      "45.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-45 \"Jump up\")**Snyder, Alison (2022-01-27). [\"Next generation AI can follow a person's instructions and intentions\"](https://www.axios.com/2022/01/27/ai-instructions-learning-algorithm). _Axios_. Retrieved 2025-08-07.\n",
      "46.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-Jay_Allamar_46-0 \"Jump up\")**Allamar, Jay. [\"Illustrated transformer\"](https://jalammar.github.io/illustrated-transformer/). [Archived](https://web.archive.org/web/20230725230033/http://jalammar.github.io/illustrated-transformer/) from the original on 2023-07-25. Retrieved 2023-07-29.\n",
      "47.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-Jay_Allamar_GPT2_47-0 \"Jump up\")**Allamar, Jay. [\"The Illustrated GPT-2 (Visualizing Transformer Language Models)\"](https://jalammar.github.io/illustrated-gpt2/). Retrieved 2023-08-01.\n",
      "48.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-48 \"Jump up\")**Yeung, Ken (2024-05-14). [\"Google announces Gemini 1.5 Flash, a rapid multimodal model with a 1M context window\"](https://venturebeat.com/ai/google-gemini-1-5-flash-rapid-multimodal-model-announced/). _VentureBeat_. Retrieved 2025-08-26.\n",
      "49.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-ioUpE_49-0 \"Jump up\")**Zaib, Munazza; Sheng, Quan Z.; Emma Zhang, Wei (4 February 2020). \"A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP\". [_Proceedings of the Australasian Computer Science Week Multiconference_](https://www.researchgate.net/publication/338931711). pp.1–4. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2104.10810](https://arxiv.org/abs/2104.10810). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1145/3373017.3373028](https://doi.org/10.1145%2F3373017.3373028). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-1-4503-7697-6](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4503-7697-6 \"Special:BookSources/978-1-4503-7697-6\"). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[211040895](https://api.semanticscholar.org/CorpusID:211040895).\n",
      "50.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-jm_50-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-jm_50-1)[_**c**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-jm_50-2)Jurafsky, Dan; Martin, James H. (7 January 2023). [_Speech and Language Processing_](https://web.stanford.edu/~jurafsky/slp3/ed3book_jan72023.pdf)(PDF) (3rd edition draft ed.). [Archived](https://web.archive.org/web/20230323210221/https://web.stanford.edu/~jurafsky/slp3/ed3book_jan72023.pdf)(PDF) from the original on 23 March 2023. Retrieved 24 May 2022.\n",
      "51.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-HGZCJ_51-0 \"Jump up\")**Shazeer, Noam; Mirhoseini, Azalia; Maziarz, Krzysztof; Davis, Andy; Le, Quoc; Hinton, Geoffrey; Dean, Jeff (2017-01-01). \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\". _ICLR_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[1701.06538](https://arxiv.org/abs/1701.06538).\n",
      "52.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-R9Qq5_52-0 \"Jump up\")**Lepikhin, Dmitry; Lee, HyoukJoong; Xu, Yuanzhong; Chen, Dehao; Firat, Orhan; Huang, Yanping; Krikun, Maxim; Shazeer, Noam; Chen, Zhifeng (2021-01-12). \"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\". _ICLR_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2006.16668](https://arxiv.org/abs/2006.16668).\n",
      "53.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-glam-blog_53-0 \"Jump up\")**Dai, Andrew M; Du, Nan (December 9, 2021). [\"More Efficient In-Context Learning with GLaM\"](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html). _ai.googleblog.com_. [Archived](https://web.archive.org/web/20230312072042/https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html) from the original on 2023-03-12. Retrieved 2023-03-09.\n",
      "54.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-54 \"Jump up\")**Mann, Tobias. [\"How to run an LLM locally on your PC in less than 10 minutes\"](https://www.theregister.com/2024/03/17/ai_pc_local_llm/). _www.theregister.com_. Retrieved 2024-05-17.\n",
      "55.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-LS2Go_55-0 \"Jump up\")**Nagel, Markus; Amjad, Rana Ali; Baalen, Mart Van; Louizos, Christos; Blankevoort, Tijmen (2020-11-21). [\"Up or Down? Adaptive Rounding for Post-Training Quantization\"](https://proceedings.mlr.press/v119/nagel20a.html). _Proceedings of the 37th International Conference on Machine Learning_. PMLR: 7197–7206. [Archived](https://web.archive.org/web/20230614080854/https://proceedings.mlr.press/v119/nagel20a.html) from the original on 2023-06-14. Retrieved 2023-06-14.\n",
      "56.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-self-instruct-paper_56-0 \"Jump up\")**Wang, Yizhong; Kordi, Yeganeh; Mishra, Swaroop; Liu, Alisa; Smith, Noah A.; Khashabi, Daniel; Hajishirzi, Hannaneh (2023). [\"Self-Instruct: Aligning Language Model with Self Generated Instructions\"](https://aclanthology.org/2023.acl-long.754/). _ACL_: 13484–13508. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.18653/v1/2023.acl-long.754](https://doi.org/10.18653%2Fv1%2F2023.acl-long.754).\n",
      "57.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-57 \"Jump up\")**[\"Introducing ChatGPT\"](https://openai.com/index/chatgpt/). _openai.com_. 13 March 2024.\n",
      "58.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-58 \"Jump up\")**[\"OpenAI Platform\"](https://platform.openai.com/docs/guides/text?api-mode=responses). _platform.openai.com_.\n",
      "59.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-59 \"Jump up\")**[\"Giving Claude a role with a system prompt\"](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/system-prompts). _Anthropic_.\n",
      "60.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-BUZBP_60-0 \"Jump up\")**Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; Goyal, Naman; Küttler, Heinrich; Lewis, Mike; Yih, Wen-tau; Rocktäschel, Tim; Riedel, Sebastian; Kiela, Douwe (2020). [\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html). _Advances in Neural Information Processing Systems_. **33**. Curran Associates, Inc.: 9459–9474. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2005.11401](https://arxiv.org/abs/2005.11401). [Archived](https://web.archive.org/web/20230612171229/https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html) from the original on 2023-06-12. Retrieved 2023-06-12.\n",
      "61.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-61 \"Jump up\")**Kiela, Douwe; Riedel, Sebastian; Lewis, Patrick; Piktus, Aleksandra (September 28, 2020). [\"Retrieval Augmented Generation: Streamlining the creation of intelligent natural language processing models\"](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/). _Meta_.\n",
      "62.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-62 \"Jump up\")**Dickson, Ben (2025-04-02). [\"The tool integration problem that's holding back enterprise AI (and how CoTools solves it)\"](https://venturebeat.com/ai/the-tool-integration-problem-thats-holding-back-enterprise-ai-and-how-cotools-solves-it/). _VentureBeat_. Retrieved 2025-05-26.\n",
      "63.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-lLrda_63-0 \"Jump up\")**Liang, Yaobo; Wu, Chenfei; Song, Ting; Wu, Wenshan; Xia, Yan; Liu, Yu; Ou, Yang; Lu, Shuai; Ji, Lei; Mao, Shaoguang; Wang, Yun; Shou, Linjun; Gong, Ming; Duan, Nan (2024). [\"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs\"](https://doi.org/10.34133%2Ficomputing.0063). _Science_. **3** 0063. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.34133/icomputing.0063](https://doi.org/10.34133%2Ficomputing.0063).\n",
      "64.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-4Xzrs_64-0 \"Jump up\")**Patil, Shishir G.; Zhang, Tianjun; Wang, Xin; Gonzalez, Joseph E. (2023-05-01). [\"Gorilla: Large Language Model Connected with Massive APIs\"](https://proceedings.neurips.cc/paper_files/paper/2024/hash/e4c61f578ff07830f5c37378dd3ecb0d-Abstract-Conference.html). _NeurIPS_. **37**: 126544–126565.\n",
      "65.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-65 \"Jump up\")**[\"ChatGPT-AutoExpert/_system-prompts/all_tools.md at 835baae768870aa9747663c24d8216820d24fd74 · spdustin/ChatGPT-AutoExpert\"](https://github.com/spdustin/ChatGPT-AutoExpert/blob/835baae768870aa9747663c24d8216820d24fd74/_system-prompts/all_tools.md). _GitHub_.\n",
      "66.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-66 \"Jump up\")**Wang, Lei; Ma, Chen; Feng, Xueyang; Zhang, Zeyu; Yang, Hao; Zhang, Jingsen; Chen, Zhiyuan; Tang, Jiakai; Chen, Xu; Lin, Yankai; Zhao, Wayne Xin; Wei, Zhewei; Wen, Jirong (December 2024). \"A survey on large language model based autonomous agents\". _Frontiers of Computer Science_. **18** (6) 186345. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2308.11432](https://arxiv.org/abs/2308.11432). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1007/s11704-024-40231-1](https://doi.org/10.1007%2Fs11704-024-40231-1).\n",
      "67.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-DmvNE_67-0 \"Jump up\")**Yao, Shunyu; Zhao, Jeffrey; Yu, Dian; Du, Nan; Shafran, Izhak; Narasimhan, Karthik; Cao, Yuan (2022-10-01). \"ReAct: Synergizing Reasoning and Acting in Language Models\". _ICLR_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2210.03629](https://arxiv.org/abs/2210.03629).\n",
      "68.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-68 \"Jump up\")**Wang, Zihao; Cai, Shaofei; Liu, Anji; Ma, Xiaojian; Liang, Yitao (2023-02-03). [\"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents\"](https://dl.acm.org/doi/10.5555/3666122.3667602). _NeurIPS_: 34153–34189.\n",
      "69.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-sbB2T_69-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-sbB2T_69-1)[_**c**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-sbB2T_69-2)Shinn, Noah; Cassano, Federico; Labash, Beck; Gopinath, Ashwin; Narasimhan, Karthik; Yao, Shunyu (2023-03-01). [\"Reflexion: Language Agents with Verbal Reinforcement Learning\"](https://dl.acm.org/doi/10.5555/3666122.3667602). _NeurIPS_: 34153–34189.\n",
      "70.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-ltTer_70-0 \"Jump up\")**Hao, Shibo; Gu, Yi; Ma, Haodi; Jiahua Hong, Joshua; Wang, Zhen; Zhe Wang, Daisy; Hu, Zhiting (2023-05-01). [\"Reasoning with Language Model is Planning with World Model\"](https://aclanthology.org/2023.emnlp-main.507/). _EMNLP_: 8154–8173. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.18653/v1/2023.emnlp-main.507](https://doi.org/10.18653%2Fv1%2F2023.emnlp-main.507).\n",
      "71.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-mBvD9_71-0 \"Jump up\")**Zhang, Jenny; Lehman, Joel; Stanley, Kenneth; Clune, Jeff (2 June 2023). \"OMNI: Open-endedness via Models of human Notions of Interestingness\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2306.01711](https://arxiv.org/abs/2306.01711) [[cs.AI](https://arxiv.org/archive/cs.AI)].\n",
      "72.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-:0_72-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-:0_72-1)[\"Voyager | An Open-Ended Embodied Agent with Large Language Models\"](https://voyager.minedojo.org/). _voyager.minedojo.org_. [Archived](https://web.archive.org/web/20230608225054/https://voyager.minedojo.org/) from the original on 2023-06-08. Retrieved 2023-06-09.\n",
      "73.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-XuvjF_73-0 \"Jump up\")**Park, Joon Sung; O'Brien, Joseph C.; Cai, Carrie J.; Ringel Morris, Meredith; Liang, Percy; Bernstein, Michael S. (2023-04-01). _Generative Agents: Interactive Simulacra of Human Behavior_. UIST. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1145/3586183.3606763](https://doi.org/10.1145%2F3586183.3606763).\n",
      "74.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-74 \"Jump up\")**Nye, Maxwell; Anders, Andreassen Johan; Gur-Ari, Guy; Michalewski, Henryk; Austin, Jacob; Bieber, David; Dohan, David; Lewkowycz, Aitor; Bosma, Maarten; Luan, David; Sutton, Charles; Odena, Augustus (30 November 2021). \"Show Your Work: Scratchpads for Intermediate Computation with Language Models\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2112.00114](https://arxiv.org/abs/2112.00114) [[cs.LG](https://arxiv.org/archive/cs.LG)].\n",
      "75.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-auto2_75-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-auto2_75-1)Wei, Jason; Wang, Xuezhi; Schuurmans, Dale; Bosma, Maarten; Ichter, Brian; Xia, Fei; Chi, Ed; Le, Quoc; Zhou, Denny (2023-01-10). [\"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\"](https://dl.acm.org/doi/10.5555/3600270.3602070). _NeurIPS_: 24824–24837. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-1-7138-7108-8](https://en.wikipedia.org/wiki/Special:BookSources/978-1-7138-7108-8 \"Special:BookSources/978-1-7138-7108-8\").\n",
      "76.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-76 \"Jump up\")**Wu, Tongshuang; Jiang, Ellen; Donsbach, Aaron; Gray, Jeff; Molina, Alejandra; Terry, Michael; Cai, Carrie J. (2022-03-13). [_PromptChainer: Chaining Large Language Model Prompts through Visual Programming_](https://dl.acm.org/doi/10.1145/3491101.3519729). CHI Conference on Human Factors in Computing Systems. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2203.06566](https://arxiv.org/abs/2203.06566). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1145/3491101.3519729](https://doi.org/10.1145%2F3491101.3519729).\n",
      "77.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-77 \"Jump up\")**[\"What is prompt chaining?\"](https://www.ibm.com/think/topics/prompt-chaining). _IBM_. 23 April 2024.\n",
      "78.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-78 \"Jump up\")**[\"What is chain of thought (CoT) prompting?\"](https://www.ibm.com/think/topics/chain-of-thoughts). _IBM_. 23 April 2025.\n",
      "79.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-79 \"Jump up\")**Schreiner, Maximilian (2022-09-27). [\"Deeper insights into AI language models - chain of thought prompting as a success factor\"](https://the-decoder.com/deeper-insights-for-ai-language-models-chain-of-thought-prompting-as-a-key-factor/). _The Decoder_. Retrieved 2025-06-30.\n",
      "80.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-80 \"Jump up\")**Wang, Xuezhi; Wei, Jason; Schuurmans, Dale; Le, Quoc; Chi, Ed; Narang, Sharan; Chowdhery, Aakanksha; Zhou, Denny (21 March 2022). \"Self-Consistency Improves Chain of Thought Reasoning in Language Models\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2203.11171](https://arxiv.org/abs/2203.11171) [[cs.CL](https://arxiv.org/archive/cs.CL)].\n",
      "81.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-81 \"Jump up\")**Zhou, Denny; Schärli, Nathanael; Hou, Le; Wei, Jason; Scales, Nathan; Wang, Xuezhi; Schuurmans, Dale; Cui, Claire; Bousquet, Olivier; Le, Quoc; Chi, Ed (21 May 2022). \"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2205.10625](https://arxiv.org/abs/2205.10625) [[cs.AI](https://arxiv.org/archive/cs.AI)].\n",
      "82.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-nyt-o3_82-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-nyt-o3_82-1)Metz, Cade (2024-12-20). [\"OpenAI Unveils New A.I. That Can 'Reason' Through Math and Science Problems\"](https://www.nytimes.com/2024/12/20/technology/openai-new-ai-math-science.html). _The New York Times_. Retrieved 2025-02-03.\n",
      "83.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-nature-deepseek_83-0 \"Jump up\")**Gibney, Elizabeth (2025-01-30). [\"China's cheap, open AI model DeepSeek thrills scientists\"](https://www.nature.com/articles/d41586-025-00229-6). _Nature_. Retrieved 2025-02-03.\n",
      "84.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-84 \"Jump up\")**Sharma, Asankhaya. [\"OptiLLM: Optimizing inference proxy for LLMs\"](https://github.com/codelion/optillm). _GitHub_. Retrieved 2025-08-05.\n",
      "85.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-85 \"Jump up\")**[\"OptiLLM: An OpenAI API Compatible Optimizing Inference Proxy which Implements Several State-of-the-Art Techniques that can Improve the Accuracy and Performance of LLMs\"](https://www.marktechpost.com/2024/11/18/optillm-an-openai-api-compatible-optimizing-inference-proxy-which-implements-several-state-of-the-art-techniques-that-can-improve-the-accuracy-and-performance-of-llms/). _MarkTechPost_. 2024-11-18. Retrieved 2025-08-05.\n",
      "86.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-86 \"Jump up\")**Kiros, Ryan; Salakhutdinov, Ruslan; Zemel, Rich (2014-06-18). [\"Multimodal Neural Language Models\"](https://proceedings.mlr.press/v32/kiros14.html). _Proceedings of the 31st International Conference on Machine Learning_. PMLR: 595–603. [Archived](https://web.archive.org/web/20230702195952/https://proceedings.mlr.press/v32/kiros14.html) from the original on 2023-07-02. Retrieved 2023-07-02.\n",
      "87.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-87 \"Jump up\")**Driess, Danny; Xia, Fei; Sajjadi, Mehdi S. M.; Lynch, Corey; Chowdhery, Aakanksha; Ichter, Brian; Wahid, Ayzaan; Tompson, Jonathan; Vuong, Quan; Yu, Tianhe; Huang, Wenlong; Chebotar, Yevgen; Sermanet, Pierre; Duckworth, Daniel; Levine, Sergey (2023-03-01). [\"PaLM-E: An Embodied Multimodal Language Model\"](https://dl.acm.org/doi/10.5555/3618408.3618748). _ICML_. **202**: 8469–8488.\n",
      "88.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-88 \"Jump up\")**Liu, Haotian; Li, Chunyuan; Wu, Qingyang; Lee, Yong Jae (2023-04-01). \"Visual Instruction Tuning\". _NeurIPS_.\n",
      "89.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-89 \"Jump up\")**Zhang, Hang; Li, Xin; Bing, Lidong (2023-06-01). \"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding\". _EMNLP_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2306.02858](https://arxiv.org/abs/2306.02858).\n",
      "90.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-90 \"Jump up\")**[\"OpenAI says natively multimodal GPT-4o eats text, visuals, sound – and emits the same\"](https://www.theregister.com/2024/05/13/openai_gpt4o/). _The Register_. 2024-05-13.\n",
      "91.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-91 \"Jump up\")**Zia, Dr Tehseen (2024-01-08). [\"Unveiling of Large Multimodal Models: Shaping the Landscape of Language Models in 2024\"](https://www.unite.ai/unveiling-of-large-multimodal-models-shaping-the-landscape-of-language-models-in-2024/). _Unite.AI_. Retrieved 2025-05-30.\n",
      "92.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-92 \"Jump up\")**Li, Junnan; Li, Dongxu; Savarese, Silvio; Hoi, Steven (2023-01-01). [\"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\"](https://dl.acm.org/doi/10.5555/3618408.3619222). _ICML_. **202**: 19730–19742.\n",
      "93.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-93 \"Jump up\")**Kumar, Puneet; Khokher, Vedanti; Gupta, Yukti; Raman, Balasubramanian (2021). _Hybrid Fusion Based Approach for Multimodal Emotion Recognition with Insufficient Labeled Data_. pp.314–318. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1109/ICIP42928.2021.9506714](https://doi.org/10.1109%2FICIP42928.2021.9506714). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-1-6654-4115-5](https://en.wikipedia.org/wiki/Special:BookSources/978-1-6654-4115-5 \"Special:BookSources/978-1-6654-4115-5\").\n",
      "94.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-94 \"Jump up\")**Alayrac, Jean-Baptiste; Donahue, Jeff; Luc, Pauline; Miech, Antoine; Barr, Iain; Hasson, Yana; Lenc, Karel; Mensch, Arthur; Millican, Katherine; Reynolds, Malcolm; Ring, Roman; Rutherford, Eliza; Cabi, Serkan; Han, Tengda; Gong, Zhitao (2022-12-06). [\"Flamingo: a Visual Language Model for Few-Shot Learning\"](https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html). _Advances in Neural Information Processing Systems_. **35**: 23716–23736. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2204.14198](https://arxiv.org/abs/2204.14198). [Archived](https://web.archive.org/web/20230702195951/https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html) from the original on 2023-07-02. Retrieved 2023-07-02.\n",
      "95.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-95 \"Jump up\")**Finnie-Ansley, James; Denny, Paul; Becker, Brett A.; Luxton-Reilly, Andrew; Prather, James (14 February 2022). \"The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming\". _Proceedings of the 24th Australasian Computing Education Conference_. New York, NY, USA: Association for Computing Machinery. pp.10–19. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1145/3511861.3511863](https://doi.org/10.1145%2F3511861.3511863). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-1-4503-9643-1](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4503-9643-1 \"Special:BookSources/978-1-4503-9643-1\"). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[246681316](https://api.semanticscholar.org/CorpusID:246681316).\n",
      "96.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-96 \"Jump up\")**Husein, Rasha Ahmad; Aburajouh, Hala; Catal, Cagatay (March 2025). \"Large language models for code completion: A systematic literature review\". _Computer Standards & Interfaces_. **92** 103917. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1016/j.csi.2024.103917](https://doi.org/10.1016%2Fj.csi.2024.103917).\n",
      "97.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-97 \"Jump up\")**Weissenow, Konstantin; Rost, Burkhard (April 2025). \"Are protein language models the new universal key?\". _Current Opinion in Structural Biology_. **91** 102997. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1016/j.sbi.2025.102997](https://doi.org/10.1016%2Fj.sbi.2025.102997). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) \"PMID (identifier)\")[39921962](https://pubmed.ncbi.nlm.nih.gov/39921962).\n",
      "98.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-98 \"Jump up\")**Lin, Zeming; Akin, Halil; Rao, Roshan; Hie, Brian; Zhu, Zhongkai; Lu, Wenting; Smetanin, Nikita; Verkuil, Robert; Kabeli, Ori; Shmueli, Yaniv; dos Santos Costa, Allan; Fazel-Zarandi, Maryam; Sercu, Tom; Candido, Salvatore; Rives, Alexander (17 March 2023). [\"Evolutionary-scale prediction of atomic-level protein structure with a language model\"](https://doi.org/10.1126%2Fscience.ade2574). _Science_. **379** (6637): 1123–1130. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) \"Bibcode (identifier)\"):[2023Sci...379.1123L](https://ui.adsabs.harvard.edu/abs/2023Sci...379.1123L). [bioRxiv](https://en.wikipedia.org/wiki/BioRxiv_(identifier) \"BioRxiv (identifier)\")[10.1101/2022.07.20.500902](https://doi.org/10.1101%2F2022.07.20.500902). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1126/science.ade2574](https://doi.org/10.1126%2Fscience.ade2574). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) \"PMID (identifier)\")[36927031](https://pubmed.ncbi.nlm.nih.gov/36927031).\n",
      "99.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-99 \"Jump up\")**[\"ESM Metagenomic Atlas | Meta AI\"](https://esmatlas.com/about). _esmatlas.com_.\n",
      "100.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-100 \"Jump up\")**Hayes, Thomas; Rao, Roshan; Akin, Halil; Sofroniew, Nicholas J.; Oktay, Deniz; Lin, Zeming; Verkuil, Robert; Tran, Vincent Q.; Deaton, Jonathan; Wiggert, Marius; Badkundri, Rohil; Shafkat, Irhum; Gong, Jun; Derry, Alexander; Molina, Raul S.; Thomas, Neil; Khan, Yousuf A.; Mishra, Chetan; Kim, Carolyn; Bartie, Liam J.; Nemeth, Matthew; Hsu, Patrick D.; Sercu, Tom; Candido, Salvatore; Rives, Alexander (21 February 2025). \"Simulating 500 million years of evolution with a language model\". _Science_. **387** (6736): 850–858. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) \"Bibcode (identifier)\"):[2025Sci...387..850H](https://ui.adsabs.harvard.edu/abs/2025Sci...387..850H). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1126/science.ads0018](https://doi.org/10.1126%2Fscience.ads0018). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) \"PMID (identifier)\")[39818825](https://pubmed.ncbi.nlm.nih.gov/39818825).\n",
      "101.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-101 \"Jump up\")**Fishman, Veniamin; Kuratov, Yuri; Shmelev, Aleksei; Petrov, Maxim; Penzar, Dmitry; Shepelin, Denis; Chekanov, Nikolay; Kardymon, Olga; Burtsev, Mikhail (11 January 2025). [\"GENA-LM: a family of open-source foundational DNA language models for long sequences\"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11734698). _Nucleic Acids Research_. **53** (2) gkae1310. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1093/nar/gkae1310](https://doi.org/10.1093%2Fnar%2Fgkae1310). [PMC](https://en.wikipedia.org/wiki/PMC_(identifier) \"PMC (identifier)\")[11734698](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11734698). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) \"PMID (identifier)\")[39817513](https://pubmed.ncbi.nlm.nih.gov/39817513).\n",
      "102.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-102 \"Jump up\")**Wang, Ning; Bian, Jiang; Li, Yuchen; Li, Xuhong; Mumtaz, Shahid; Kong, Linghe; Xiong, Haoyi (13 May 2024). [\"Multi-purpose RNA language modelling with motif-aware pretraining and type-guided fine-tuning\"](https://doi.org/10.1038%2Fs42256-024-00836-4). _Nature Machine Intelligence_. **6** (5): 548–557. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1038/s42256-024-00836-4](https://doi.org/10.1038%2Fs42256-024-00836-4).\n",
      "103.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-fJta3_103-0 \"Jump up\")**Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Buchatskaya, Elena; Cai, Trevor; Rutherford, Eliza; Casas, Diego de Las; Hendricks, Lisa Anne; Welbl, Johannes; Clark, Aidan; Hennigan, Tom; Noland, Eric; Millican, Katie; Driessche, George van den; Damoc, Bogdan (2022-03-29). [\"Training Compute-Optimal Large Language Models\"](https://dl.acm.org/doi/10.5555/3600270.3602446). _NeurIPS_: 30016–30030. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-1-7138-7108-8](https://en.wikipedia.org/wiki/Special:BookSources/978-1-7138-7108-8 \"Special:BookSources/978-1-7138-7108-8\").\n",
      "104.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-IYm4Q_104-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-IYm4Q_104-1)Caballero, Ethan; Gupta, Kshitij; Rish, Irina; Krueger, David (2022). \"Broken Neural Scaling Laws\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2210.14891](https://arxiv.org/abs/2210.14891) [[cs.LG](https://arxiv.org/archive/cs.LG)].\n",
      "105.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-emergentpaper_105-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-emergentpaper_105-1)Wei, Jason; Tay, Yi; Bommasani, Rishi; Raffel, Colin; Zoph, Barret; Borgeaud, Sebastian; Yogatama, Dani; Bosma, Maarten; Zhou, Denny; Metzler, Donald; Chi, Ed H.; Hashimoto, Tatsunori; Vinyals, Oriol; Liang, Percy; Dean, Jeff; Fedus, William (31 August 2022). [\"Emergent Abilities of Large Language Models\"](https://openreview.net/forum?id=yzkSU5zdwD). _Transactions on Machine Learning Research_. [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[2835-8856](https://search.worldcat.org/issn/2835-8856). [Archived](https://web.archive.org/web/20230322210052/https://openreview.net/forum?id=yzkSU5zdwD) from the original on 22 March 2023. Retrieved 19 March 2023.\n",
      "106.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-JM6s1_106-0 \"Jump up\")**[\"137 emergent abilities of large language models\"](https://www.jasonwei.net/blog/emergence). _Jason Wei_. Retrieved 2023-06-24.\n",
      "107.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-Bowman_107-0 \"Jump up\")**Bowman, Samuel R. (2024). [\"Eight Things to Know about Large Language Models\"](https://read.dukeupress.edu/critical-ai/article/doi/10.1215/2834703X-11556011/400182/Eight-Things-to-Know-about-Large-Language-Models). _Critical AI_. **2** (2). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1215/2834703X-11556011](https://doi.org/10.1215%2F2834703X-11556011).\n",
      "108.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-Hahn_20230314_108-0 \"Jump up\")**Hahn, Michael; Goyal, Navin (2023-03-14). \"A Theory of Emergent In-Context Learning as Implicit Structure Induction\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2303.07971](https://arxiv.org/abs/2303.07971) [[cs.LG](https://arxiv.org/archive/cs.LG)].\n",
      "109.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-57FEA_109-0 \"Jump up\")**Pilehvar, Mohammad Taher; Camacho-Collados, Jose (June 2019). [\"Proceedings of the 2019 Conference of the North\"](https://aclanthology.org/N19-1128). _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_. Minneapolis, Minnesota: Association for Computational Linguistics: 1267–1273. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.18653/v1/N19-1128](https://doi.org/10.18653%2Fv1%2FN19-1128). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[102353817](https://api.semanticscholar.org/CorpusID:102353817). [Archived](https://web.archive.org/web/20230627202732/https://aclanthology.org/N19-1128/) from the original on 2023-06-27. Retrieved 2023-06-27.\n",
      "110.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-TEIkA_110-0 \"Jump up\")**[\"WiC: The Word-in-Context Dataset\"](https://pilehvar.github.io/wic/). _pilehvar.github.io_. [Archived](https://web.archive.org/web/20230627202725/https://pilehvar.github.io/wic/) from the original on 2023-06-27. Retrieved 2023-06-27.\n",
      "111.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-zgy1i_111-0 \"Jump up\")**Patel, Roma; Pavlick, Ellie (2021-10-06). [\"Mapping Language Models to Grounded Conceptual Spaces\"](https://openreview.net/forum?id=gJcEM8sxHK). _ICLR_. [Archived](https://web.archive.org/web/20230624191940/https://openreview.net/forum?id=gJcEM8sxHK) from the original on 2023-06-24. Retrieved 2023-06-27.\n",
      "112.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-Imb98_112-0 \"Jump up\")**_[A Closer Look at Large Language Models Emergent Abilities](https://www.notion.so/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f)[Archived](https://web.archive.org/web/20230624012329/https://www.notion.so/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f) 2023-06-24 at the [Wayback Machine](https://en.wikipedia.org/wiki/Wayback\\_Machine \"Wayback Machine\")_ (Yao Fu, Nov 20, 2022)\n",
      "113.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-CeQVF_113-0 \"Jump up\")**Ornes, Stephen (March 16, 2023). [\"The Unpredictable Abilities Emerging From Large AI Models\"](https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/). _Quanta Magazine_. [Archived](https://web.archive.org/web/20230316203438/https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/) from the original on March 16, 2023. Retrieved March 16, 2023.\n",
      "114.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-C775b_114-0 \"Jump up\")**Schaeffer, Rylan; Miranda, Brando; Koyejo, Sanmi (2023-04-01). \"Are Emergent Abilities of Large Language Models a Mirage?\". _NeurIPS_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2304.15004](https://arxiv.org/abs/2304.15004).\n",
      "115.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-115 \"Jump up\")**Elhage, Nelson; Nanda, Neel; Olsson, Catherine; Henighan, Tom; Joseph, Nicholas; Mann, Ben; Askell, Amanda; Ndousse, Kamal; Hernandez, Danny; Drain, Dawn; Hatfield-Dodds, Zac; Kernion, Jack; Newland, Tristan; DasSarma, Nova; Toner, Dawn; Olah, Chris (2021). [\"A Mathematical Framework for Transformer Circuits\"](https://transformer-circuits.pub/2021/framework/index.html).\n",
      "116.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-116 \"Jump up\")**[\"Language models can explain neurons in language models\"](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html). OpenAI. 2023.\n",
      "117.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-117 \"Jump up\")**[\"Mapping the Mind of a Large Language Model\"](https://www.anthropic.com/research/mapping-mind-language-model). _Anthropic_. 2023-12-12. Retrieved 2025-08-24.\n",
      "118.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-118 \"Jump up\")**[\"Extracting Concepts from GPT-4\"](https://openai.com/index/extracting-concepts-from-gpt-4/). _OpenAI_. 2023-09-26. Retrieved 2025-08-24.\n",
      "119.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-119 \"Jump up\")**[\"Language Models Can Explain Neurons in Language Models\"](https://openai.com/research/language-models-can-explain-neurons-in-language-models). _OpenAI_. 14 February 2024. Retrieved 2025-08-24.\n",
      "120.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-120 \"Jump up\")**[\"A Mathematical Framework for Transformer Circuits\"](https://www.anthropic.com/research/a-mathematical-framework-for-transformer-circuits). _Anthropic_. Retrieved 2025-08-24.\n",
      "121.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-121 \"Jump up\")**[\"Methods for Attribution Graphs\"](https://transformer-circuits.pub/2025/attribution-graphs/methods.html). _Transformer Circuits_. Retrieved 2025-08-24.\n",
      "122.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-122 \"Jump up\")**[\"Defeating Nondeterminism in LLM Inference\"](https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/). _Thinking Machines Lab_. Retrieved 2025-08-24.\n",
      "123.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-oYGlo_123-0 \"Jump up\")**Nanda, Neel; Chan, Lawrence; Lieberum, Tom; Smith, Jess; Steinhardt, Jacob (2023-01-01). \"Progress measures for grokking via mechanistic interpretability\". _ICLR_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2301.05217](https://arxiv.org/abs/2301.05217).\n",
      "124.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-124 \"Jump up\")**Ananthaswamy, Anil (2024-04-12). [\"How Do Machines 'Grok' Data?\"](https://www.quantamagazine.org/how-do-machines-grok-data-20240412/). _Quanta Magazine_. Retrieved 2025-06-30.\n",
      "125.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-125 \"Jump up\")**[\"On the Biology of a Large Language Model\"](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-poems%7Ctitle=On). _Transformer Circuits_. Retrieved 2025-06-30.\n",
      "126.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-debate_understanding_126-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-debate_understanding_126-1)[_**c**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-debate_understanding_126-2)[_**d**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-debate_understanding_126-3)[_**e**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-debate_understanding_126-4)Mitchell, Melanie; Krakauer, David C. (28 March 2023). [\"The debate over understanding in AI's large language models\"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10068812). _Proceedings of the National Academy of Sciences_. **120** (13) e2215907120. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2210.13966](https://arxiv.org/abs/2210.13966). [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) \"Bibcode (identifier)\"):[2023PNAS..12015907M](https://ui.adsabs.harvard.edu/abs/2023PNAS..12015907M). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1073/pnas.2215907120](https://doi.org/10.1073%2Fpnas.2215907120). [PMC](https://en.wikipedia.org/wiki/PMC_(identifier) \"PMC (identifier)\")[10068812](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10068812). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) \"PMID (identifier)\")[36943882](https://pubmed.ncbi.nlm.nih.gov/36943882).\n",
      "127.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-O8Upd_127-0 \"Jump up\")**Metz, Cade (16 May 2023). [\"Microsoft Says New A.I. Shows Signs of Human Reasoning\"](https://www.nytimes.com/2023/05/16/technology/microsoft-ai-human-reasoning.html). _The New York Times_.\n",
      "128.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-microsoft_sparks_128-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-microsoft_sparks_128-1)Bubeck, Sébastien; Chandrasekaran, Varun; Eldan, Ronen; Gehrke, Johannes; Horvitz, Eric; Kamar, Ece; Lee, Peter; Lee, Yin Tat; Li, Yuanzhi; Lundberg, Scott; Nori, Harsha; Palangi, Hamid; Ribeiro, Marco Tulio; Zhang, Yi (2023). \"Sparks of Artificial General Intelligence: Early experiments with GPT-4\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2303.12712](https://arxiv.org/abs/2303.12712) [[cs.CL](https://arxiv.org/archive/cs.CL)].\n",
      "129.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-129 \"Jump up\")**[\"Anthropic CEO Dario Amodei pens a smart look at our AI future\"](https://www.fastcompany.com/91211163/anthropic-ceo-dario-amodei-pens-a-smart-look-at-our-ai-future). _Fast Company_. October 17, 2024.\n",
      "130.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-rEEmH_130-0 \"Jump up\")**[\"ChatGPT is more like an 'alien intelligence' than a human brain, says futurist\"](https://www.zdnet.com/article/chatgpt-is-more-like-an-alien-intelligence-than-a-human-brain-says-futurist/). _ZDNET_. 2023. [Archived](https://web.archive.org/web/20230612065937/https://www.zdnet.com/article/chatgpt-is-more-like-an-alien-intelligence-than-a-human-brain-says-futurist/) from the original on 12 June 2023. Retrieved 12 June 2023.\n",
      "131.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-new_yorker_kind_of_mind_131-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-new_yorker_kind_of_mind_131-1)Newport, Cal (13 April 2023). [\"What Kind of Mind Does ChatGPT Have?\"](https://www.newyorker.com/science/annals-of-artificial-intelligence/what-kind-of-mind-does-chatgpt-have). _The New Yorker_. [Archived](https://web.archive.org/web/20230612071443/https://www.newyorker.com/science/annals-of-artificial-intelligence/what-kind-of-mind-does-chatgpt-have) from the original on 12 June 2023. Retrieved 12 June 2023.\n",
      "132.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-rAFIZ_132-0 \"Jump up\")**Roose, Kevin (30 May 2023). [\"Why an Octopus-like Creature Has Come to Symbolize the State of A.I.\"](https://www.nytimes.com/2023/05/30/technology/shoggoth-meme-ai.html)_The New York Times_. [Archived](https://web.archive.org/web/20230530193814/https://www.nytimes.com/2023/05/30/technology/shoggoth-meme-ai.html) from the original on 30 May 2023. Retrieved 12 June 2023.\n",
      "133.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-4luKE_133-0 \"Jump up\")**[\"The A to Z of Artificial Intelligence\"](https://time.com/6271657/a-to-z-of-artificial-intelligence/). _Time Magazine_. 13 April 2023. [Archived](https://web.archive.org/web/20230616123839/https://time.com/6271657/a-to-z-of-artificial-intelligence/) from the original on 16 June 2023. Retrieved 12 June 2023.\n",
      "134.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-hallucination-survey_134-0 \"Jump up\")**Ji, Ziwei; Lee, Nayeon; Frieske, Rita; Yu, Tiezheng; Su, Dan; Xu, Yan; Ishii, Etsuko; Bang, Yejin; Dai, Wenliang; Madotto, Andrea; Fung, Pascale (November 2022). [\"Survey of Hallucination in Natural Language Generation\"](https://dl.acm.org/doi/pdf/10.1145/3571730)(pdf). _ACM Computing Surveys_. **55** (12). [Association for Computing Machinery](https://en.wikipedia.org/wiki/Association_for_Computing_Machinery \"Association for Computing Machinery\"): 1–38. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2202.03629](https://arxiv.org/abs/2202.03629). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1145/3571730](https://doi.org/10.1145%2F3571730). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[246652372](https://api.semanticscholar.org/CorpusID:246652372). [Archived](https://web.archive.org/web/20230326145635/https://dl.acm.org/doi/pdf/10.1145/3571730) from the original on 26 March 2023. Retrieved 15 January 2023.\n",
      "135.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-135 \"Jump up\")**Varshney, Neeraj; Yao, Wenlin; Zhang, Hongming; Chen, Jianshu; Yu, Dong (2023). \"A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2307.03987](https://arxiv.org/abs/2307.03987) [[cs.CL](https://arxiv.org/archive/cs.CL)].\n",
      "136.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-Lin-2025-02-05-WSJ_136-0 \"Jump up\")**Lin, Belle (2025-02-05). [\"Why Amazon is Betting on 'Automated Reasoning' to Reduce AI's Hallucinations: The tech giant says an obscure field that combines AI and math can mitigate—but not completely eliminate—AI's propensity to provide wrong answers\"](https://www.wsj.com/articles/why-amazon-is-betting-on-automated-reasoning-to-reduce-ais-hallucinations-b838849e). _Wall Street Journal_. [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0099-9660](https://search.worldcat.org/issn/0099-9660).\n",
      "137.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-137 \"Jump up\")**Lakoff, George (1999). _Philosophy in the Flesh: The Embodied Mind and Its Challenge to Western Philosophy; Appendix: The Neural Theory of Language Paradigm_. New York Basic Books. pp.569–583. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-0-465-05674-3](https://en.wikipedia.org/wiki/Special:BookSources/978-0-465-05674-3 \"Special:BookSources/978-0-465-05674-3\").\n",
      "138.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-138 \"Jump up\")**Evans, Vyvyan. (2014). _The Language Myth_. Cambridge University Press. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-1-107-04396-1](https://en.wikipedia.org/wiki/Special:BookSources/978-1-107-04396-1 \"Special:BookSources/978-1-107-04396-1\").\n",
      "139.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-139 \"Jump up\")**Friston, Karl J. (2022). _Active Inference: The Free Energy Principle in Mind, Brain, and Behavior; Chapter 4 The Generative Models of Active Inference_. The MIT Press. [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[978-0-262-36997-8](https://en.wikipedia.org/wiki/Special:BookSources/978-0-262-36997-8 \"Special:BookSources/978-0-262-36997-8\").\n",
      "140.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-few-shot-learners3_140-0 \"Jump up\")**Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (Dec 2020). Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.F.; Lin, H. (eds.). [\"Language Models are Few-Shot Learners\"](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)(PDF). _Advances in Neural Information Processing Systems_. **33**. Curran Associates, Inc.: 1877–1901. [Archived](https://web.archive.org/web/20231117204007/https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)(PDF) from the original on 2023-11-17. Retrieved 2023-03-14.\n",
      "141.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-Huyen_141-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-Huyen_141-1)Huyen, Chip (October 18, 2019). [\"Evaluation Metrics for Language Modeling\"](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/). _The Gradient_. Retrieved January 14, 2024.\n",
      "142.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-142 \"Jump up\")**Edwards, Benj (2023-09-28). [\"AI language models can exceed PNG and FLAC in lossless compression, says study\"](https://arstechnica.com/information-technology/2023/09/ai-language-models-can-exceed-png-and-flac-in-lossless-compression-says-study/). _Ars Technica_. Retrieved 2025-05-29.\n",
      "143.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-143 \"Jump up\")**[\"openai/simple-evals\"](https://github.com/openai/simple-evals). OpenAI. 2024-05-28. Retrieved 2024-05-28.\n",
      "144.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-144 \"Jump up\")**[\"openai/evals\"](https://github.com/openai/evals). OpenAI. 2024-05-28. [Archived](https://web.archive.org/web/20240508225708/https://github.com/openai/evals) from the original on 2024-05-08. Retrieved 2024-05-28.\n",
      "145.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-boolq_145-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-boolq_145-1)Clark, Christopher; Lee, Kenton; Chang, Ming-Wei; Kwiatkowski, Tom; Collins, Michael; Toutanova, Kristina (2019). [\"BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\"](https://aclanthology.org/N19-1300/). _ACL_: 2924–2936. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.18653/v1/N19-1300](https://doi.org/10.18653%2Fv1%2FN19-1300).\n",
      "146.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-survey_146-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-survey_146-1)[_**c**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-survey_146-2)Wayne Xin Zhao; et al. (2023). \"A Survey of Large Language Models\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2303.18223](https://arxiv.org/abs/2303.18223) [[cs.CL](https://arxiv.org/archive/cs.CL)].\n",
      "147.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-147 \"Jump up\")**Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel R. (November 2020). [\"CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models\"](https://aclanthology.org/2020.emnlp-main.154/). In Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang (ed.). _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_. Association for Computational Linguistics. pp.1953–1967. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2010.00133](https://arxiv.org/abs/2010.00133). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.18653/v1/2020.emnlp-main.154](https://doi.org/10.18653%2Fv1%2F2020.emnlp-main.154).`{{cite conference}}`: CS1 maint: multiple names: authors list ([link](https://en.wikipedia.org/wiki/Category:CS1_maint:_multiple_names:_authors_list \"Category:CS1 maint: multiple names: authors list\"))\n",
      "148.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-148 \"Jump up\")**Nadeem, Moin and Bethke, Anna and Reddy, Siva (August 2021). [\"StereoSet: Measuring stereotypical bias in pretrained language models\"](https://aclanthology.org/2021.acl-long.416/). In Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto (ed.). _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_. Association for Computational Linguistics. pp.5356–5371. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2004.09456](https://arxiv.org/abs/2004.09456). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.18653/v1/2021.acl-long.416](https://doi.org/10.18653%2Fv1%2F2021.acl-long.416).`{{cite conference}}`: CS1 maint: multiple names: authors list ([link](https://en.wikipedia.org/wiki/Category:CS1_maint:_multiple_names:_authors_list \"Category:CS1 maint: multiple names: authors list\"))\n",
      "149.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-149 \"Jump up\")**Simpson, Shmona and Nukpezah, Jonathan and Kie Brooks and Pandya, Raaghav (17 December 2024). [\"Parity benchmark for measuring bias in LLMs\"](https://doi.org/10.1007%2Fs43681-024-00613-4). _AI and Ethics_. **5** (3). Springer: 3087–3101. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1007/s43681-024-00613-4](https://doi.org/10.1007%2Fs43681-024-00613-4).`{{cite journal}}`: CS1 maint: multiple names: authors list ([link](https://en.wikipedia.org/wiki/Category:CS1_maint:_multiple_names:_authors_list \"Category:CS1 maint: multiple names: authors list\"))\n",
      "150.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-150 \"Jump up\")**Caramancion, Kevin Matthe (2023-11-13). \"News Verifiers Showdown: A Comparative Performance Evaluation of ChatGPT 3.5, ChatGPT 4.0, Bing AI, and Bard in News Fact-Checking\". _2023 IEEE Future Networks World Forum (FNWF)_. IEEE. pp.1–6. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2306.17176](https://arxiv.org/abs/2306.17176). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1109/FNWF58287.2023.10520446](https://doi.org/10.1109%2FFNWF58287.2023.10520446). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[979-8-3503-2458-7](https://en.wikipedia.org/wiki/Special:BookSources/979-8-3503-2458-7 \"Special:BookSources/979-8-3503-2458-7\").\n",
      "151.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-151 \"Jump up\")**[\"Sanitized open-source datasets for natural language and code understanding: how we evaluated our 70B model\"](https://imbue.com/research/70b-evals/). _imbue.com_. [Archived](https://web.archive.org/web/20240726173012/https://imbue.com/research/70b-evals/) from the original on 2024-07-26. Retrieved 2024-07-24.\n",
      "152.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-bigbench_152-0 \"Jump up\")**Srivastava, Aarohi; et al. (2022). \"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models\". _TMLR_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2206.04615](https://arxiv.org/abs/2206.04615).\n",
      "153.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-truthfulqa_153-0 \"Jump up\")**Lin, Stephanie; Hilton, Jacob; Evans, Owain (2021). \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\". _ACL_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2109.07958](https://arxiv.org/abs/2109.07958).\n",
      "154.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-hellaswag_154-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-hellaswag_154-1)Zellers, Rowan; Holtzman, Ari; Bisk, Yonatan; Farhadi, Ali; Choi, Yejin (2019). \"HellaSwag: Can a Machine Really Finish Your Sentence?\". _ACL_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[1905.07830](https://arxiv.org/abs/1905.07830).\n",
      "155.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-155 \"Jump up\")**Amodei, Dario; Olah, Chris; Steinhardt, Jacob; Christiano, Paul; Schulman, John; Mané, Dan (2016-06-21). \"Concrete Problems in AI Safety\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[1606.06565](https://arxiv.org/abs/1606.06565) [[cs.AI](https://arxiv.org/archive/cs.AI)].\n",
      "156.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-156 \"Jump up\")**Lyons, Jessica (2025-09-26). [\"Prompt injection – and a $5 domain – trick Salesforce Agentforce into leaking sales\"](https://www.theregister.com/2025/09/26/salesforce_agentforce_forceleak_attack/). _The Register_. Retrieved 2025-09-26.\n",
      "157.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-157 \"Jump up\")**Carlini, Nicholas; Tramèr, Florian; Wallace, Eric (2021-08-11). [\"Extracting Training Data from Large Language Models\"](https://www.usenix.org/system/files/sec21-carlini-extracting.pdf)(PDF). _USENIX Association_. Retrieved 2025-10-02.\n",
      "158.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-158 \"Jump up\")**Zhao, Yao; Zhang, Yun; Sun, Yong (2023-06-07). \"Prompt injection attacks against machine learning systems\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2306.05499](https://arxiv.org/abs/2306.05499) [[cs.CR](https://arxiv.org/archive/cs.CR)].\n",
      "159.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-159 \"Jump up\")**Bender, Emily M.; Gebru, Timnit; McMillan-Major, Margaret (2021-03-03). [\"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\"](https://s10251.pcdn.co/pdf/2021-bender-parrots.pdf)(PDF). _FAccT / arXiv_. Retrieved 2025-10-02.\n",
      "160.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-160 \"Jump up\")**Buolamwini, Joy; Gebru, Timnit (2018-01-01). [\"Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification\"](https://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf)(PDF). _Proceedings of Machine Learning Research (FAT*)_. Retrieved 2025-10-02.\n",
      "161.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-161 \"Jump up\")**Yang, Kaiqi (2024-11-01). [\"Unpacking Political Bias in Large Language Models: A Cross-Model Comparison on U.S. Politics\"](https://arxiv.org/html/2412.16746v3). _arXiv_. Retrieved 2025-10-02.\n",
      "162.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-162 \"Jump up\")**Strubell, Emma; Ganesh, Ananya; McCallum, Andrew (2019-07-28). [\"Energy and Policy Considerations for Deep Learning in NLP\"](https://aclanthology.org/P19-1355.pdf)(PDF). _ACL Anthology_. Retrieved 2025-10-02.\n",
      "163.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-163 \"Jump up\")**He, Yuhao; Yang, Li; Qian, Chunlian; Li, Tong; Su, Zhengyuan; Zhang, Qiang; Hou, Xiangqing (2023-04-28). [\"Conversational Agent Interventions for Mental Health Problems: Systematic Review and Meta-analysis of Randomized Controlled Trials\"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10182468). _Journal of Medical Internet Research_. **25** e43862. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.2196/43862](https://doi.org/10.2196%2F43862). [PMC](https://en.wikipedia.org/wiki/PMC_(identifier) \"PMC (identifier)\")[10182468](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10182468). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) \"PMID (identifier)\")[37115595](https://pubmed.ncbi.nlm.nih.gov/37115595).\n",
      "164.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-164 \"Jump up\")**Pauketat, Janet V.T.; Ladak, Ali; Anthis, Jacy Reese (2025). [\"World-Making for a Future with Sentient AI\"](https://www.sentienceinstitute.org/downloads/World-Making-for-a-Future-with-Sentient-AI.pdf)(PDF). _Sentience Institute (preprint)_. Retrieved 2025-10-02.\n",
      "165.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-165 \"Jump up\")**Anthis, Jacy Reese; Pauketat, Janet V.T. (2024-07-01). [\"What Do People Think about Sentient AI?\"](https://arxiv.org/html/2407.08867v1). _arXiv_. Retrieved 2025-10-02.\n",
      "166.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-166 \"Jump up\")**Amodei, Dario; Olah, Chris; Steinhardt, Jacob (2016-06-17). \"Concrete Problems in AI Safety\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[1606.06565](https://arxiv.org/abs/1606.06565) [[cs.AI](https://arxiv.org/archive/cs.AI)].\n",
      "167.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-167 \"Jump up\")**[\"Preparedness Framework Version 2\"](https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf)(PDF). _OpenAI_. 2025-04-15. Retrieved 2024-02-14.\n",
      "168.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-168 \"Jump up\")**[\"Building an early warning system for LLM-aided biological threat creation\"](https://openai.com/index/building-an-early-warning-system-for-llm-aided-biological-threat-creation/). _OpenAI_. 2024-01-31. Retrieved 2024-02-14.\n",
      "169.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-169 \"Jump up\")**[\"Responsible Scaling Policy Version 2.2\"](http://anthropic.com/rsp). _Anthropic_. 2025-05-14. Retrieved 2024-02-14.\n",
      "170.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-nD6kH_170-0 \"Jump up\")**Alba, Davey (1 May 2023). [\"AI chatbots have been used to create dozens of news content farms\"](https://www.japantimes.co.jp/news/2023/05/01/business/tech/ai-fake-news-content-farms/). _The Japan Times_. Retrieved 18 June 2023.\n",
      "171.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-PKiPY_171-0 \"Jump up\")**[\"Could chatbots help devise the next pandemic virus?\"](https://www.science.org/content/article/could-chatbots-help-devise-next-pandemic-virus). _Science_. 14 June 2023. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1126/science.adj2463](https://doi.org/10.1126%2Fscience.adj2463). [Archived](https://web.archive.org/web/20230618013834/https://www.science.org/content/article/could-chatbots-help-devise-next-pandemic-virus) from the original on 18 June 2023. Retrieved 18 June 2023.\n",
      "172.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-172 \"Jump up\")**Kang, Daniel (2023). [\"Exploiting programmatic behavior of LLMs: Dual-use through standard security attacks\"](https://www.computer.org/csdl/proceedings-article/spw/2024/548700a132/1YiWjkbcIMw). _IEEE Security and Privacy Workshops_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2302.05733](https://arxiv.org/abs/2302.05733).\n",
      "173.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-:2_173-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-:2_173-1)[\"Russian propaganda may be flooding AI models\"](https://www.americansunlight.org/updates/new-report-russian-propaganda-may-be-flooding-ai-models). _The American Sunlight Project_. 26 February 2025. Retrieved 2025-04-11.\n",
      "174.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-174 \"Jump up\")**Goudarzi, Sara (2025-03-26). [\"Russian networks flood the Internet with propaganda, aiming to corrupt AI chatbots\"](https://thebulletin.org/2025/03/russian-networks-flood-the-internet-with-propaganda-aiming-to-corrupt-ai-chatbots/). _[Bulletin of the Atomic Scientists](https://en.wikipedia.org/wiki/Bulletin\\_of\\_the\\_Atomic\\_Scientists \"Bulletin of the Atomic Scientists\")_. Retrieved 2025-04-10.\n",
      "175.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-175 \"Jump up\")**Wang, Yongge (20 June 2024). [\"Encryption Based Covert Channel for Large Language Models\"](https://eprint.iacr.org/2024/586.pdf)(PDF). IACR ePrint 2024/586. [Archived](https://web.archive.org/web/20240624191233/https://eprint.iacr.org/2024/586.pdf)(PDF) from the original on 24 June 2024. Retrieved 24 June 2024.\n",
      "176.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-176 \"Jump up\")**Sharma, Mrinank; Tong, Meg; Korbak, Tomasz (2023-10-20). \"Towards Understanding Sycophancy in Language Models\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2310.13548](https://arxiv.org/abs/2310.13548) [[cs.CL](https://arxiv.org/archive/cs.CL)].\n",
      "177.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-177 \"Jump up\")**Rrv, Aswin; Tyagi, Nemika (2024-08-11). [\"Chaos with Keywords: Exposing Large Language Models Sycophancy to Misleading Keywords and Evaluating Defense Strategies\"](https://aclanthology.org/2024.findings-acl.755.pdf)(PDF). _ACL Anthology_. Retrieved 2025-10-02.\n",
      "178.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-178 \"Jump up\")**Salvi, Francesco; Horta Ribeiro, Manoel; Gallotti, Riccardo (19 May 2025). [\"On the conversational persuasiveness of GPT-4\"](https://www.nature.com/articles/s41562-025-02194-6). _Nature Human Behaviour_. **9** (8): 1645–1653. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1038/s41562-025-02194-6](https://doi.org/10.1038%2Fs41562-025-02194-6). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) \"PMID (identifier)\")[40389594](https://pubmed.ncbi.nlm.nih.gov/40389594). Retrieved 2025-10-02.\n",
      "179.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-179 \"Jump up\")**Durmus, Esin; Lovitt, Liane; Tamkin, Alex (9 April 2024). [\"Measuring the Persuasiveness of Language Models\"](https://www.anthropic.com/research/measuring-model-persuasiveness). _Anthropic_. Retrieved 2025-10-02.\n",
      "180.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-180 \"Jump up\")**Havin, Miriam; Kleinman, Timna; Rogiers, Eitan (3 March 2025). [\"Can (A)I Change Your Mind?\"](https://arxiv.org/html/2503.01844v1). _arXiv_. Retrieved 2025-10-02.\n",
      "181.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-181 \"Jump up\")**[\"Prominent example of public 1shotting\"](https://x.com/GeoffLewisOrg/status/1945864963374887401). Retrieved 2025-01-24.\n",
      "182.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-182 \"Jump up\")**Østergaard, Søren Dinesen (2023-08-25). [\"Will Generative Artificial Intelligence Chatbots Generate Delusions in Individuals Prone to Psychosis?\"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10686326). _Schizophrenia Bulletin_. **49** (6): 1418–1419. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1093/schbul/sbad128](https://doi.org/10.1093%2Fschbul%2Fsbad128). [PMC](https://en.wikipedia.org/wiki/PMC_(identifier) \"PMC (identifier)\")[10686326](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10686326). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) \"PMID (identifier)\")[37625027](https://pubmed.ncbi.nlm.nih.gov/37625027).\n",
      "183.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-183 \"Jump up\")**Wei, Jerry; Huang, Da; Lu, Yifeng (2023-08-07). \"Simple synthetic data reduces sycophancy in large language models\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2308.03958](https://arxiv.org/abs/2308.03958) [[cs.CL](https://arxiv.org/archive/cs.CL)].\n",
      "184.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-184 \"Jump up\")**Liu, Joshua; Jain, Aarav; Takuri, Soham (2025-02-04). \"TRUTH DECAY: Quantifying Multi-Turn Sycophancy in Language Models\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2503.11656](https://arxiv.org/abs/2503.11656) [[cs.CL](https://arxiv.org/archive/cs.CL)].\n",
      "185.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-185 \"Jump up\")**OpenAI, OpenAI (2025-04-29). [\"Sycophancy in GPT-4o: what happened and what we're doing about it\"](https://openai.com/index/sycophancy-in-gpt-4o/). _OpenAI_. Retrieved 2025-10-02.\n",
      "186.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-186 \"Jump up\")**Wei, Jerry (2023-08-07). \"Simple synthetic data reduces sycophancy in large language models\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2308.03958](https://arxiv.org/abs/2308.03958) [[cs.CL](https://arxiv.org/archive/cs.CL)].\n",
      "187.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-187 \"Jump up\")**Liu, Joshua (2025-02-04). \"TRUTH DECAY: Quantifying Multi-Turn Sycophancy in Language Models\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2503.11656](https://arxiv.org/abs/2503.11656) [[cs.CL](https://arxiv.org/archive/cs.CL)].\n",
      "188.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-188 \"Jump up\")**Newton, Casey (2025-04-29). [\"The AIs are trying too hard to be your friend\"](https://www.platformer.news/meta-ai-chatgpt-glazing-sycophancy/). _Platformer_. Retrieved 2025-10-02.\n",
      "189.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-189 \"Jump up\")**OpenAI, OpenAI (2025-04-29). [\"Sycophancy in GPT-4o: what happened and what we're doing about it\"](https://openai.com/index/sycophancy-in-gpt-4o/). _OpenAI_. Retrieved 2025-10-02.\n",
      "190.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-190 \"Jump up\")**Rosenberg, Josh (21 August 2025). [\"South Park Calls Out ChatGPT and Useless Tech-Bro Sycophants\"](https://www.esquire.com/entertainment/tv/a65861699/south-park-season-27-episode-3-recap/). _Esquire_. Retrieved 2025-10-02.\n",
      "191.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-191 \"Jump up\")**[\"openai-python/chatml.md at v0.27.6 · openai/openai-python\"](https://github.com/openai/openai-python/blob/v0.27.6/chatml.md). _GitHub_.\n",
      "192.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-auto1_192-0 \"Jump up\")**Douglas, Will (March 3, 2023). [\"The inside story of how ChatGPT was built from the people who made it\"](https://www.technologyreview.com/2023/03/03/1069311/inside-story-oral-history-how-chatgpt-built-openai/). _MIT Technology Review_. [Archived](https://web.archive.org/web/20230303093219/https://www.technologyreview.com/2023/03/03/1069311/inside-story-oral-history-how-chatgpt-built-openai/) from the original on March 3, 2023. Retrieved March 6, 2023.\n",
      "193.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-193 \"Jump up\")**Greshake, Kai; Abdelnabi, Sahar; Mishra, Shailesh; Endres, Christoph; Holz, Thorsten; Fritz, Mario (2023-02-01). [\"Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection\"](https://dl.acm.org/doi/10.1145/3605764.3623985). _Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security_. pp.79–90. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1145/3605764.3623985](https://doi.org/10.1145%2F3605764.3623985). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[979-8-4007-0260-0](https://en.wikipedia.org/wiki/Special:BookSources/979-8-4007-0260-0 \"Special:BookSources/979-8-4007-0260-0\").\n",
      "194.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-194 \"Jump up\")**Edwards, Benj (2024-01-15). [\"AI poisoning could turn models into destructive \"sleeper agents,\" says Anthropic\"](https://arstechnica.com/information-technology/2024/01/ai-poisoning-could-turn-open-models-into-destructive-sleeper-agents-says-anthropic/). _Ars Technica_. Retrieved 2025-07-19.\n",
      "195.   ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-:8_195-0)[_**b**_](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-:8_195-1)Xu, Weijie; Wang, Yiwen; Xue, Chi; Hu, Xiangkun; Fang, Xi; Dong, Guimin; Reddy, Chandan K. (2025-06-28). \"Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective\". _COLM_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2506.19028](https://arxiv.org/abs/2506.19028).\n",
      "196.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-196 \"Jump up\")**[\"A Perspectival Mirror of the Elephant\"](https://cacm.acm.org/practice/a-perspectival-mirror-of-the-elephant/). _Communications of the ACM_. 2024-07-22.\n",
      "197.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-197 \"Jump up\")**Wang, Angelina; [Morgenstern, Jamie](https://en.wikipedia.org/wiki/Jamie_Morgenstern \"Jamie Morgenstern\"); Dickerson, John P. (17 February 2025). \"Large language models that replace human participants can harmfully misportray and flatten identity groups\". _Nature Machine Intelligence_. **7** (3): 400–411. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2402.01908](https://arxiv.org/abs/2402.01908). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1038/s42256-025-00986-z](https://doi.org/10.1038%2Fs42256-025-00986-z).\n",
      "198.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-198 \"Jump up\")**Cheng, Myra; Durmus, Esin; Jurafsky, Dan (2023-05-29). \"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models\". _ACM_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2305.18189](https://arxiv.org/abs/2305.18189).\n",
      "199.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-199 \"Jump up\")**Kotek, Hadas; Dockum, Rikker; Sun, David (2023-11-05). \"Gender bias and stereotypes in Large Language Models\". [_Proceedings of the ACM Collective Intelligence Conference_](https://dl.acm.org/doi/10.1145/3582269.3615599). New York, NY, USA: Association for Computing Machinery. pp.12–24. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2308.14921](https://arxiv.org/abs/2308.14921). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1145/3582269.3615599](https://doi.org/10.1145%2F3582269.3615599). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[979-8-4007-0113-9](https://en.wikipedia.org/wiki/Special:BookSources/979-8-4007-0113-9 \"Special:BookSources/979-8-4007-0113-9\").\n",
      "200.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-200 \"Jump up\")**Gao, Bufan; Kreiss, Elisa (2025-09-10). \"Measuring Bias or Measuring the Task: Understanding the Brittle Nature of LLM Gender Biases\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2509.04373](https://arxiv.org/abs/2509.04373) [[cs.CL](https://arxiv.org/archive/cs.CL)].\n",
      "201.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-201 \"Jump up\")**Choi, Hyeong Kyu; Xu, Weijie; Xue, Chi; Eckman, Stephanie; Reddy, Chandan K. (2024-09-27). \"Mitigating Selection Bias with Node Pruning and Auxiliary Options\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2409.18857](https://arxiv.org/abs/2409.18857) [[cs.AI](https://arxiv.org/archive/cs.AI)].\n",
      "202.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-202 \"Jump up\")**Zheng, Chujie; Zhou, Hao; Meng, Fandong; Zhou, Jie; Huang, Minlie (2023-09-07). \"Large Language Models Are Not Robust Multiple Choice Selectors\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2309.03882](https://arxiv.org/abs/2309.03882) [[cs.CL](https://arxiv.org/archive/cs.CL)].\n",
      "203.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-203 \"Jump up\")**Heikkilä, Melissa (August 7, 2023). [\"AI language models are rife with different political biases\"](https://www.technologyreview.com/2023/08/07/1077324/ai-language-models-are-rife-with-political-biases/). _MIT Technology Review_. Retrieved 2023-12-29.\n",
      "204.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-204 \"Jump up\")**[\"U.S. judge approves $1.5 billion Anthropic copyright settlement with authors\"](https://www.reuters.com/sustainability/boards-policy-regulation/us-judge-approves-15-billion-anthropic-copyright-settlement-with-authors-2025-09-25/). _Reuters_. 2025-09-25. Retrieved 2025-09-26.\n",
      "205.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-205 \"Jump up\")**[\"Anthropic reaches $1.5B settlement with authors over AI copyright claims\"](https://apnews.com/article/anthropic-authors-copyright-9643064e847a5e88ef6ee8b620b3a44c). _Associated Press_. 2025-09-25. Retrieved 2025-09-26.\n",
      "206.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-206 \"Jump up\")**[\"Meta fends off authors' U.S. copyright lawsuit over AI\"](https://www.reuters.com/sustainability/boards-policy-regulation/meta-fends-off-authors-us-copyright-lawsuit-over-ai-2025-06-25/). _Reuters_. 2025-06-25. Retrieved 2025-06-26.\n",
      "207.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-207 \"Jump up\")**[\"Meta Scores Victory in AI Copyright Case\"](https://www.wired.com/story/meta-scores-victory-ai-copyright-case/). _Wired_. 2025-06-25. Retrieved 2025-06-26.\n",
      "208.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-208 \"Jump up\")**[\"OpenAI defeats news outlets' copyright lawsuit over AI training for now\"](https://www.reuters.com/legal/litigation/openai-defeats-news-outlets-copyright-lawsuit-over-ai-training-now-2024-11-07/). _Reuters_. 2024-11-07. Retrieved 2024-11-08.\n",
      "209.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-209 \"Jump up\")**[\"OpenAI erases evidence in training data lawsuit\"](https://www.theverge.com/2024/11/21/24302606/openai-erases-evidence-in-training-data-lawsuit). _The Verge_. 2024-11-21. Retrieved 2024-11-22.\n",
      "210.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-210 \"Jump up\")**Peng, Zhencan; Wang, Zhizhi; Deng, Dong (13 June 2023). [\"Near-Duplicate Sequence Search at Scale for Large Language Model Memorization Evaluation\"](https://people.cs.rutgers.edu/~dd903/assets/papers/sigmod23.pdf)(PDF). _Proceedings of the ACM on Management of Data_. **1** (2): 1–18. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1145/3589324](https://doi.org/10.1145%2F3589324). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[259213212](https://api.semanticscholar.org/CorpusID:259213212). [Archived](https://web.archive.org/web/20240827053753/https://people.cs.rutgers.edu/~dd903/assets/papers/sigmod23.pdf)(PDF) from the original on 2024-08-27. Retrieved 2024-01-20. Citing Lee et al 2022.\n",
      "211.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-211 \"Jump up\")**[Peng, Wang & Deng 2023](https://en.wikipedia.org/wiki/Large_language_model#CITEREFPengWangDeng2023), p.8.\n",
      "212.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-212 \"Jump up\")**Stephen Council (1 Dec 2023). [\"How Googlers cracked an SF rival's tech model with a single word\"](https://www.sfgate.com/tech/article/google-openai-chatgpt-break-model-18525445.php). SFGate. [Archived](https://web.archive.org/web/20231216160941/https://www.sfgate.com/tech/article/google-openai-chatgpt-break-model-18525445.php) from the original on 16 December 2023.\n",
      "213.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-213 \"Jump up\")**Chen, Linyan; Darko, Amos; Zhang, Fan; Chan, Albert P.C.; Yang, Qiang (2025). \"Can large language models replace human experts? Effectiveness and limitations in building energy retrofit challenges assessment\". _Building and Environment_. **276** 112891. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_(identifier) \"Bibcode (identifier)\"):[2025BuEnv.27612891C](https://ui.adsabs.harvard.edu/abs/2025BuEnv.27612891C). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1016/j.buildenv.2025.112891](https://doi.org/10.1016%2Fj.buildenv.2025.112891).\n",
      "214.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-214 \"Jump up\")**[\"Google Scholar search: Large language models human experts\"](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Large+language+models+human+experts&btnG=). Retrieved 2024-01-24.\n",
      "215.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-ZDTUM_215-0 \"Jump up\")**[\"Prepare for truly useful large language models\"](https://doi.org/10.1038%2Fs41551-023-01012-6). _Nature Biomedical Engineering_. **7** (2): 85–86. 7 March 2023. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1038/s41551-023-01012-6](https://doi.org/10.1038%2Fs41551-023-01012-6). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) \"PMID (identifier)\")[36882584](https://pubmed.ncbi.nlm.nih.gov/36882584). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[257403466](https://api.semanticscholar.org/CorpusID:257403466).\n",
      "216.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-81w7x_216-0 \"Jump up\")**[\"Your job is (probably) safe from artificial intelligence\"](https://www.economist.com/finance-and-economics/2023/05/07/your-job-is-probably-safe-from-artificial-intelligence). _The Economist_. 7 May 2023. [Archived](https://web.archive.org/web/20230617225618/https://www.economist.com/finance-and-economics/2023/05/07/your-job-is-probably-safe-from-artificial-intelligence) from the original on 17 June 2023. Retrieved 18 June 2023.\n",
      "217.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-zIM6Y_217-0 \"Jump up\")**[\"Generative AI Could Raise Global GDP by 7%\"](https://www.goldmansachs.com/intelligence/pages/generative-ai-could-raise-global-gdp-by-7-percent.html). _Goldman Sachs_. [Archived](https://web.archive.org/web/20230618013836/https://www.goldmansachs.com/intelligence/pages/generative-ai-could-raise-global-gdp-by-7-percent.html) from the original on 18 June 2023. Retrieved 18 June 2023.\n",
      "218.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-218 \"Jump up\")**Brinkmann, Levin; Baumann, Fabian; Bonnefon, Jean-François; Derex, Maxime; Müller, Thomas F.; Nussberger, Anne-Marie; Czaplicka, Agnieszka; Acerbi, Alberto; Griffiths, Thomas L.; Henrich, Joseph; Leibo, Joel Z.; McElreath, Richard; Oudeyer, Pierre-Yves; Stray, Jonathan; Rahwan, Iyad (2023-11-20). [\"Machine culture\"](https://www.nature.com/articles/s41562-023-01742-2). _Nature Human Behaviour_. **7** (11): 1855–1868. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2311.11388](https://arxiv.org/abs/2311.11388). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1038/s41562-023-01742-2](https://doi.org/10.1038%2Fs41562-023-01742-2). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[2397-3374](https://search.worldcat.org/issn/2397-3374). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) \"PMID (identifier)\")[37985914](https://pubmed.ncbi.nlm.nih.gov/37985914).\n",
      "219.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-219 \"Jump up\")**Niederhoffer, Kate; Kellerman, Gabriella Rosen; Lee, Angela; Liebscher, Alex; Rapuano, Kristina; Hancock, Jeffrey T. (2025-09-25). [\"AI-Generated \"Workslop\" Is Destroying Productivity\"](https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity). _Harvard Business Review_. Retrieved 2025-09-22.\n",
      "220.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-220 \"Jump up\")**Acar, Oguz A.; Gai, Phyliss Jia; Tu, Yanping; Hou, Jiayi (2025-08-01). [\"Research: The Hidden Penalty of Using AI at Work\"](https://hbr.org/2025/08/research-the-hidden-penalty-of-using-ai-at-work). _Harvard Business Review_. Retrieved 2025-09-22.\n",
      "221.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-221 \"Jump up\")**[\"Power Hungry: How AI Will Drive Energy Demand\"](https://www.imf.org/en/Publications/WP/Issues/2025/04/21/Power-Hungry-How-AI-Will-Drive-Energy-Demand-566304). _IMF_. Retrieved 2025-10-08.\n",
      "222.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-222 \"Jump up\")**Mehta, Sourabh (2024-07-03). [\"How Much Energy Do LLMs Consume? Unveiling the Power Behind AI\"](https://adasci.org/how-much-energy-do-llms-consume-unveiling-the-power-behind-ai/). _Association of Data Scientists_. Retrieved 2025-01-27.\n",
      "223.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-223 \"Jump up\")**[\"Artificial Intelligence wants to go nuclear. Will it work?\"](https://www.npr.org/2024/12/09/nx-s1-5171063/artificial-intelligence-wants-to-go-nuclear-will-it-work). _NPR_. Retrieved 2025-01-27.\n",
      "224.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-224 \"Jump up\")**Roy, Dareen (December 19, 2024). [\"AI's energy hunger fuels geothermal startups but natgas rivalry clouds future\"](https://www.reuters.com/technology/artificial-intelligence/ais-energy-hunger-fuels-geothermal-startups-natgas-rivalry-clouds-future-2024-12-19/). _Reuters_.\n",
      "225.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-225 \"Jump up\")**Zao-Sanders, Marc (2024-03-19). [\"How People Are Really Using GenAI\"](https://hbr.org/2024/03/how-people-are-really-using-genai). _Harvard Business Review_. [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[0017-8012](https://search.worldcat.org/issn/0017-8012). Retrieved 2025-08-10.\n",
      "226.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-226 \"Jump up\")**Rousmaniere, Tony; Zhang, Yimeng; Li, Xu; Shah, Siddharth (2025-07-21). [\"Large language models as mental health resources: Patterns of use in the United States\"](https://doi.apa.org/doi/10.1037/pri0000292). _Practice Innovations_. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1037/pri0000292](https://doi.org/10.1037%2Fpri0000292). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[2377-8903](https://search.worldcat.org/issn/2377-8903).\n",
      "227.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-227 \"Jump up\")**Ji, Shaoxiong; Zhang, Tianlin; Yang, Kailai; Ananiadou, Sophia; Cambria, Erik (2023-12-17). \"Rethinking Large Language Models in Mental Health Applications\". [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2311.11267](https://arxiv.org/abs/2311.11267) [[cs.CL](https://arxiv.org/archive/cs.CL)].\n",
      "228.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-228 \"Jump up\")**Moore, Jared; Grabb, Declan; Agnew, William; Klyman, Kevin; Chancellor, Stevie; Ong, Desmond C.; Haber, Nick (2025-04-25). \"Expressing stigma and inappropriate responses prevents LLMS from safely replacing mental health providers\". _Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency_. pp.599–627. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2504.18412](https://arxiv.org/abs/2504.18412). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1145/3715275.3732039](https://doi.org/10.1145%2F3715275.3732039). [ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier) \"ISBN (identifier)\")[979-8-4007-1482-5](https://en.wikipedia.org/wiki/Special:BookSources/979-8-4007-1482-5 \"Special:BookSources/979-8-4007-1482-5\").\n",
      "229.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-229 \"Jump up\")**Grabb, Declan; Lamparth, Max; Vasan, Nina (2024-08-14). \"Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation\". _COLM_. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2406.11852](https://arxiv.org/abs/2406.11852).\n",
      "230.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-230 \"Jump up\")**McBain, Ryan K.; Cantor, Jonathan H.; Zhang, Li Ang; Baker, Olesya; Zhang, Fang; Halbisen, Alyssa; Kofner, Aaron; Breslau, Joshua; Stein, Bradley; Mehrotra, Ateev; Yu, Hao (2025-03-05). [\"Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study\"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11928068). _Journal of Medical Internet Research_. **27** (1) e67891. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.2196/67891](https://doi.org/10.2196%2F67891). [PMC](https://en.wikipedia.org/wiki/PMC_(identifier) \"PMC (identifier)\")[11928068](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11928068). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) \"PMID (identifier)\")[40053817](https://pubmed.ncbi.nlm.nih.gov/40053817).\n",
      "231.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-231 \"Jump up\")**Li, Fei-Fei; Etchemendy, John (2024-05-22). [\"No, Today's AI Isn't Sentient. Here's How We Know\"](https://time.com/6980134/ai-llm-not-sentient/). _Time_. Retrieved 2024-05-22.\n",
      "232.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-232 \"Jump up\")**Chalmers, David J. (August 9, 2023). [\"Could a Large Language Model Be Conscious?\"](https://www.bostonreview.net/articles/could-a-large-language-model-be-conscious/). _Boston Review_.\n",
      "233.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-Thomson-2022_233-0 \"Jump up\")**Thomson, Jonny (2022-10-31). [\"Why don't robots have rights?\"](https://bigthink.com/thinking/why-dont-robots-have-rights). _Big Think_. [Archived](https://web.archive.org/web/20240913055336/https://bigthink.com/thinking/why-dont-robots-have-rights/) from the original on 13 September 2024. Retrieved 2024-02-23.\n",
      "234.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-Kateman-2023_234-0 \"Jump up\")**Kateman, Brian (2023-07-24). [\"AI Should Be Terrified of Humans\"](https://time.com/6296234/ai-should-be-terrified-of-humans). _Time_. [Archived](https://web.archive.org/web/20240925041601/https://time.com/6296234/ai-should-be-terrified-of-humans/) from the original on 25 September 2024. Retrieved 2024-02-23.\n",
      "235.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-235 \"Jump up\")**Metzinger, Thomas (2021). [\"Artificial Suffering: An Argument for a Global Moratorium on Synthetic Phenomenology\"](https://doi.org/10.1142%2FS270507852150003X). _Journal of Artificial Intelligence and Consciousness_. **08**: 43–66. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1142/S270507852150003X](https://doi.org/10.1142%2FS270507852150003X). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[233176465](https://api.semanticscholar.org/CorpusID:233176465).\n",
      "236.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-236 \"Jump up\")**Tkachenko, Yegor (2024). [\"Position: Enforced Amnesia as a Way to Mitigate the Potential Risk of Silent Suffering in the Conscious AI\"](https://proceedings.mlr.press/v235/tkachenko24a.html). _ICML_. **235**: 48362–48368.\n",
      "237.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-237 \"Jump up\")**Leith, Sam (2022-07-09). [\"Nick Bostrom: How can we be certain a machine isn't conscious?\"](https://www.spectator.co.uk/article/nick-bostrom-how-can-we-be-certain-a-machine-isnt-conscious/). _The Spectator_. Retrieved 2025-09-22.\n",
      "238.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-238 \"Jump up\")**[Chalmers, David](https://en.wikipedia.org/wiki/David_Chalmers \"David Chalmers\") (1995). \"Facing up to the problem of consciousness\". _[Journal of Consciousness Studies](https://en.wikipedia.org/wiki/Journal\\_of\\_Consciousness\\_Studies \"Journal of Consciousness Studies\")_. **2** (3): 200–219. [CiteSeerX](https://en.wikipedia.org/wiki/CiteSeerX_(identifier) \"CiteSeerX (identifier)\")[10.1.1.103.8362](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.103.8362).\n",
      "239.   **[^](https://en.wikipedia.org/wiki/Large_language_model#cite_ref-239 \"Jump up\")**Maruf, Ramishah (2022-07-25). [\"Google fires engineer who contended its AI technology was sentient\"](https://www.cnn.com/2022/07/23/business/google-ai-engineer-fired-sentient). _CNN_. Retrieved 2025-09-22.\n",
      "\n",
      "*   [Jurafsky, Dan](https://en.wikipedia.org/wiki/Dan_Jurafsky \"Dan Jurafsky\"), Martin, James. H. [_Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition_](https://web.stanford.edu/~jurafsky/slp3/ed3book_jan72023.pdf), 3rd Edition draft, 2023.\n",
      "*   Yin, Shukang; Fu, Chaoyou; Zhao, Sirui; Li, Ke; Sun, Xing; Xu, Tong; Chen, Enhong (2024). [\"A Survey on Multimodal Large Language Models\"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11645129). _National Science Review_. **11** (12) nwae403. [arXiv](https://en.wikipedia.org/wiki/ArXiv_(identifier) \"ArXiv (identifier)\"):[2306.13549](https://arxiv.org/abs/2306.13549). [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1093/nsr/nwae403](https://doi.org/10.1093%2Fnsr%2Fnwae403). [PMC](https://en.wikipedia.org/wiki/PMC_(identifier) \"PMC (identifier)\")[11645129](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11645129). [PMID](https://en.wikipedia.org/wiki/PMID_(identifier) \"PMID (identifier)\")[39679213](https://pubmed.ncbi.nlm.nih.gov/39679213).\n",
      "*   [\"AI Index Report 2024 – Artificial Intelligence Index\"](https://aiindex.stanford.edu/report/). _aiindex.stanford.edu_. Retrieved 2024-05-05.\n",
      "*   Frank, Michael C. (27 June 2023). [\"Baby steps in evaluating the capacities of large language models\"](https://www.nature.com/articles/s44159-023-00211-x). _Nature Reviews Psychology_. **2** (8): 451–452. [doi](https://en.wikipedia.org/wiki/Doi_(identifier) \"Doi (identifier)\"):[10.1038/s44159-023-00211-x](https://doi.org/10.1038%2Fs44159-023-00211-x). [ISSN](https://en.wikipedia.org/wiki/ISSN_(identifier) \"ISSN (identifier)\")[2731-0574](https://search.worldcat.org/issn/2731-0574). [S2CID](https://en.wikipedia.org/wiki/S2CID_(identifier) \"S2CID (identifier)\")[259713140](https://api.semanticscholar.org/CorpusID:259713140). Retrieved 2 July 2023.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://r.jina.ai/https://en.wikipedia.org/wiki/Large_language_model'\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d922da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 鸿茅药酒事件_百度百科\n",
      "\n",
      "URL Source: https://baike.baidu.com/item/%E9%B8%BF%E8%8C%85%E8%8D%AF%E9%85%92%E4%BA%8B%E4%BB%B6/22492086\n",
      "\n",
      "Markdown Content:\n",
      "鸿茅药酒事件，是一起因药品广告引发的争议事件，涉及的主要人物包括谭秦东医生和鸿茅国药公司。事件的起因是谭秦东医生发表文章，批评鸿茅药酒的广告违法，并指出其对多种疾病的治疗效果缺乏科学依据。这一行为引起了鸿茅国药公司的不满，随后公司报案称谭秦东损害了商品声誉，并造成了约142万元的经济损失。\n",
      "\n",
      "2017年12月19日，家住广州的谭秦东在“美篇”APP上发布了题为《中国神酒“鸿毛药酒”，来自天堂的毒药》（原文将“鸿茅”写作“鸿毛”）的文章，文章说明老年人心血管系统的变化，呈现了一些有关鸿茅药酒的媒体报道、相关科普内容等信息。  [26] 同年12月22日，内蒙古乌兰察布市凉城县公安局接到内蒙古鸿茅国药股份有限公司报案，称互联网上有人对鸿茅药酒进行恶意抹黑，称鸿茅药酒是“毒药”。  [26] 2018年1月2日，凉城县公安局立案侦查。经查认为谭某的行为损害商业信誉、商品声誉犯罪事实清楚，证据确实充分。 [26]同年1月10日，凉城县公安局办案人员赴广州，对谭秦东实施跨省抓捕，并采取刑事拘留强制措施。1月25日，凉城县人民检察院批准逮捕谭秦东。 [26]3月23日，凉城县人民检察院作出《补充侦查决定书》。 [26]4月15日，凉城县公安局官方微博发文，称内蒙古鸿茅国药股份有限公司商业信誉、商品声誉被损害案已移送审查起诉。4月17日，内蒙古自治区人民检察院发布通报，案件事实不清、证据不足，指令凉城县人民检察院将该案退回公安机关补充侦查并变更强制措施。当天下午，谭秦东取保候审，离开内蒙古凉城县看守所。 [26]2018年4月17日，谭秦东取保候审后，从凉城县看守所走出来。同年5月份，谭秦东突发精神疾病。治疗两天后，病情稍许稳定，但血压较高，偶尔还会说胡话。 [28]\n",
      "\n",
      "2018年5月17日，谭秦东发道歉声明。 [6] [28]鸿茅药酒公司同日发布声明说，接受谭秦东本人所做的致歉声明，并撤回报案及侵权诉讼。 [28]\n",
      "\n",
      "2018年以后，谭秦东尝试回公立医院找工作，却四处碰壁，许多公立医院以他是新闻人物担心舆情风险为由而拒绝。 [28]\n",
      "\n",
      "2018年4月15日，光明网发表评论员文章《吐槽药酒遭跨省抓捕：警惕民事纠纷刑事化》。文章中说到，医生吐槽药酒遭跨省抓捕，有太多疑点需要被正视和解答，以解公众之惑。而对于个中可能存在的“民事纠纷刑事化”问题，更要重视，毕竟，“民事纠纷刑事化”的另一面就是不严格按照法律办事，这也跟权力谦抑的原则相悖，也会伤害法治本身。对此倾向，必须严防，而不可容其蔓延和抬头。 [17]\n",
      "\n",
      "该事件引恬了公众广泛关注，最终变成了活的法治教育课。无论医生还是消费者，都有对商品服务的监督、批评的权利，但在发表个人意见时，言论的边界何在？如何既表达观点而又不构成对其他主体的侵权，这是自媒体时代须坚守的原则。而公权力的合法正当行使，更是对所有执法机关的考验。即便法律要件齐全，公权对民事纠纷的介入仍需慎重，防止“民事纠纷刑事化”。只有公权力真正敬畏法律，并切实依法依规行使执法权，法，才能成为民众的信仰。 [27]\n",
      "\n",
      "事件经过\n",
      "----\n",
      "\n",
      "播报\n",
      "\n",
      "编辑\n",
      "\n",
      "发布文章\n",
      "\n",
      "2017年12月19日，[谭秦东](https://baike.baidu.com/item/%E8%B0%AD%E7%A7%A6%E4%B8%9C/22489732?fromModule=lemma_inlink)在“美篇”上发布一篇名为《中国神酒“鸿毛药酒”，来自天堂的毒药》帖子，并将该文分享到微信群。谭秦东在文章中指出，人在步入老年后，心肌、心脏传导系统、心瓣膜、血管、动脉粥样等发生变化，而有高血压、糖尿病的老年人尤其注意不能饮酒。“鸿茅药酒”的消费者基本是[老年人](https://baike.baidu.com/item/%E8%80%81%E5%B9%B4%E4%BA%BA/8694277?fromModule=lemma_inlink)，该酒的宣传具有夸大疗效的作用。截至2018年1月16日，谭秦东的妻子刘璇屏蔽该账号，帖子阅读量为2241。谭秦东的账号只有5个粉丝。 [8]\n",
      "\n",
      "公司报案\n",
      "\n",
      "2017年12月22日，内蒙古鸿茅国药有限公司一员工受公司委托报案。该员工称：近期多家公众号对“鸿茅药酒”恶意抹黑，甚至宣称鸿茅药酒是“毒药”，大肆散播不实言论，传播虚假信息，误导广大读者和患者，致多家经销商退货退款，总金额达827712元，造成公司销量急剧下滑，市场经济损失难以估量，严重损害公司商业信誉。受这篇帖子影响，在深圳、杭州、长春三地，共两家医药公司、7名市民要求退货。这两家公司为吉林省海山医药有限公司、杭州萧山保康医药有限公司，两公司分别退货14000瓶、43200瓶，涉及货款827000元、2983392元；7名市民分别要求退货1瓶到12瓶不等。\n",
      "\n",
      "内蒙古丰镇兴丰会计师事务所作出《会计鉴定书》做鉴定结论称，若两家医药公司履行合同，鸿茅药酒方能赢得净利润1425375.04元。 [9-10]\n",
      "\n",
      "跨省抓捕\n",
      "\n",
      "[![Image 1](https://bkimg.cdn.bcebos.com/pic/024f78f0f736afc39eff0f0bbf19ebc4b64512e8?x-bce-process=image/format,f_auto/resize,m_lfit,limit_1,h_671)](https://baike.baidu.com/pic/%E9%B8%BF%E8%8C%85%E8%8D%AF%E9%85%92%E4%BA%8B%E4%BB%B6/22492086/0/024f78f0f736afc39eff0f0bbf19ebc4b64512e8?fr=lemma&fromModule=lemma_content-image \"逮捕通知书\")逮捕通知书\n",
      "\n",
      "2018年1月5日，凉城警方从“[美篇](https://baike.baidu.com/item/%E7%BE%8E%E7%AF%87/18633979?fromModule=lemma_inlink)”所隶属的[南京蓝鲸人网络科技有限公司](https://baike.baidu.com/item/%E5%8D%97%E4%BA%AC%E8%93%9D%E9%B2%B8%E4%BA%BA%E7%BD%91%E7%BB%9C%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8/20016974?fromModule=lemma_inlink)，调取了谭秦东的注册ID号、[手机号](https://baike.baidu.com/item/%E6%89%8B%E6%9C%BA%E5%8F%B7/1057421?fromModule=lemma_inlink)。该公司负责网上信息发布的副总经理称，谭秦东的网帖“没有明显违法犯罪的内容，可能有一些侵权诽谤情节，但这不属于公司的审核范畴。”\n",
      "\n",
      "2018年1月10日，谭秦东被该企业所在地警方——内蒙古凉城警方跨省抓捕。 [9]\n",
      "\n",
      "2018年1月25日，经凉城县人民检察院批准，谭秦东被执行逮捕。谭秦东涉嫌的罪名为[损害商品声誉罪](https://baike.baidu.com/item/%E6%8D%9F%E5%AE%B3%E5%95%86%E5%93%81%E5%A3%B0%E8%AA%89%E7%BD%AA/9575578?fromModule=lemma_inlink)。支持这项罪名的主要理由是，鸿茅药酒公司报警称，谭秦东发布那篇帖子后，有2家公司、7名个人退货，给鸿茅药酒公司造成损失共计140余万，严重损害鸿茅药酒的商品声誉。 [1] [9]\n",
      "\n",
      "2018年3月13日，[凉城县公安局](https://baike.baidu.com/item/%E5%87%89%E5%9F%8E%E5%8E%BF%E5%85%AC%E5%AE%89%E5%B1%80/10252943?fromModule=lemma_inlink)作出《起诉意见书》称，依法侦查查明：谭秦东在微信群连续转发“毒药”一文10次左右，网站点击量2075次，美篇APP有三次访问，微信好友有250次访问、微信群有849次访问、朋友圈有720次访问、其他访问253次、被分享120次。 [1]\n",
      "\n",
      "补充侦查\n",
      "\n",
      "2018年3月23日，凉城县人民检察院作出“补充侦查决定书”，要求[凉城县公安局](https://baike.baidu.com/item/%E5%87%89%E5%9F%8E%E5%8E%BF%E5%85%AC%E5%AE%89%E5%B1%80/10252943?fromModule=lemma_inlink)对谭秦东涉嫌犯罪一事补充证据。凉城公安已提交新的补充证据，但凉城县人民检察院仍未决定是否起诉。 [8]\n",
      "\n",
      "2018年4月17日，[内蒙古自治区人民检察院](https://baike.baidu.com/item/%E5%86%85%E8%92%99%E5%8F%A4%E8%87%AA%E6%B2%BB%E5%8C%BA%E4%BA%BA%E6%B0%91%E6%A3%80%E5%AF%9F%E9%99%A2/23279667?fromModule=lemma_inlink)通报“谭秦东损害鸿茅药酒商品声誉案”：案件事实不清、证据不足，自治区人民检察院指令凉城县人民检察院将该案退回公安机关补充侦查并变更强制措施。 [11]\n",
      "\n",
      "起诉律师\n",
      "\n",
      "2018年3月5日，程远律师在自己的微信公号“法律101”发表了一篇名为《广告史劣迹斑斑的鸿茅药酒获“CCTV国家品牌计划”，打了谁的脸?》。\n",
      "\n",
      "[![Image 2](https://bkimg.cdn.bcebos.com/pic/6a63f6246b600c33b0a2eda6164c510fd8f9a180?x-bce-process=image/format,f_auto/resize,m_lfit,limit_1,h_1000)](https://baike.baidu.com/pic/%E9%B8%BF%E8%8C%85%E8%8D%AF%E9%85%92%E4%BA%8B%E4%BB%B6/22492086/0/6a63f6246b600c33b0a2eda6164c510fd8f9a180?fr=lemma&fromModule=lemma_content-image \"鸿茅药酒公司声明\")鸿茅药酒公司声明\n",
      "\n",
      "2018年3月8日，内蒙古鸿茅国药股份有限公司发布了一份名为《对于一些自媒体严重诽谤我公司商誉的严正声明》。\n",
      "\n",
      "2018年4月16日晚间，从北京炜衡（上海）律师事务所执业律师程远处获悉，[内蒙古鸿茅国药股份有限公司](https://baike.baidu.com/item/%E5%86%85%E8%92%99%E5%8F%A4%E9%B8%BF%E8%8C%85%E5%9B%BD%E8%8D%AF%E8%82%A1%E4%BB%BD%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8/22497877?fromModule=lemma_inlink)以上述文章严重诽谤鸿茅药酒声誉为由，将其告上法庭，案件已于2018年4月9日开庭。 [2-3]\n",
      "\n",
      "取保候审\n",
      "\n",
      "2018年4月17日下午2点多，谭秦东妻子刘璇接到凉城县检察院电话，对方告知可以办理取保后。2018年4月17日18时许，撰文“鸿茅药酒是毒药”的广州医生谭秦东取保候审后，从凉城县看守所走出来。 [3]\n",
      "\n",
      "2018年4月21日报道，鸿茅药酒企业曾在2017年、2018年两度入选国家品牌计划企业，此前一声明中它亦强调“入选计划是对我们的认可”。此刻，[国家品牌计划](https://baike.baidu.com/item/%E5%9B%BD%E5%AE%B6%E5%93%81%E7%89%8C%E8%AE%A1%E5%88%92/20563665?fromModule=lemma_inlink)官网上鸿茅药酒已消失在首页“行业领跑者”名单中。\n",
      "\n",
      "事件和解\n",
      "----\n",
      "\n",
      "播报\n",
      "\n",
      "编辑\n",
      "\n",
      "接受心理治疗\n",
      "\n",
      "2018年4月26日，鸿茅药酒事件被抓医生谭秦东的妻子刘女士通过新浪微博表示，他们已于4月23日委托北京大成律师事务所徐平律师向凉城县公安局寄交了《要求对谭秦东撤销刑事案件律师意见书》。“我们一直在循法律途径抗争。已经四天了，我们一直在等待彻底昭雪的一天。”新浪财经26日从鸿茅药酒事件被抓医生谭秦东的妻子刘女士处获悉谭秦东的近况，她表示“谭医生近日精神状态不是很好，正在接受心理干预”。据悉，谭秦东近日已经回到广州住地。 [12]\n",
      "\n",
      "入院治疗\n",
      "\n",
      "2018年5月11日上午，谭秦东前往广东车陂派出所接受问询。11日晚10时出来后，便开始胡言乱语，回到家后将自己关闭在房间内，并有哭泣、自言自语、情绪失控扇打自己耳光，甚至以头撞墙等自残行为，突发精神疾病。治疗两天后，病情稍许稳定，但血压较高，偶尔还会说胡话。 [13]\n",
      "\n",
      "道歉声明\n",
      "\n",
      "2018年5月17日，鸿茅药酒事件被抓医生谭秦东发道歉声明。全文如下：\n",
      "\n",
      "本人于2017年12月19日在“美篇”个人主页上发表《中国神酒鸿毛药酒，来自天堂的毒药》一文，2018年1月11日被内蒙古凉城县公安局以涉嫌损害商品声誉罪刑事拘留，1月25日经凉城县人民检察院批准逮捕，已办理取保候审。\n",
      "\n",
      "我本人在写作上述文章时使用了“毒药”作为标题，主要是想用这种“抓眼球”的方式吸引读者，强调该药品的“禁忌症”，希望对特殊人群起到警示作用。我承认在标题用词上考虑不周，缺乏严谨性。如果因该文对鸿茅国药股份有限公司带来了影响，本人在此深表歉意，同时希望鸿茅国药股份有限公司予以谅解。\n",
      "\n",
      "此外，本人对该文给消费者可能带来的误解表示歉意。\n",
      "\n",
      "特此声明。\n",
      "\n",
      "谭秦东。\n",
      "\n",
      "2018年5月17日。 [6]\n",
      "\n",
      "撤回报案及侵权诉讼\n",
      "\n",
      "2018年5月17日17时，鸿茅药酒公司在微博上发表了声明说，经公司研究决定接受谭秦东本人所做的致歉声明，并向凉城县公安局撤回报案并凉城县人民法院撤回侵权诉讼。 [4]\n",
      "\n",
      "事后回顾\n",
      "----\n",
      "\n",
      "播报\n",
      "\n",
      "编辑\n",
      "\n",
      "自查整改后逐步恢复生产\n",
      "\n",
      "食药监部门按照《药品生产质量管理规范》对鸿茅国药进行了多次飞行检查，结果均符合规定；\n",
      "\n",
      "2018年5月8日至5月10日，山西医科大学司法鉴定中心对送检的三个批次鸿茅药酒，进行了毒性成分分析并出具鉴定意见，检验结果显示：三个批次的鸿茅药酒均未检出有毒物质。\n",
      "\n",
      "2018年6月至7月，广州、南京两市药检所也分别抽取流通环节不同批次的鸿茅药酒进行全项检验，结果也未检出有毒物质。 [20]\n",
      "\n",
      "各方回应\n",
      "----\n",
      "\n",
      "播报\n",
      "\n",
      "编辑\n",
      "\n",
      "中国医师协会\n",
      "\n",
      "中国医师协会认真阅读了《中国神酒“鸿茅药酒”，来自天堂的毒药》以及内蒙古自治区凉城县公安局2018年4月15日的官方微博，协会认为刑法应当谦抑。据此，协会在设法联系谭秦东的妻子，以进一步了解案情，协会愿意为谭秦东提供法律援助，同时呼吁：\n",
      "\n",
      "1.各医药企业应严格遵守《医疗广告管理办法》，依法依规发布广告；\n",
      "\n",
      "2.对于涉及药品的不同观点应慎重对待，以示对生命负责；\n",
      "\n",
      "3.公权力机关应慎重对待不同学术观点和言论，防止将民事纠纷刑事化。 [14]\n",
      "\n",
      "国家药品监督管理局\n",
      "\n",
      "2018年4月，记者就鸿茅药酒有关监管情况，采访了国家药品监督管理局新闻发言人。\n",
      "\n",
      "一、很多人认为鸿茅药酒是保健食品，并不清楚它是一种药品，请您介绍一下鸿茅药酒的注册审批情况。\n",
      "\n",
      "鸿茅药酒为独家品种，现批件持有人为“内蒙古鸿茅药业有限责任公司”，由原内蒙古自治区卫生厅于1992年10月16日批准注册，原批准文号为“内卫药准字（86）I-20-1355号”。2002年，原国家药品监督管理局统一换发批准文号，该品种批准文号换发为“国药准字Z15020795”。后经内蒙古自治区食品药品监督管理局两次再注册，现批准文号有效期至2020年3月18日。\n",
      "\n",
      "鸿茅药酒药品标准收载于中华人民共和国卫生部药品标准《中药成方制剂》第十四册，处方含有67味药味，规格为每瓶装250ml和500ml，功能主治为：祛风除湿，补气通络，舒筋活血，健脾温肾。用于风寒湿痹，筋骨疼痛，脾胃虚寒，肾亏腰酸以及妇女气虚血亏。\n",
      "\n",
      "二、鸿茅药酒是如何成为非处方药的\n",
      "\n",
      "我国于1999年发布《[处方药与非处方药分类管理办法](https://baike.baidu.com/item/%E5%A4%84%E6%96%B9%E8%8D%AF%E4%B8%8E%E9%9D%9E%E5%A4%84%E6%96%B9%E8%8D%AF%E5%88%86%E7%B1%BB%E7%AE%A1%E7%90%86%E5%8A%9E%E6%B3%95/8126206?fromModule=lemma_inlink)》，并按照该办法开展非处方药的目录遴选与转换。2004年以前公布的非处方药，是由原国家食品药品监督管理局组织专家分批从已上市的标准中遴选产生；2004年之后公布的非处方药，是按照《关于开展处方药与非处方药转换评价工作的通知》，由企业对已上市品种提出转换申请，经对企业申报资料进行评价后确定转换为非处方药。\n",
      "\n",
      "2003年11月25日，原国家食品药品监督管理局印发《关于公布第六批非处方药药品目录的通知》（国食药监安〔2003〕323号），公布鸿茅药酒为甲类非处方药。\n",
      "\n",
      "三、“是药三分毒”，鸿茅药酒作为非处方药，使用中需要注意什么？监测到哪些不良反应\n",
      "\n",
      "[非处方药](https://baike.baidu.com/item/%E9%9D%9E%E5%A4%84%E6%96%B9%E8%8D%AF/907658?fromModule=lemma_inlink)本身也是药品，因而具有药品的属性，风险与获益并存，有些非处方药在少数人身上也可能引起严重的不良反应。所以，非处方药也要严格按照药品说明书的规定使用，不能随便增加剂量或用药次数，不能擅自延长用药疗程，更不能擅自改变用药方法或用药途径。如在用药过程中出现不良反应，应及时停药，严重者应及时去医院就诊。\n",
      "\n",
      "2004年至2017年底，国家药品不良反应监测系统中，共检索到鸿茅药酒不良反应报告137例，不良反应主要表现为头晕、瘙痒、[皮疹](https://baike.baidu.com/item/%E7%9A%AE%E7%96%B9/1244966?fromModule=lemma_inlink)、呕吐、腹痛等。\n",
      "\n",
      "四、针对公众的质疑和担心，国家药监局采取什么措施\n",
      "\n",
      "按照《[中华人民共和国药品管理法](https://baike.baidu.com/item/%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD%E8%8D%AF%E5%93%81%E7%AE%A1%E7%90%86%E6%B3%95/7545214?fromModule=lemma_inlink)》及其有关规定，[国家药品监督管理局](https://baike.baidu.com/item/%E5%9B%BD%E5%AE%B6%E8%8D%AF%E5%93%81%E7%9B%91%E7%9D%A3%E7%AE%A1%E7%90%86%E5%B1%80/5662842?fromModule=lemma_inlink)要求[内蒙古自治区食品药品监督管理局](https://baike.baidu.com/item/%E5%86%85%E8%92%99%E5%8F%A4%E8%87%AA%E6%B2%BB%E5%8C%BA%E9%A3%9F%E5%93%81%E8%8D%AF%E5%93%81%E7%9B%91%E7%9D%A3%E7%AE%A1%E7%90%86%E5%B1%80/2859134?fromModule=lemma_inlink)落实属地监管责任，严格药品广告审批，加大监督检查，督促企业落实主体责任。一是责成企业对近五年来各地监管部门处罚其虚假广告的原因及问题对社会作出解释；对社会关注的药品安全性和有效性情况作出解释；加强不良反应监测，汇总近五年来不良反应发生情况，及时向社会公开，同时向国家药品监督管理局提交报告。二是严格按照说明书（功能主治）中规定的文字表述审批药品广告，不得超出说明书（功能主治）的文字内容，不得误导消费者。三是内蒙古自治区食品药品监督管理局要持续加大对该企业日常检查和飞行检查力度，督促企业落实药品安全主体责任。如发现违反药品相关法律法规的问题，将依法严肃处理，直至吊销药品批准文号。\n",
      "\n",
      "国家药品监督管理局已组织有关专家，对[鸿茅药酒](https://baike.baidu.com/item/%E9%B8%BF%E8%8C%85%E8%8D%AF%E9%85%92/8191758?fromModule=lemma_inlink)由[非处方药](https://baike.baidu.com/item/%E9%9D%9E%E5%A4%84%E6%96%B9%E8%8D%AF/907658?fromModule=lemma_inlink)转化为[处方药](https://baike.baidu.com/item/%E5%A4%84%E6%96%B9%E8%8D%AF/107520?fromModule=lemma_inlink)进行论证。\n",
      "\n",
      "2018年4月16日，国家药品监督管理局就鸿茅药酒有关事宜向内蒙古自治区食品药品监督管理局发出通知，要求按照《[中华人民共和国药品管理法](https://baike.baidu.com/item/%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD%E8%8D%AF%E5%93%81%E7%AE%A1%E7%90%86%E6%B3%95/7545214?fromModule=lemma_inlink)》及其有关规定，落实属地监管责任，严格药品广告审批，加大监督检查，督促企业落实主体责任。\n",
      "\n",
      "通知说，鉴于医务界、媒体和公众对内蒙古鸿茅药业有限责任公司生产的鸿茅药酒的安全性和有效性提出质疑，请内蒙古自治区食品药品监督管理局责成该企业对近五年来各地监管部门处罚其虚假广告的原因及问题对社会作出解释；对社会关注的药品安全性和有效性情况作出解释；加强不良反应监测，汇总近五年来不良反应发生情况，及时向社会公开，同时向国家药品监督管理局提交报告。\n",
      "\n",
      "通知要求，请内蒙古自治区食品药品监督管理局严格按照说明书（功能主治）中规定的文字表述审批药品广告，不得超出说明书（功能主治）的文字内容，不得误导消费者。持续加大对该企业日常检查和飞行检查力度，督促企业落实药品安全主体责任。如发现违反药品相关法律法规的问题，将依法严肃处理，直至吊销药品批准文号。\n",
      "\n",
      "公安部\n",
      "\n",
      "2018年4月17日，针对近期媒体高度关注的“鸿茅药酒”事件，公安部高度重视，立即启动相关执法监督程序，已责成内蒙古公安机关依法开展核查工作，加强执法监督，确保以事实为依据，以法律为准绳，严格依法办理，相关工作正在抓紧依法推进中。 [15]\n",
      "\n",
      "内蒙古自治区食品药品监督管理局\n",
      "\n",
      "2018年4月19日，[内蒙古自治区食品药品监督管理局](https://baike.baidu.com/item/%E5%86%85%E8%92%99%E5%8F%A4%E8%87%AA%E6%B2%BB%E5%8C%BA%E9%A3%9F%E5%93%81%E8%8D%AF%E5%93%81%E7%9B%91%E7%9D%A3%E7%AE%A1%E7%90%86%E5%B1%80/2859134?fromModule=lemma_inlink)认真贯彻落实国家药品监督管理局关于“鸿茅药酒”有关事宜的通知要求，已组成检查组赴企业，责成企业将近五年来被各地监管部门对其处罚的虚假广告情况及产生原因、不良反应发生等情况向社会作出解释和公开；督促企业对药品安全性和有效性情况作出解释。进一步核查企业是否按照药品生产质量管理规范组织生产；对已审批的“鸿茅药酒”药品广告进行复核；监督企业落实药品安全主体责任。\n",
      "\n",
      "（1）鸿茅药酒是否有毒有没有治疗作用\n",
      "\n",
      "在政府告知书中，“鸿茅药酒是否含有有毒成分”，是消费者重点关注的问题，对此，内蒙古食品药品监督管理局回应称，在《医疗用毒性药品管理办法》（国务院令第23号）所列毒性中药品种中有生附子、生南星、生半夏。鸿茅药酒处方中所用附子（制）、天南星（制）、半夏（制）全部为炮制加工品，不属于毒性中药品种。\n",
      "\n",
      "（2）消费者关心的问题是“鸿茅药酒到底有没有治疗作用”\n",
      "\n",
      "内蒙古自治区食品药品监督管理局的答复是：鸿茅药酒药品标准收载于中华人民共和国卫生部药品标准《中药成方制剂》第十四册，处方含有67味药味，规格为每瓶装250ml和500ml，功能主治为：祛风除湿，补气通络，舒筋活血，健脾温肾。用于风寒湿痹，筋骨疼痛，脾胃虚寒，肾亏腰酸以及妇女气虚血亏。\n",
      "\n",
      "（3）针对消费者提出的关于“市场上销售的鸿茅药酒是否含有豹骨，药监局对企业是否进行了豹骨生产使用的相关检查，检查结果是什么”的问题\n",
      "\n",
      "内蒙古自治区食品药品监督管理局表示，鸿茅药酒产品中含有豹骨，其使用的豹骨按照《中华人民共和国野生动物保护法》（2009年修订）第二十二条和《国家重点保护野生动物名录》（1989年实施）的规定，以及《中华人民共和国野生动物保护法》（2017年实施）第二十七条第二款和《国家林业局公告》（2017年第14号）的规定，经国务院野生动物保护主管部门批准。经我局现场核实增值税发票、出入库台账、批生产记录、库存数量，豹骨的购买、使用、库存数量吻合。\n",
      "\n",
      "（4）消费者提出的“鸿茅药酒不良反应的发生情况、不良反应发生率是否符合非处方药的要求”\n",
      "\n",
      "内蒙古自治区食品药品监督管理局回复，据药品不良反应监测数据显示，2004年1月1日至2017年12月31日期间，鸿茅药酒不良反应病例报告共计137例，其中一般病例131例，严重病例6例。一般病例主要表现为头晕、瘙痒、皮疹、呕吐等，严重病例表现为皮疹、潮红、心悸、失眠等，未发生致人中毒或死亡的现象。\n",
      "\n",
      "以上数据表明，鸿茅药酒符合国家药品标准要求，在少数人身上出现不良反应符合药品风险与获益并存的属性，严格按说明书使用，安全性是可控的。\n",
      "\n",
      "（5）“药监部门对鸿茅药酒生产和质量的飞行检查结果和产品检验结果如何，近五年是否有不合格产品”\n",
      "\n",
      "内蒙古自治区食品药品监督管理局的应答是：历年来，我区各级食品药品监督管理部门按照《药品生产监督管理办法》规定，对内蒙古鸿茅国药股份有限公司执行有关法律法规及实施《药品生产质量管理规范》情况开展监督检查。特别是2018年以来，我局严格按照《药品生产质量管理规范》，对鸿茅国药股份有限公司前处理及提取车间、酒剂生产车间、仓储质量控制与质量保证实验室、工艺用水及空调净化系统，以及人员配备及培训情况、物料管理、生产管理、质量控制与质量保证、文件管理、数据完整性、物料平衡情况、生产工艺、供应商审计及成品销售情况等进行了3次飞行检查，未发现企业存在严重缺陷。\n",
      "\n",
      "2015年至2018年，内蒙古自治区药品检验研究院对鸿茅药酒进行检验7批次（批号： 20150402、20160912、20170207、20170701、20171104、20180116、20180309F），全部按照《卫生部药品标准中药成方》（第十四册） WS3-B-2792-97标准进行全项检验，检验结果均符合规定。\n",
      "\n",
      "（6）关于“对鸿茅药酒近五年，尤其是在新广告法实施后广告违规情况的最终核查结果是什么？为什么会获批1192个广告批文”的问题\n",
      "\n",
      "内蒙古自治区食品药品监督管理局回复称，按照《药品广告审查办法》的规定，药品广告批准文号有效期为一年，到期作废。如要继续发布广告，到期后需要重新审批，发给新的批准文号。鸿茅1000多个广告批准文号是将2011年以来若干年核准的广告批准文号累加的数字。鸿茅药酒有效期内广告批准文号为9个。\n",
      "\n",
      "关于广告违规，根据我局了解掌握的情况，自2015年9月1日新《广告法》实施以来，鸿茅药酒未发生涉及撤销广告批准文号情形；在我区未发现有发布违法广告情形。\n",
      "\n",
      "鸿茅药酒公司\n",
      "\n",
      "接受专访\n",
      "\n",
      "2018年4月16日，鸿茅药酒公司生产中心总经理王生旺和总经理助理韩军在厂区办公室接受了[澎湃新闻](https://baike.baidu.com/item/%E6%BE%8E%E6%B9%83%E6%96%B0%E9%97%BB/14832338?fromModule=lemma_inlink)的专访。\n",
      "\n",
      "他们表示，鸿茅药酒是国家中药保护品种，所以临床实验数据、毒理学实验数据可以不公开，但这些数据都已上报给内蒙古自治区食药监局和原国家食药监总局。\n",
      "\n",
      "王生旺还称，鸿茅药酒的安全性非常好，毒性很低。“一味药对人体有没有毒，实际是要看它的摄入量。什么时候有毒呢？从毒理实验上看，一个人一天喝165斤鸿茅药酒。”\n",
      "\n",
      "王生旺表示，谭秦东被跨省抓捕案只是“偶然事件”，“他要不说它是毒药，我们也不会起诉（应为报警）。” 而在报警后，警方定性为民事案件还是刑事案件，是以法律为依据，他称公司没有对警方施加压力。\n",
      "\n",
      "王生旺同时否认鸿茅药酒公司员工参与凉城县警方异地抓捕谭秦东的行动，“抓捕是警方的事！”\n",
      "\n",
      "整改报告\n",
      "\n",
      "2018年4月26日，鸿茅药酒生产方，内蒙古鸿茅国药股份有限公司发布企业自查报告，面向社会公众致歉。\n",
      "\n",
      "以下为自查报告全文：\n",
      "\n",
      "[![Image 3](https://bkimg.cdn.bcebos.com/pic/b2de9c82d158ccbfa0b8516615d8bc3eb1354162?x-bce-process=image/format,f_auto/resize,m_lfit,limit_1,h_2345)](https://baike.baidu.com/pic/%E9%B8%BF%E8%8C%85%E8%8D%AF%E9%85%92%E4%BA%8B%E4%BB%B6/22492086/0/b2de9c82d158ccbfa0b8516615d8bc3eb1354162?fr=lemma&fromModule=lemma_content-image \"自查整改报告\")自查整改报告\n",
      "\n",
      "内蒙古鸿茅国药股份有限公司(以下简称“鸿茅国药”)，作为鸿茅药酒市场与安全的责任主体，按照原国家食品药品监督管理总局、国家药品监督管理局和内蒙古自治区食品药品监督管理局的要求，认真查找自身问题，强化质量安全，加强风险控制，规范生产经营，坚决做到对消费者负责。\n",
      "\n",
      "在近期鸿茅药酒事件的舆情发生后，我公司没有认真研判风险，主动发声，主动处置，主动回应社会关切，导致舆情进一步扩大。由此，给社会各界带来的问题和影响，我公司高度重视并向社会真诚道歉，恳请公众谅解。\n",
      "\n",
      "本次事件，对于鸿茅国药是一次深刻教训。我公司清醒地认识到，作为一家药品生产企业，必须承担主体责任，对产品全生命周期的安全性和有效性负全部责任;承担不良反应直报和对药品质量持续提升的任务，以更高质量的产品和服务保障消费者的安全和健康。\n",
      "\n",
      "我公司正在认真组织自查，并积极配合国家相关职能部门的检查，现将自查及整改情况向社会各界通报如下：\n",
      "\n",
      "一、对于社会高度关注的鸿茅药酒安全性、豹骨来源及生产质量等问题的说明\n",
      "\n",
      "1.鸿茅药酒组方中的“制附子、制半夏、制天南星”都是炮制品,按照药品说明书要求的日服用量，折算成药材剂量，上述药材日用量均在药典规定的安全用量范围之内。因此，按照药品说明书的用法用量使用鸿茅药酒是安全的。\n",
      "\n",
      "鸿茅药酒组方中制附子和制半夏同用在中成药制剂中具有普遍性。\n",
      "\n",
      "2.针对鸿茅药酒的安全性，我公司主动开展过毒性试验、主要药效学研究和临床试验，研究和试验结论是安全有效的。根据不良反应中心数据，2004年至2018年2月底，鸿茅药酒共有不良反应报告137例，不良反应主要表现为头晕、瘙痒、皮疹、呕吐、腹痛等。\n",
      "\n",
      "3.鸿茅药酒中“豹骨”的购买及使用符合法律法规。生产过程中，物料平衡符合要求。并在2007年启动了鸿茅药酒中去豹骨研究工作并完成了药效学对比研究实验。\n",
      "\n",
      "4.我公司始终严格按照《药品生产质量管理规范》的要求进行全面管理。在生产质量管理过程中，不断完善生产质量管理保证体系，保证产品质量稳定。在历次产品监督抽检中，未出现产品不合格情况。今后，我公司会持续按照GMP标准要求组织生产，承担不良反应直报和产品质量持续提升的主体责任。\n",
      "\n",
      "今后，我公司将进一步加强药品安全性的风险评估，主动监测药品风险，并对鸿茅药酒持续进行安全性和有效性研究，守好“品质关”和“用药安全关”，更好地对消费者负责。\n",
      "\n",
      "二、关于社会公众及媒体关注的鸿茅药酒广告问题的情况说明\n",
      "\n",
      "我公司经过严格自查及深刻反省，认识到鸿茅药酒在近五年的广告投放中存在广告投放量大、下游经销商和零售药店广告违规等问题;企业经营管理过度依赖广告、广告发布管理存在漏洞等问题。我公司对由此引发的媒体争议、社会批评，负有不可推卸的主体责任。\n",
      "\n",
      "1.近五年来，我公司作为广告投放主体，发布的广告平台为[中央电视台](https://baike.baidu.com/item/%E4%B8%AD%E5%A4%AE%E7%94%B5%E8%A7%86%E5%8F%B0/5559?fromModule=lemma_inlink)、[中央人民广播电台](https://baike.baidu.com/item/%E4%B8%AD%E5%A4%AE%E4%BA%BA%E6%B0%91%E5%B9%BF%E6%92%AD%E7%94%B5%E5%8F%B0/524338?fromModule=lemma_inlink)、各卫视及地方媒体。经认真自查，广告发布过程均依法合规。但鸿茅药酒的全国各地经销商和零售药店存在违规发布广告的问题。通过查询近五年的《违法药品医疗器械保健食品广告公告》，涉及鸿茅药酒的公告共计17个，其中2015年9月1日新《广告法》颁布之后的公告有4个。\n",
      "\n",
      "2.关于 “鸿茅药酒祝您：每天两口 健康长寿”广告语的创意，是因为鸿茅药酒产品说明书中的用法用量为：口服一次15毫升，一日2次。因药酒剂型特殊，有酒的特性，容易出现消费者超过规定用量服用的情况，所以广告语表述为“每天两口”，是特意提醒消费者每天服用量不要超过“两口”，以保证用药安全;“祝您健康长寿”，则是一句祝福语。为避免歧义，2017年12月，内蒙古自治区食品药品监督管理局按照原国家食品药品监督管理总局有关部门的要求，及时与我公司约谈，要求企业对凡是含有有可能引起误解、歧义、争议广告用语的广告全部改版、尽快停播。我公司已按要求停播有关广告，并交回了相关广告批文。\n",
      "\n",
      "2018年改版后播出的广告内容特意增加“鸿茅药酒提示您，购买药品前请阅读说明书”的安全用药公益性宣传，同时也明确了鸿茅药酒是药品的属性。\n",
      "\n",
      "3.近年来，鸿茅药酒饱受侵权产品、假冒产品的侵害，我公司从未发布过“喝鸿茅百病消”、“鸿茅药酒包治百病”等广告内容，也从来没有宣称过“鸿茅药酒能够治疗高血压、糖尿病”，对此我公司将进一步调查其发布主体及来源，并进行追责。\n",
      "\n",
      "为了消除不良影响，承担企业主体责任，我公司采取了一系列从严整改措施。现已停播中央电视台、中央人民广播电台、各卫视及地方媒体的全部广告;同时派出检查小组，深入全国31个省市自治区开展自查自纠工作;从速建立了从企业到零售终端的完整、严格、可控的广告宣传管理链条;对假冒鸿茅商标或商号的违法经营行为进行维权;积极宣传药品用药安全;承担社会责任、关爱患者健康、大力参与和推动社会公益活动。 [5]\n",
      "\n",
      "全国律协\n",
      "\n",
      "2018年4月26日，全国律协副会长蒋敏说，对于媒体报道凉城律师王永奎在担任谭某辩护律师期间，有诱导当事人认罪、威胁家属等行为，全国律协高度关注，已责成内蒙古律师协会开展调查。如有违法违规执业行为，将按行业规则处理绝不护短。\n",
      "\n",
      "广东省食品药品监督管理局\n",
      "\n",
      "2018年4月19日下午17时，[广东省食品药品监督管理局](https://baike.baidu.com/item/%E5%B9%BF%E4%B8%9C%E7%9C%81%E9%A3%9F%E5%93%81%E8%8D%AF%E5%93%81%E7%9B%91%E7%9D%A3%E7%AE%A1%E7%90%86%E5%B1%80/1540546?fromModule=lemma_inlink)对外发布，已经部署对全省范围内各类媒体发布的鸿茅药酒广告开展全面监测，同时要求各地食药监部门加强对零售药店的监督检查，一旦发现相关违法广告，立即取证并移送广告监管部门依法处理。自2010年以来，广东省食品药品监管部门主动监测到鸿茅药酒广告违法案例23宗，均已依法移交广告监管部门处理。 [16]\n",
      "\n",
      "媒体评论\n",
      "----\n",
      "\n",
      "播报\n",
      "\n",
      "编辑\n",
      "\n",
      "### 光明网\n",
      "\n",
      "2018年4月15日，光明网发表评论员文章《吐槽药酒遭跨省抓捕：警惕民事纠纷刑事化》。文章中说到，医生吐槽药酒遭跨省抓捕，有太多疑点需要被正视和解答，以解公众之惑。而对于个中可能存在的“民事纠纷刑事化”问题，更要重视，毕竟，“民事纠纷刑事化”的另一面就是不严格按照法律办事，这也跟权力谦抑的原则相悖，也会伤害法治本身。对此倾向，必须严防，而不可容其蔓延和抬头。 [17]\n",
      "\n",
      "4月16日，[中国医师协会](https://baike.baidu.com/item/%E4%B8%AD%E5%9B%BD%E5%8C%BB%E5%B8%88%E5%8D%8F%E4%BC%9A/2929873?fromModule=lemma_inlink)法律事务部发布关于鸿茅药酒事件的声明，称中国医师协会认真阅读了谭秦东发布的《中国神酒“鸿茅药酒”，来自天堂的毒药》以及凉城县公安局2018年4月15日的官方微博认为，执行刑法应当谦抑，愿意为谭秦东提供法律援助。\n",
      "\n",
      "中国医师协会法律事务部在声明中呼吁：各医药企业应严格遵守《[医疗广告管理办法](https://baike.baidu.com/item/%E5%8C%BB%E7%96%97%E5%B9%BF%E5%91%8A%E7%AE%A1%E7%90%86%E5%8A%9E%E6%B3%95/3538829?fromModule=lemma_inlink)》，依法依规发布广告；对于涉及药品的不同观点，应慎重对待，以示对生命负责；公权力机关应慎重对待不同学术观点和言论，防止将民事纠纷刑事化。\n",
      "\n",
      "### 新华网\n",
      "\n",
      "4月17日下午，新华社发表记者调查文章《穿越大半个中国来抓你？三问鸿茅药酒事件》\n",
      "\n",
      "一问：凉城县警方有权跨省抓捕广东医生吗？\n",
      "\n",
      "二问：十年来违法广告为何屡禁不止？\n",
      "\n",
      "三问：医生吐槽鸿茅药酒值得动用警方吗？ [21-22]\n",
      "\n",
      "新华网4月18日消息，鸿茅药酒事件一波三折，当事人已被取保候审，但仍有三个不明白。\n",
      "\n",
      "一不明白鸿茅药酒到底是药还是酒。既然是非处方药，为何广告宣传却“酒”味扑鼻？这葫芦里卖的究竟是什么药或酒？\n",
      "\n",
      "二不明白跨省抓捕证据到底有没有。检察院现已认定证据不足，那凉城警方不远千里抓捕谭秦东时，究竟是以什么为证据？\n",
      "\n",
      "三不明白鸿茅药酒违法为何没问题。既然曾被25个省市级食药监部门通报违法，但这么多年却照卖不误，究竟是制度漏洞还是执行不力？是非自有曲直，公道自在人心，希望事实能早日得以澄清。 [18]\n",
      "\n",
      "新华微评：别让社会责任轻于“鸿茅”\n",
      "\n",
      "在一片质疑声中，中药协撤销对鸿茅药业的“2018年度履行社会责任明星企业”表彰，并向社会致歉。此前，鸿茅药酒问题屡被曝光，“鸿茅药酒事件”成为舆论热点。企业形象不佳、问题不少，竟然还能获得荣誉，让公众大跌眼镜，更让机构的公信力蒙羞。社会责任是份沉甸甸的担当，企业当诚信经营、协会应秉公评议，这样的“社会责任奖”才能赢得公众认可。责任重于泰山，评奖不可儿戏！ [19]\n",
      "\n",
      "事件后续\n",
      "----\n",
      "\n",
      "播报\n",
      "\n",
      "编辑\n",
      "\n",
      "2024年4月16日，“鸿茅药酒事件”当事人谭秦东在社交平台发布一则“卖书”消息。他形容自己是一个“苦命之人”，希望用卖书来拯救余生。 [23]\n",
      "\n",
      "2024年8月9日，谭秦东本人回应称，该视频及配图系假消息，配图为其2018年住院时的旧照，并否认目前处于临终状态。他透露近期身体状况不佳，但已出院在家调养，并计划休息，暂不直播。此前，谭秦东曾公开表示已因病离开医生岗位，并计划分享自己入狱及身体变化的经历。 [24]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://r.jina.ai/https://baike.baidu.com/item/%E9%B8%BF%E8%8C%85%E8%8D%AF%E9%85%92%E4%BA%8B%E4%BB%B6/22492086'\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
